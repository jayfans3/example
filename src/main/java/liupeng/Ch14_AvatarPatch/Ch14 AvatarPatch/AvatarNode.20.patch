Index: src/core/org/apache/hadoop/fs/FsShell.java
===================================================================
--- src/core/org/apache/hadoop/fs/FsShell.java	(revision 954586)
+++ src/core/org/apache/hadoop/fs/FsShell.java	(working copy)
@@ -76,7 +76,7 @@
     trash = null;
   }
   
-  protected void init() throws IOException {
+  public void init() throws IOException {
     getConf().setQuietMode(true);
     if (this.fs == null) {
      this.fs = FileSystem.get(getConf());
@@ -1043,7 +1043,7 @@
   }
     
   /* delete a file */
-  private void delete(Path src, FileSystem srcFs, boolean recursive, 
+  public void delete(Path src, FileSystem srcFs, boolean recursive, 
                       boolean skipTrash) throws IOException {
     FileStatus fs = null;
     try {
Index: src/core/org/apache/hadoop/fs/BlockMissingException.java
===================================================================
--- src/core/org/apache/hadoop/fs/BlockMissingException.java	(revision 0)
+++ src/core/org/apache/hadoop/fs/BlockMissingException.java	(revision 0)
@@ -0,0 +1,55 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.fs;
+
+import java.io.IOException;
+
+/** Thrown for file corruption errors. */
+public class BlockMissingException extends IOException {
+
+  private String filename;
+  private long   offset;
+
+  /**
+   * An exception that indicates that file was corrupted.
+   * @param filename name of corrupted file
+   * @param description a description of the corruption details
+   */
+  public BlockMissingException(String filename, String description, long offset) {
+    super(description);
+    this.filename = filename;
+    this.offset = offset;
+  }
+
+  /**
+   * Returns the name of the corrupted file.
+   * @return name of corrupted file
+   */
+  public String getFile() {
+    return filename;
+  }
+
+  /**
+   * Returns the offset at which this file is corrupted
+   * @return name of corrupted file
+   */
+  public long getOffset() {
+    return offset;
+  }
+}
Index: src/contrib/highavailability/ivy.xml
===================================================================
--- src/contrib/highavailability/ivy.xml	(revision 0)
+++ src/contrib/highavailability/ivy.xml	(revision 0)
@@ -0,0 +1,78 @@
+<?xml version="1.0" ?>
+<ivy-module version="1.0">
+  <info organisation="org.apache.hadoop" module="${ant.project.name}">
+    <license name="Apache 2.0"/>
+    <ivyauthor name="Apache Hadoop Team" url="http://hadoop.apache.org"/>
+    <description>
+        Apache Hadoop contrib
+    </description>
+  </info>
+  <configurations defaultconfmapping="default">
+    <!--these match the Maven configurations-->
+    <conf name="default" extends="master,runtime"/>
+    <conf name="master" description="contains the artifact but no dependencies"/>
+    <conf name="runtime" description="runtime but not the artifact" />
+
+    <conf name="common" visibility="private" 
+      description="artifacts needed to compile/test the application"/>
+  </configurations>
+
+  <publications>
+    <!--get the artifact from our module name-->
+    <artifact conf="master"/>
+  </publications>
+  <dependencies>
+    <dependency org="commons-httpclient"
+      name="commons-httpclient"
+      rev="${commons-httpclient.version}"
+      conf="common->master"/>
+    <dependency org="commons-logging"
+      name="commons-logging"
+      rev="${commons-logging.version}"
+      conf="common->default"/>
+    <dependency org="log4j"
+      name="log4j"
+      rev="${log4j.version}"
+      conf="common->master"/>
+    <dependency org="org.mortbay.jetty"
+      name="servlet-api-2.5"
+      rev="${servlet-api-2.5.version}"
+      conf="common->default"/>
+    <dependency org="commons-logging"
+      name="commons-logging"
+      rev="${commons-logging.version}"
+      conf="common->master"/>
+    <dependency org="commons-logging"
+      name="commons-logging-api"
+      rev="${commons-logging-api.version}"
+      conf="common->default"/>
+    <dependency org="junit"
+      name="junit"
+      rev="${junit.version}"
+      conf="common->default"/>
+    <dependency org="org.slf4j"
+      name="slf4j-api"
+      rev="${slf4j-api.version}"
+      conf="common->default"/>
+    <dependency org="org.slf4j"
+      name="slf4j-log4j12"
+      rev="${slf4j-log4j12.version}"
+      conf="common->master"/>
+    <dependency org="xmlenc"
+      name="xmlenc"
+      rev="${xmlenc.version}"
+      conf="common->default"/>
+    <dependency org="org.mortbay.jetty"
+      name="jetty"
+      rev="${jetty.version}"
+      conf="common->default"/>
+    <dependency org="org.mortbay.jetty"
+      name="servlet-api-2.5"
+      rev="${servlet-api-2.5.version}"
+      conf="common->default"/>
+    <dependency org="org.eclipse.jdt"
+      name="core"
+      rev="${core.version}"
+      conf="common->default"/>
+  </dependencies>
+</ivy-module>
Index: src/contrib/highavailability/ivy/libraries.properties
===================================================================
--- src/contrib/highavailability/ivy/libraries.properties	(revision 0)
+++ src/contrib/highavailability/ivy/libraries.properties	(revision 0)
@@ -0,0 +1,5 @@
+#This properties file lists the versions of the various artifacts used by hadoop.
+#It drives ivy and the generation of a maven POM
+#These are the versions of our dependencies (in alphabetical order)
+log4j.version=1.2.15
+slf4j-api.version=1.4.3
Index: src/contrib/highavailability/src/java/org/apache/hadoop/hdfs/protocol/AvatarProtocol.java
===================================================================
--- src/contrib/highavailability/src/java/org/apache/hadoop/hdfs/protocol/AvatarProtocol.java	(revision 0)
+++ src/contrib/highavailability/src/java/org/apache/hadoop/hdfs/protocol/AvatarProtocol.java	(revision 0)
@@ -0,0 +1,63 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hdfs.protocol;
+
+import java.io.IOException;
+import org.apache.hadoop.hdfs.protocol.AvatarConstants.Avatar;
+import org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration;
+
+/**********************************************************************
+ * AvatarProtocol is a superset of the ClientPotocol. It includes
+ * methods to switch avatars from Namenode to StandbyNode and
+ * vice versa
+ *
+ **********************************************************************/
+public interface AvatarProtocol extends ClientProtocol {
+
+  // The version of this protocl is the same as the version of 
+  // the underlying client protocol.
+
+  /**
+   * Get the avatar of this instance
+   * @return the current avatar of this instance
+   * @throws IOException
+   */
+  public Avatar getAvatar() throws IOException;
+
+  /**
+   * Set the avatar of this instance
+   * @throws IOException
+   */
+  public void setAvatar(Avatar avatar) throws IOException;
+
+  /**
+   * Override the blockReceived message in the DatanodeProtocol
+   * This makes the namenode return the list of blocks that do not
+   * belong to any file, the AvatarDataNode then retries this
+   * blockreceived message. This trick populates newly created files/block
+   * with their correct replica locations on the StandbyNamenode.
+   * If a block truly does not belong to any file, then it will be 
+   * cleared up in the next block report.
+   * @return the list of blocks that do not belong to any file in the
+   * namenode.
+   */
+  public Block[] blockReceivedNew(DatanodeRegistration registration,
+                                  Block blocks[],
+                                  String[] delHints) throws IOException;
+}
+
Index: src/contrib/highavailability/src/java/org/apache/hadoop/hdfs/protocol/AvatarConstants.java
===================================================================
--- src/contrib/highavailability/src/java/org/apache/hadoop/hdfs/protocol/AvatarConstants.java	(revision 0)
+++ src/contrib/highavailability/src/java/org/apache/hadoop/hdfs/protocol/AvatarConstants.java	(revision 0)
@@ -0,0 +1,107 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hdfs.protocol;
+
+import java.io.FileNotFoundException;
+import java.io.IOException;
+import java.io.DataInput;
+import java.net.URI;
+import java.net.URISyntaxException;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.permission.FsPermission;
+import org.apache.hadoop.fs.ChecksumException;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.FilterFileSystem;
+import org.apache.hadoop.fs.FSDataInputStream;
+import org.apache.hadoop.fs.FSInputStream;
+import org.apache.hadoop.util.Progressable;
+import org.apache.hadoop.util.ReflectionUtils;
+import org.apache.hadoop.hdfs.server.namenode.NameNode;
+
+/**
+ * Some global definitions for AvatarNode.
+ */
+
+public interface AvatarConstants {
+
+  /**
+   * Define the various avatars of the NameNode.
+   */
+  static public enum Avatar {
+    ACTIVE    ("Primary"),
+    STANDBY   ("Standby"),
+    UNKNOWN   ("UnknownAvatar");
+
+    private String description = null;
+    private Avatar(String arg) {this.description = arg;}
+
+    public String toString() {
+      return description;
+    }
+  }
+
+  /**
+   * Define unique names for the instances of the AvatarNode.
+   * At present, there can be only two.
+   */
+  static public enum InstanceId {
+    NODEZERO    ("FirstNode"),
+    NODEONE   ("SecondNode"),
+    UNKNOWN   ("Unknown");
+
+    private String description = null;
+    private InstanceId(String arg) {this.description = arg;}
+
+    public String toString() {
+      return description;
+    }
+  }
+
+  /** Startup options */
+  static public enum StartupOption {
+    NODEZERO("-zero"),
+    NODEONE("-one"),
+    SYNC("-sync"),
+    ACTIVE ("-active"),
+    STANDBY  ("-standby"),
+    FORMAT  ("-format"),     // these are namenode options
+    REGULAR ("-regular"),
+    UPGRADE ("-upgrade"),
+    ROLLBACK("-rollback"),
+    FINALIZE("-finalize"),
+    IMPORT  ("-importCheckpoint");
+
+    private String name = null;
+    private StartupOption(String arg) {this.name = arg;}
+    public String getName() {return name;}
+    public Avatar toAvatar() {
+      switch(this) {
+      case STANDBY:
+        return Avatar.STANDBY;
+      case ACTIVE:
+        return Avatar.ACTIVE;
+      default:
+        return Avatar.UNKNOWN;
+      }
+    }
+  }
+}
Index: src/contrib/highavailability/src/java/org/apache/hadoop/hdfs/AvatarShell.java
===================================================================
--- src/contrib/highavailability/src/java/org/apache/hadoop/hdfs/AvatarShell.java	(revision 0)
+++ src/contrib/highavailability/src/java/org/apache/hadoop/hdfs/AvatarShell.java	(revision 0)
@@ -0,0 +1,303 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hdfs;
+
+import java.io.IOException;
+import java.util.Collection;
+import java.util.Map;
+import java.util.HashMap;
+import java.util.concurrent.TimeUnit;
+import java.net.InetSocketAddress;
+import javax.security.auth.login.LoginException;
+
+import org.apache.hadoop.ipc.*;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.net.NetUtils;
+import org.apache.hadoop.conf.Configured;
+import org.apache.hadoop.util.Tool;
+import org.apache.hadoop.util.ToolRunner;
+import org.apache.hadoop.io.retry.RetryPolicy;
+import org.apache.hadoop.io.retry.RetryPolicies;
+import org.apache.hadoop.io.retry.RetryProxy;
+import org.apache.hadoop.security.UnixUserGroupInformation;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hdfs.protocol.AvatarProtocol;
+import org.apache.hadoop.hdfs.protocol.AvatarConstants.Avatar;
+import org.apache.hadoop.hdfs.server.namenode.AvatarNode;
+
+/**
+ * A {@link AvatarShell} that allows browsing configured avatar policies.
+ */
+public class AvatarShell extends Configured implements Tool {
+  public static final Log LOG = LogFactory.getLog( "org.apache.hadoop.AvatarShell");
+
+  public AvatarProtocol avatarnode;
+  final AvatarProtocol rpcAvatarnode;
+  private UnixUserGroupInformation ugi;
+  volatile boolean clientRunning = true;
+  private Configuration conf;
+
+  /**
+   * Start AvatarShell.
+   * <p>
+   * The AvatarShell connects to the specified AvatarNode and performs basic
+   * configuration options.
+   * @throws IOException
+   */
+  public AvatarShell() throws IOException {
+    this(new Configuration());
+  }
+
+  /**
+   * The AvatarShell connects to the specified AvatarNode and performs basic
+   * configuration options.
+   * @param conf The Hadoop configuration
+   * @throws IOException
+   */
+  public AvatarShell(Configuration conf) throws IOException {
+    super(conf);
+    try {
+      this.ugi = UnixUserGroupInformation.login(conf, true);
+    } catch (LoginException e) {
+      throw (IOException)(new IOException().initCause(e));
+    }
+
+    this.rpcAvatarnode = createRPCAvatarnode(AvatarNode.getAddress(conf), conf, ugi);
+    this.avatarnode = createAvatarnode(rpcAvatarnode);
+    this.conf = conf;
+  }
+
+  public static AvatarProtocol createAvatarnode(Configuration conf) throws IOException {
+    return createAvatarnode(AvatarNode.getAddress(conf), conf);
+  }
+
+  public static AvatarProtocol createAvatarnode(InetSocketAddress avatarNodeAddr,
+      Configuration conf) throws IOException {
+    try {
+      return createAvatarnode(createRPCAvatarnode(avatarNodeAddr, conf,
+        UnixUserGroupInformation.login(conf, true)));
+    } catch (LoginException e) {
+      throw (IOException)(new IOException().initCause(e));
+    }
+  }
+
+  private static AvatarProtocol createRPCAvatarnode(InetSocketAddress avatarNodeAddr,
+      Configuration conf, UnixUserGroupInformation ugi)
+    throws IOException {
+    LOG.info("AvatarShell connecting to " + avatarNodeAddr);
+    return (AvatarProtocol)RPC.getProxy(AvatarProtocol.class,
+        AvatarProtocol.versionID, avatarNodeAddr, ugi, conf,
+        NetUtils.getSocketFactory(conf, AvatarProtocol.class));
+  }
+
+  private static AvatarProtocol createAvatarnode(AvatarProtocol rpcAvatarnode)
+    throws IOException {
+    RetryPolicy createPolicy = RetryPolicies.retryUpToMaximumCountWithFixedSleep(
+        5, 5000, TimeUnit.MILLISECONDS);
+
+    Map<Class<? extends Exception>,RetryPolicy> remoteExceptionToPolicyMap =
+      new HashMap<Class<? extends Exception>, RetryPolicy>();
+
+    Map<Class<? extends Exception>,RetryPolicy> exceptionToPolicyMap =
+      new HashMap<Class<? extends Exception>, RetryPolicy>();
+    exceptionToPolicyMap.put(RemoteException.class,
+        RetryPolicies.retryByRemoteException(
+            RetryPolicies.TRY_ONCE_THEN_FAIL, remoteExceptionToPolicyMap));
+    RetryPolicy methodPolicy = RetryPolicies.retryByException(
+        RetryPolicies.TRY_ONCE_THEN_FAIL, exceptionToPolicyMap);
+    Map<String,RetryPolicy> methodNameToPolicyMap = new HashMap<String,RetryPolicy>();
+
+    methodNameToPolicyMap.put("create", methodPolicy);
+
+    return (AvatarProtocol) RetryProxy.create(AvatarProtocol.class,
+        rpcAvatarnode, methodNameToPolicyMap);
+  }
+
+  private void checkOpen() throws IOException {
+    if (!clientRunning) {
+      IOException result = new IOException("AvatarNode closed");
+      throw result;
+    }
+  }
+
+  /**
+   * Close the connection to the avatarNode.
+   */
+  public synchronized void close() throws IOException {
+    if(clientRunning) {
+      clientRunning = false;
+      RPC.stopProxy(rpcAvatarnode);
+    }
+  }
+
+  /**
+   * Displays format of commands.
+   */
+  private static void printUsage(String cmd) {
+    String prefix = "Usage: java " + AvatarShell.class.getSimpleName();
+    if ("-showAvatar".equals(cmd)) {
+      System.err.println("Usage: java AvatarShell" + 
+                         " [-showAvatar]"); 
+    } else if ("-setAvatar".equals(cmd)) {
+      System.err.println("Usage: java AvatarShell" +
+                         " [-setAvatar {primary|standby}]");
+    } else {
+      System.err.println("Usage: java AvatarShell");
+      System.err.println("           [-showAvatar ]");
+      System.err.println("           [-setAvatar {primary|standby}]");
+      System.err.println();
+      ToolRunner.printGenericCommandUsage(System.err);
+    }
+  }
+
+  /**
+   * run
+   */
+  public int run(String argv[]) throws Exception {
+
+    if (argv.length < 1) {
+      printUsage("");
+      return -1;
+    }
+
+    int exitCode = -1;
+    int i = 0;
+    String cmd = argv[i++];
+    //
+    // verify that we have enough command line parameters
+    //
+    if ("-showAvatar".equals(cmd)) {
+      if (argv.length < 1) {
+        printUsage(cmd);
+        return exitCode;
+      }
+    } else if ("-setAvatar".equals(cmd)) {
+      if (argv.length < 2) {
+        printUsage(cmd);
+        return exitCode;
+      }
+    }
+
+    try {
+      if ("-showAvatar".equals(cmd)) {
+        exitCode = showAvatar(cmd, argv, i);
+      } else if ("-setAvatar".equals(cmd)) {
+        exitCode = setAvatar(cmd, argv, i);
+      } else {
+        exitCode = -1;
+        System.err.println(cmd.substring(1) + ": Unknown command");
+        printUsage("");
+      }
+    } catch (IllegalArgumentException arge) {
+      exitCode = -1;
+      System.err.println(cmd.substring(1) + ": " + arge.getLocalizedMessage());
+      printUsage(cmd);
+    } catch (RemoteException e) {
+      //
+      // This is a error returned by avatarnode server. Print
+      // out the first line of the error mesage, ignore the stack trace.
+      exitCode = -1;
+      try {
+        String[] content;
+        content = e.getLocalizedMessage().split("\n");
+        System.err.println(cmd.substring(1) + ": " +
+                           content[0]);
+      } catch (Exception ex) {
+        System.err.println(cmd.substring(1) + ": " +
+                           ex.getLocalizedMessage());
+      }
+    } catch (IOException e) {
+      //
+      // IO exception encountered locally.
+      // 
+      exitCode = -1;
+      System.err.println(cmd.substring(1) + ": " +
+                         e.getLocalizedMessage());
+    } catch (Exception re) {
+      exitCode = -1;
+      System.err.println(cmd.substring(1) + ": " + re.getLocalizedMessage());
+    } finally {
+    }
+    return exitCode;
+  }
+
+  /**
+   * Apply operation specified by 'cmd' on all parameters
+   * starting from argv[startindex].
+   */
+  private int showAvatar(String cmd, String argv[], int startindex) throws IOException {
+    int exitCode = 0;
+    Avatar avatar = avatarnode.getAvatar();
+    System.out.println("The current avatar of " + 
+                        AvatarNode.getAddress(conf) + " is " +
+                        avatar);
+    return exitCode;
+  }
+
+  /**
+   * Sets the avatar to the specified value
+   */
+  public int setAvatar(String cmd, String argv[], int startindex)
+    throws IOException {
+    int exitCode = 0;
+    String input = argv[startindex];
+    Avatar dest;
+    if (Avatar.ACTIVE.toString().equalsIgnoreCase(input)) {
+      dest = Avatar.ACTIVE;
+    } else if (Avatar.STANDBY.toString().equalsIgnoreCase(input)) {
+      dest = Avatar.STANDBY;
+    } else {
+      throw new IOException("Unknown avatar type " + input);
+    }
+    Avatar current = avatarnode.getAvatar();
+    if (current == dest) {
+      System.out.println("This instance is already in " + current + " avatar.");
+    } else {
+      avatarnode.setAvatar(dest);
+    }
+    return 0;
+  }
+
+  /**
+   * main() has some simple utility methods
+   */
+  public static void main(String argv[]) throws Exception {
+    AvatarShell shell = null;
+    try {
+      shell = new AvatarShell();
+    } catch (RPC.VersionMismatch v) {
+      System.err.println("Version Mismatch between client and server" +
+                         "... command aborted.");
+      System.exit(-1);
+    } catch (IOException e) {
+      System.err.println("Bad connection to AvatarNode. command aborted.");
+      System.exit(-1);
+    }
+
+    int res;
+    try {
+      res = ToolRunner.run(shell, argv);
+    } finally {
+      shell.close();
+    }
+    System.exit(res);
+  }
+}
Index: src/contrib/highavailability/src/java/org/apache/hadoop/hdfs/server/datanode/OfferService.java
===================================================================
--- src/contrib/highavailability/src/java/org/apache/hadoop/hdfs/server/datanode/OfferService.java	(revision 0)
+++ src/contrib/highavailability/src/java/org/apache/hadoop/hdfs/server/datanode/OfferService.java	(revision 0)
@@ -0,0 +1,441 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hdfs.server.datanode;
+
+import java.io.IOException;
+import java.io.File;
+import java.io.FileOutputStream;
+import java.io.DataOutputStream;
+import java.net.InetSocketAddress;
+import java.util.Date;
+import java.util.Iterator;
+import java.util.Collection;
+import java.util.LinkedList;
+import java.util.TreeSet;
+import java.util.Random;
+import java.text.SimpleDateFormat;
+
+import org.apache.hadoop.ipc.*;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.FileUtil;
+import org.apache.hadoop.io.DataOutputBuffer;
+import org.apache.hadoop.hdfs.protocol.FSConstants;
+import org.apache.hadoop.util.ReflectionUtils;
+import org.apache.hadoop.util.StringUtils;
+import org.apache.hadoop.hdfs.protocol.Block;
+import org.apache.hadoop.hdfs.protocol.BlockListAsLongs;
+import org.apache.hadoop.hdfs.protocol.UnregisteredDatanodeException;
+import org.apache.hadoop.hdfs.server.protocol.BlockCommand;
+import org.apache.hadoop.hdfs.server.namenode.NameNode;
+import org.apache.hadoop.hdfs.server.protocol.DatanodeCommand;
+import org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol;
+import org.apache.hadoop.hdfs.server.protocol.DisallowedDatanodeException;
+import org.apache.hadoop.hdfs.server.protocol.UpgradeCommand;
+import org.apache.hadoop.hdfs.server.protocol.NamespaceInfo;
+import org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration;
+import org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeMetrics;
+import org.apache.hadoop.hdfs.server.common.IncorrectVersionException;
+import org.apache.hadoop.hdfs.protocol.AvatarProtocol;
+
+public class OfferService implements Runnable {
+
+  public static final Log LOG = LogFactory.getLog(OfferService.class.getName());
+ 
+  long lastHeartbeat = 0;
+  volatile boolean shouldRun = true;
+  long lastBlockReport = 0;
+  boolean resetBlockReportTime = true;
+  AvatarDataNode anode;
+  DatanodeProtocol namenode;
+  AvatarProtocol avatarnode;
+  InetSocketAddress namenodeAddress;
+  InetSocketAddress avatarnodeAddress;
+  DatanodeRegistration dnRegistration = null;
+  FSDatasetInterface data;
+  DataNodeMetrics myMetrics;
+  private static final Random R = new Random();
+  private int backlogSize; // if we accumulate this many blockReceived, then it is time
+                           // to send a block report. Otherwise the receivedBlockList
+                           // might exceed our Heap size.
+  /**
+   * A data structure to store Block and delHints together
+   */
+  private static class BlockInfo extends Block {
+    String delHints;
+
+    BlockInfo(Block blk, String delHints) {
+      super(blk);
+      this.delHints = delHints;
+    }
+  }
+  private TreeSet<BlockInfo> retryBlockList = new TreeSet<BlockInfo>();
+  private TreeSet<BlockInfo> receivedBlockList = new TreeSet<BlockInfo>();
+  private long lastBlockReceivedFailed = 0; 
+
+  /**
+   * Offer service to the specified namenode
+   */
+  public OfferService(AvatarDataNode anode, 
+                      DatanodeProtocol namenode, InetSocketAddress namenodeAddress,
+                      AvatarProtocol avatarnode, InetSocketAddress avatarnodeAddress) {
+    this.anode = anode;
+    this.namenode = namenode;
+    this.avatarnode = avatarnode;
+    this.namenodeAddress = namenodeAddress;
+    this.avatarnodeAddress = avatarnodeAddress;
+    dnRegistration = anode.dnRegistration;
+    data = anode.data;
+    myMetrics = anode.myMetrics;
+    scheduleBlockReport(anode.initialBlockReportDelay);
+    backlogSize = anode.getConf().getInt("dfs.datanode.blockreceived.backlog", 10000);
+  }
+
+  public void stop() {
+    shouldRun = false;
+  }
+
+  public void run() {
+    while (shouldRun) {
+      try {
+        offerService();
+      } catch (Exception e) {
+        LOG.error("OfferService encountered exception " +
+                   StringUtils.stringifyException(e));
+      }
+    }
+  }
+
+  public void offerService() throws Exception {
+     
+    LOG.info("using BLOCKREPORT_INTERVAL of " + anode.blockReportInterval + "msec" + 
+       " Initial delay: " + anode.initialBlockReportDelay + "msec");
+
+    //
+    // Now loop for a long time....
+    //
+    while (shouldRun) {
+      try {
+
+        // If we are falling behind in confirming blockReceived to NN, then
+        // we clear the backlog and schedule a block report. This scenario
+        // is likely to arise if one of the NN is down for an extended period.
+        if (receivedBlockList.size() + retryBlockList.size() > backlogSize) {
+          LOG.warn("The backlog of blocks to be confirmed has exceeded the " +
+                   " configured maximum of " + backlogSize +
+                   " records. Cleaning up and scheduling a block report.");
+          synchronized(receivedBlockList) {
+            receivedBlockList.clear();
+            retryBlockList.clear();
+          }
+          scheduleBlockReport(0);
+        }
+
+        long startTime = anode.now();
+
+        //
+        // Every so often, send heartbeat or block-report
+        //
+        if (startTime - lastHeartbeat > anode.heartBeatInterval) {
+          //
+          // All heartbeat messages include following info:
+          // -- Datanode name
+          // -- data transfer port
+          // -- Total capacity
+          // -- Bytes remaining
+          //
+          lastHeartbeat = startTime;
+          DatanodeCommand[] cmds = namenode.sendHeartbeat(dnRegistration,
+                                                       data.getCapacity(),
+                                                       data.getDfsUsed(),
+                                                       data.getRemaining(),
+                                                       anode.xmitsInProgress.get(),
+                                                       anode.getXceiverCount());
+          myMetrics.heartbeats.inc(anode.now() - startTime);
+          //LOG.info("Just sent heartbeat, with name " + localName);
+          if (!processCommand(cmds))
+            continue;
+        }
+            
+        // check if there are newly received blocks
+        BlockInfo [] blockArray=null;
+        int numBlocks = 0;
+        synchronized(receivedBlockList) {
+            // retry previously failed blocks every few seconds
+          if (lastBlockReceivedFailed + 10000 < anode.now()) {
+            for (BlockInfo blk : retryBlockList) {
+              receivedBlockList.add(blk);
+            }
+            retryBlockList.clear();
+          }
+          numBlocks = receivedBlockList.size();
+          if (numBlocks > 0) {
+            blockArray = receivedBlockList.toArray(new BlockInfo[numBlocks]);
+          }
+        }
+        if (blockArray != null) {
+
+          String[] delHintArray = new String[numBlocks];
+          Block[]  blist = new Block[numBlocks];
+          for (int i = 0; i < numBlocks; i++) {
+            delHintArray[i] = blockArray[i].delHints;
+            blist[i] = new Block(blockArray[i]);
+          }
+
+          Block[] failed = avatarnode.blockReceivedNew(dnRegistration, blist, 
+                                                       delHintArray);
+          synchronized (receivedBlockList) {
+            // Blocks that do not belong to an Inode are saved for retransmisions
+            for (int i = 0; i < failed.length; i++) {
+              BlockInfo info = null;
+              for(int j = 0; j < blockArray.length; j++) {
+                if (blockArray[j].equals(failed[i])) {
+                  info = blockArray[j];
+                  break;
+                }
+              }
+              if (info == null) {
+                LOG.warn("BlockReceived failed for block " + failed[i] +
+                         " but it is not in our request list.");
+              } else if (receivedBlockList.contains(info)) {
+                 // Insert into retry list only if the block was not deleted
+                 // on this datanode. That is why we have to recheck if the
+                 // block still exists in receivedBlockList.
+                 LOG.info("Block " + info + " does not belong to any file " +
+                          "on namenode " + avatarnodeAddress + " Retry later.");
+                 retryBlockList.add(info);
+                 lastBlockReceivedFailed = anode.now();
+              } else {
+                 LOG.info("Block " + info + " does not belong to any file " +
+                          "on namenode " + avatarnodeAddress + 
+                          " but will not be retried.");
+              }
+            }
+            for (int i = 0; i < blockArray.length; i++) {
+              receivedBlockList.remove(blockArray[i]);
+            }
+          }
+        }
+
+        // send block report
+        if (startTime - lastBlockReport > anode.blockReportInterval) {
+          //
+          // Send latest blockinfo report if timer has expired.
+          // Get back a list of local block(s) that are obsolete
+          // and can be safely GC'ed.
+          //
+          long brStartTime = anode.now();
+          Block[] bReport = data.getBlockReport();
+          DatanodeCommand cmd = namenode.blockReport(dnRegistration,
+                  BlockListAsLongs.convertToArrayLongs(bReport));
+          long brTime = anode.now() - brStartTime;
+          myMetrics.blockReports.inc(brTime);
+          LOG.info("BlockReport of " + bReport.length +
+              " blocks got processed in " + brTime + " msecs on " +
+              namenodeAddress);
+          //
+          // If we have sent the first block report, then wait a random
+          // time before we start the periodic block reports.
+          //
+          if (resetBlockReportTime) {
+            lastBlockReport = startTime - R.nextInt((int)(anode.blockReportInterval));
+            resetBlockReportTime = false;
+          } else {
+            /* say the last block report was at 8:20:14. The current report 
+             * should have started around 9:20:14 (default 1 hour interval). 
+             * If current time is :
+             *   1) normal like 9:20:18, next report should be at 10:20:14
+             *   2) unexpected like 11:35:43, next report should be at 12:20:14
+             */
+            lastBlockReport += (anode.now() - lastBlockReport) / 
+                               anode.blockReportInterval * anode.blockReportInterval;
+          }
+          processCommand(cmd);
+        }
+
+        // start block scanner is moved to the Dataode.run()
+            
+        //
+        // There is no work to do;  sleep until hearbeat timer elapses, 
+        // or work arrives, and then iterate again.
+        //
+        long waitTime = anode.heartBeatInterval - (System.currentTimeMillis() - lastHeartbeat);
+        synchronized(receivedBlockList) {
+          if (waitTime > 0 && receivedBlockList.size() == 0) {
+            try {
+              receivedBlockList.wait(waitTime);
+            } catch (InterruptedException ie) {
+            }
+          }
+        } // synchronized
+      } catch(RemoteException re) {
+        // If either the primary or standby NN throws these exceptions, this
+        // datanode will exit. I think this is the right behaviour because
+        // the excludes list on both namenode better be the same.
+        String reClass = re.getClassName(); 
+        if (UnregisteredDatanodeException.class.getName().equals(reClass) ||
+            DisallowedDatanodeException.class.getName().equals(reClass) ||
+            IncorrectVersionException.class.getName().equals(reClass)) {
+          LOG.warn("DataNode is shutting down: " + 
+                   StringUtils.stringifyException(re));
+          anode.shutdown();
+          return;
+        }
+        LOG.warn(StringUtils.stringifyException(re));
+      } catch (IOException e) {
+        LOG.warn(StringUtils.stringifyException(e));
+      }
+    } // while (shouldRun)
+  } // offerService
+
+  /**
+   * Process an array of datanode commands
+   * 
+   * @param cmds an array of datanode commands
+   * @return true if further processing may be required or false otherwise. 
+   */
+  private boolean processCommand(DatanodeCommand[] cmds) {
+    if (cmds != null) {
+      for (DatanodeCommand cmd : cmds) {
+        try {
+          if (processCommand(cmd) == false) {
+            return false;
+          }
+        } catch (IOException ioe) {
+          LOG.warn("Error processing datanode Command", ioe);
+        }
+      }
+    }
+    return true;
+  }
+  
+    /**
+     * 
+     * @param cmd
+     * @return true if further processing may be required or false otherwise. 
+     * @throws IOException
+     */
+  private boolean processCommand(DatanodeCommand cmd) throws IOException {
+    if (cmd == null)
+      return true;
+    final BlockCommand bcmd = cmd instanceof BlockCommand? (BlockCommand)cmd: null;
+
+    switch(cmd.getAction()) {
+    case DatanodeProtocol.DNA_TRANSFER:
+      // Send a copy of a block to another datanode
+      anode.transferBlocks(bcmd.getBlocks(), bcmd.getTargets());
+      myMetrics.blocksReplicated.inc(bcmd.getBlocks().length);
+      break;
+    case DatanodeProtocol.DNA_INVALIDATE:
+      //
+      // Some local block(s) are obsolete and can be 
+      // safely garbage-collected.
+      //
+      Block toDelete[] = bcmd.getBlocks();
+      try {
+        if (anode.blockScanner != null) {
+          anode.blockScanner.deleteBlocks(toDelete);
+        }
+        data.invalidate(toDelete);
+        anode.removeReceivedBlocks(toDelete);
+      } catch(IOException e) {
+        anode.checkDiskError();
+        throw e;
+      }
+      myMetrics.blocksRemoved.inc(toDelete.length);
+      break;
+    case DatanodeProtocol.DNA_SHUTDOWN:
+      // shut down the data node
+      anode.shutdown();
+      return false;
+    case DatanodeProtocol.DNA_REGISTER:
+      // namenode requested a registration - at start or if NN lost contact
+      LOG.info("AvatarDatanodeCommand action: DNA_REGISTER");
+      if (shouldRun) {
+        anode.register(namenode, namenodeAddress);
+        scheduleBlockReport(0);
+      }
+      break;
+    case DatanodeProtocol.DNA_FINALIZE:
+      anode.getStorage().finalizeUpgrade();
+      break;
+    case UpgradeCommand.UC_ACTION_START_UPGRADE:
+      // start distributed upgrade here
+      anode.upgradeManager.processUpgradeCommand((UpgradeCommand)cmd);
+      break;
+    case DatanodeProtocol.DNA_RECOVERBLOCK:
+      anode.recoverBlocks(bcmd.getBlocks(), bcmd.getTargets());
+      break;
+    default:
+      LOG.warn("Unknown DatanodeCommand action: " + cmd.getAction());
+    }
+    return true;
+  }
+
+  /**
+   * This methods  arranges for the data node to send the block report at the next heartbeat.
+   */
+  public void scheduleBlockReport(long delay) {
+    if (delay > 0) { // send BR after random delay
+      lastBlockReport = System.currentTimeMillis()
+                            - ( anode.blockReportInterval - R.nextInt((int)(delay)));
+    } else { // send at next heartbeat
+      lastBlockReport = lastHeartbeat - anode.blockReportInterval;
+    }
+    resetBlockReportTime = true; // reset future BRs for randomness
+  }
+
+  /**
+   * Inform the namenode that we have received a block
+   */
+  void notifyNamenodeReceivedBlock(Block block, String delHint) {
+    if (block==null || delHint==null) {
+      throw new IllegalArgumentException(block==null?"Block is null":"delHint is null");
+    }
+    synchronized (receivedBlockList) {
+      receivedBlockList.add(new BlockInfo(block, delHint));
+      receivedBlockList.notifyAll();
+    }
+  }
+
+  /**
+   * Remove  blocks from blockReceived queues
+   */
+  void removeReceivedBlocks(Block[] removeList) {
+    synchronized(receivedBlockList) {
+      for (int i = 0; i < removeList.length; i++) {
+        if (receivedBlockList.remove(removeList[i])) {
+          LOG.info("Block deletion command deleted from receivedBlockList " + 
+                   removeList[i]);
+        }
+        else if (retryBlockList.remove(removeList[i])) {
+          LOG.info("Block deletion command deleted from retryBlockList " + 
+                   removeList[i]);
+        } else {
+          LOG.info("Block deletion command did not find block in " +
+                   "pending blockReceived. " + removeList[i]);
+        }
+      }
+    }
+  }
+}
+
+
Index: src/contrib/highavailability/src/java/org/apache/hadoop/hdfs/server/datanode/DatanodeProtocols.java
===================================================================
--- src/contrib/highavailability/src/java/org/apache/hadoop/hdfs/server/datanode/DatanodeProtocols.java	(revision 0)
+++ src/contrib/highavailability/src/java/org/apache/hadoop/hdfs/server/datanode/DatanodeProtocols.java	(revision 0)
@@ -0,0 +1,228 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hdfs.server.datanode;
+
+import java.io.IOException;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.util.StringUtils;
+import org.apache.hadoop.hdfs.protocol.Block;
+import org.apache.hadoop.hdfs.protocol.LocatedBlock;
+import org.apache.hadoop.hdfs.protocol.DatanodeID;
+import org.apache.hadoop.hdfs.protocol.DatanodeInfo;
+import org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol;
+import org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration;
+import org.apache.hadoop.hdfs.server.protocol.DatanodeCommand;
+import org.apache.hadoop.hdfs.server.protocol.UpgradeCommand;
+import org.apache.hadoop.hdfs.server.protocol.NamespaceInfo;
+
+/**********************************************************************
+ * Protocol that a DFS datanode uses to communicate with the NameNode.
+ * This class encapsules multiple objects that expose DatanodeProtocol.
+ *
+ **********************************************************************/
+public class DatanodeProtocols implements DatanodeProtocol {
+
+  public static final Log LOG = LogFactory.getLog(DatanodeProtocols.class.getName());
+
+  DatanodeProtocol node[];
+  int numProtocol;
+
+  private String errMessage = " should occur individually " +
+                              " for each namenode.";
+
+  /**
+   * Maximum number of protocol object encapsulated here
+   */
+  DatanodeProtocols(int max) {
+    numProtocol = max;
+    node = new DatanodeProtocol[max];
+    for (int i = 0; i < max; i++) {
+      node[i] = null;
+    }
+  }
+
+  void setDatanodeProtocol(DatanodeProtocol prot, int index) {
+    this.node[index] = prot;
+  }
+
+  /** {@inheritDoc} */
+  public long getProtocolVersion(String protocol,
+                                 long clientVersion) throws IOException {
+    IOException last = new IOException("No DatanodeProtocol found.");
+    long lastProt = -1;
+    for (int i = 0; i < numProtocol; i++) {
+      try {
+        if (node[i] != null) {
+          long prot =  node[i].getProtocolVersion(protocol, clientVersion);
+          if (lastProt != -1) {
+            if (prot != lastProt) {
+              throw new IOException("Versions of DatanodeProtocol " +
+                                    " objects have to be same." +
+                                    " Found version " + prot +
+                                    " does not match with " + lastProt);
+            }
+            lastProt = prot;
+          }
+        }
+      } catch (IOException e) {
+        last = e;
+        LOG.info("Server " + node[i] + " " +
+                 StringUtils.stringifyException(e));
+      }
+    }
+    if (lastProt == -1) {
+      throw last; // fail if all DatanodeProtocol object failed.
+    }
+    return lastProt; // all objects have the same version
+  }
+
+
+  /**
+   * This method should not be invoked on the composite 
+   * DatanodeProtocols object. You can call these on the individual
+   * DatanodeProcol objects.
+   */
+  public DatanodeRegistration register(DatanodeRegistration registration
+                                       ) throws IOException {
+    throw new IOException("Registration" + errMessage);
+  }
+
+  /**
+   * This method should not be invoked on the composite 
+   * DatanodeProtocols object. You can call these on the individual
+   * DatanodeProcol objects.
+   */
+  public DatanodeCommand[] sendHeartbeat(DatanodeRegistration registration,
+                                       long capacity,
+                                       long dfsUsed, long remaining,
+                                       int xmitsInProgress,
+                                       int xceiverCount) throws IOException {
+    throw new IOException("sendHeartbeat" + errMessage);
+  }
+
+  /**
+   * This method should not be invoked on the composite 
+   * DatanodeProtocols object. You can call these on the individual
+   * DatanodeProcol objects.
+   */
+  public DatanodeCommand blockReport(DatanodeRegistration registration,
+                                     long[] blocks) throws IOException {
+    throw new IOException("blockReport" + errMessage);
+  }
+    
+  /**
+   * This method should not be invoked on the composite 
+   * DatanodeProtocols object. You can call these on the individual
+   * DatanodeProcol objects.
+   */
+  public void blockReceived(DatanodeRegistration registration,
+                            Block blocks[],
+                            String[] delHints) throws IOException {
+    throw new IOException("blockReceived" + errMessage);
+  }
+
+  /** {@inheritDoc} */
+  public void errorReport(DatanodeRegistration registration,
+                          int errorCode, 
+                          String msg) throws IOException {
+    for (int i = 0; i < numProtocol; i++) {
+      try {
+        if (node[i] != null) {
+          node[i].errorReport(registration, errorCode, msg);
+        }
+      } catch (IOException e) {
+        LOG.info("Server " + node[i] + " " +
+                 StringUtils.stringifyException(e));
+      }
+    }
+  }
+    
+  /**
+   * This method should not be invoked on the composite 
+   * DatanodeProtocols object. You can call these on the individual
+   * DatanodeProcol objects.
+   */
+  public NamespaceInfo versionRequest() throws IOException {
+    throw new IOException("versionRequest" + errMessage);
+  }
+
+  /**
+   * This method should not be invoked on the composite 
+   * DatanodeProtocols object. You can call these on the individual
+   * DatanodeProcol objects.
+   */
+  public UpgradeCommand processUpgradeCommand(UpgradeCommand comm) throws IOException {
+    throw new IOException("processUpgradeCommand" + errMessage);
+  }
+  
+  /** {@inheritDoc} */
+  public void reportBadBlocks(LocatedBlock[] blocks) throws IOException {
+    for (int i = 0; i < numProtocol; i++) {
+      try {
+        if (node[i] != null) {
+          node[i].reportBadBlocks(blocks);
+        }
+      } catch (IOException e) {
+        LOG.info("Server " + node[i] + " " +
+                 StringUtils.stringifyException(e));
+      }
+    }
+  }
+  
+  /** {@inheritDoc} */
+  public long nextGenerationStamp(Block block) throws IOException {
+    IOException last = new IOException("No DatanodeProtocol found.");
+    for (int i = 0; i < numProtocol; i++) {
+      try {
+        return node[i].nextGenerationStamp(block);
+      } catch (IOException e) {
+        last = e;
+        LOG.info("Server " + node[i] + " " +
+                 StringUtils.stringifyException(e));
+      }
+    }
+    throw last; // fail if all DatanodeProtocol object failed.
+  }
+
+  /** {@inheritDoc} */
+  public void commitBlockSynchronization(Block block,
+      long newgenerationstamp, long newlength,
+      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets
+      ) throws IOException {
+    IOException last = new IOException("No DatanodeProtocol found.");
+    for (int i = 0; i < numProtocol; i++) {
+      try {
+        if (node[i] != null) {
+          node[i].commitBlockSynchronization(block, newgenerationstamp,
+                                             newlength, closeFile,
+                                             deleteblock, newtargets);
+          return;
+        }
+      } catch (IOException e) {
+        last = e;
+        LOG.info("Server " + node[i] + " " +
+                 StringUtils.stringifyException(e));
+      }
+    }
+    throw last; // fail if all DatanodeProtocol object failed.
+  }
+}
Index: src/contrib/highavailability/src/java/org/apache/hadoop/hdfs/server/datanode/AvatarDataNode.java
===================================================================
--- src/contrib/highavailability/src/java/org/apache/hadoop/hdfs/server/datanode/AvatarDataNode.java	(revision 0)
+++ src/contrib/highavailability/src/java/org/apache/hadoop/hdfs/server/datanode/AvatarDataNode.java	(revision 0)
@@ -0,0 +1,738 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hdfs.server.datanode;
+
+import java.io.IOException;
+import java.io.File;
+import java.io.FileOutputStream;
+import java.io.DataOutputStream;
+import java.net.InetSocketAddress;
+import java.net.SocketTimeoutException;
+import java.net.ServerSocket;
+import java.nio.channels.ServerSocketChannel;
+import java.nio.channels.SocketChannel;
+import java.net.ConnectException;
+import java.util.Date;
+import java.util.Iterator;
+import java.util.Collection;
+import java.util.AbstractList;
+import java.util.ArrayList;
+import java.text.SimpleDateFormat;
+import java.lang.reflect.Method;
+
+import org.apache.hadoop.ipc.*;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.FileUtil;
+import org.apache.hadoop.io.DataOutputBuffer;
+import org.apache.hadoop.net.NetUtils;
+import org.apache.hadoop.net.DNS;
+import org.apache.hadoop.hdfs.protocol.FSConstants;
+import org.apache.hadoop.util.Daemon;
+import org.apache.hadoop.util.DiskChecker;
+import org.apache.hadoop.util.ReflectionUtils;
+import org.apache.hadoop.util.StringUtils;
+import org.apache.hadoop.hdfs.protocol.Block;
+import org.apache.hadoop.hdfs.protocol.LocatedBlock;
+import org.apache.hadoop.hdfs.protocol.DatanodeInfo;
+import org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol;
+import org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration;
+import org.apache.hadoop.hdfs.server.protocol.NamespaceInfo;
+import org.apache.hadoop.hdfs.server.namenode.NameNode;
+import org.apache.hadoop.hdfs.server.namenode.FileChecksumServlets;
+import org.apache.hadoop.hdfs.server.namenode.StreamFile;
+import org.apache.hadoop.hdfs.server.common.Storage;
+import org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeMetrics;
+import org.apache.hadoop.util.DiskChecker.DiskErrorException;
+import org.apache.hadoop.hdfs.server.common.HdfsConstants.StartupOption;
+import org.apache.hadoop.hdfs.server.common.HdfsConstants;
+import org.apache.hadoop.security.SecurityUtil;
+import org.apache.hadoop.security.authorize.ConfiguredPolicy;
+import org.apache.hadoop.security.authorize.PolicyProvider;
+import org.apache.hadoop.security.authorize.ServiceAuthorizationManager;
+import org.apache.hadoop.hdfs.HDFSPolicyProvider;
+import org.apache.hadoop.http.HttpServer;
+
+import org.apache.hadoop.hdfs.protocol.AvatarProtocol;
+import org.apache.hadoop.hdfs.server.namenode.AvatarNode;
+
+/**
+ * This is an implementation of the AvatarDataNode, a wrapper
+ * for a regular datanode that works with AvatarNode.
+ * 
+ * The AvatarDataNode is needed to make a vanilla DataNode send
+ * block reports to Primary and standby namenodes. The AvatarDataNode
+ * does not know which one of the namenodes is primary and which is
+ * secondary.
+ *
+ * Typically, an adminstrator will have to specify the pair of
+ * AvatarNodes via fs1.default.name and fs2.default.name
+ *
+ */
+
+public class AvatarDataNode extends DataNode {
+
+  public static final Log LOG = LogFactory.getLog(AvatarDataNode.class.getName());
+
+  // public DatanodeProtocol namenode; // override in DataNode class
+  InetSocketAddress nameAddr1;
+  InetSocketAddress nameAddr2;
+  DatanodeProtocol namenode1;
+  DatanodeProtocol namenode2;
+  AvatarProtocol avatarnode1;
+  AvatarProtocol avatarnode2;
+  InetSocketAddress avatarAddr1;
+  InetSocketAddress avatarAddr2;
+  boolean doneRegister1 = false;    // not yet registered with namenode1
+  boolean doneRegister2 = false;    // not yet registered with namenode2
+  OfferService offerService1;
+  OfferService offerService2;
+  Thread of1;
+  Thread of2;
+  private DataStorage storage;
+  private static String dnThreadName;
+  private InetSocketAddress selfAddr;
+  private HttpServer infoServer;
+  private Thread dataNodeThread;
+  Method transferBlockMethod;
+
+  public AvatarDataNode(Configuration conf, AbstractList<File> dataDirs) 
+    throws IOException {
+    super(conf, dataDirs);
+
+    // access a private member of the base DataNode class
+    try { 
+      Method[] methods = DataNode.class.getDeclaredMethods();
+      for (int i = 0; i < methods.length; i++) {
+        if (methods[i].getName().equals("transferBlock")) {
+          transferBlockMethod = methods[i];
+        }
+      }
+      if (transferBlockMethod == null) {
+        throw new IOException("Unable to find method DataNode.transferBlock.");
+      }
+      transferBlockMethod.setAccessible(true);
+    } catch (java.lang.SecurityException exp) {
+      throw new IOException(exp);
+    }
+  }
+
+  @Override
+  void startDataNode(Configuration conf, 
+                     AbstractList<File> dataDirs
+                     ) throws IOException {
+    // use configured nameserver & interface to get local hostname
+    if (conf.get("slave.host.name") != null) {
+      machineName = conf.get("slave.host.name");   
+    }
+    if (machineName == null) {
+      machineName = DNS.getDefaultHost(
+                                     conf.get("dfs.datanode.dns.interface","default"),
+                                     conf.get("dfs.datanode.dns.nameserver","default"));
+    }
+    
+    this.socketTimeout =  conf.getInt("dfs.socket.timeout",
+                                      HdfsConstants.READ_TIMEOUT);
+    this.socketWriteTimeout = conf.getInt("dfs.datanode.socket.write.timeout",
+                                          HdfsConstants.WRITE_TIMEOUT);
+    /* Based on results on different platforms, we might need set the default 
+     * to false on some of them. */
+    this.transferToAllowed = conf.getBoolean("dfs.datanode.transferTo.allowed", 
+                                             true);
+    this.writePacketSize = conf.getInt("dfs.write.packet.size", 64*1024);
+    String address = 
+      NetUtils.getServerAddress(conf,
+                                "dfs.datanode.bindAddress", 
+                                "dfs.datanode.port",
+                                "dfs.datanode.address");
+    InetSocketAddress socAddr = NetUtils.createSocketAddr(address);
+    int tmpPort = socAddr.getPort();
+    storage = new DataStorage();
+    // construct registration
+    this.dnRegistration = new DatanodeRegistration(machineName + ":" + tmpPort);
+
+    this.namenode = new DatanodeProtocols(2); // override DataNode.namenode
+    nameAddr1 = AvatarDataNode.getNameNodeAddress(getConf(), "fs.default.name0", "dfs.namenode.dn-address0");
+    nameAddr2 = AvatarDataNode.getNameNodeAddress(getConf(), "fs.default.name1", "dfs.namenode.dn-address1");
+    avatarAddr1 = AvatarDataNode.getAvatarNodeAddress(getConf(), "fs.default.name0");
+    avatarAddr2 = AvatarDataNode.getAvatarNodeAddress(getConf(), "fs.default.name1");
+
+    // get version and id info from the name-node
+    NamespaceInfo nsInfo = handshake(true);
+    StartupOption startOpt = getStartupOption(conf);
+    assert startOpt != null : "Startup option must be set.";
+    
+    boolean simulatedFSDataset = 
+        conf.getBoolean("dfs.datanode.simulateddatastorage", false);
+    if (simulatedFSDataset) {
+        setNewStorageID(dnRegistration);
+        dnRegistration.storageInfo.layoutVersion = FSConstants.LAYOUT_VERSION;
+        dnRegistration.storageInfo.namespaceID = nsInfo.namespaceID;
+        // it would have been better to pass storage as a parameter to
+        // constructor below - need to augment ReflectionUtils used below.
+        conf.set("StorageId", dnRegistration.getStorageID());
+        try {
+          //Equivalent of following (can't do because Simulated is in test dir)
+          //  this.data = new SimulatedFSDataset(conf);
+          this.data = (FSDatasetInterface) ReflectionUtils.newInstance(
+              Class.forName("org.apache.hadoop.hdfs.server.datanode.SimulatedFSDataset"), conf);
+        } catch (ClassNotFoundException e) {
+          throw new IOException(StringUtils.stringifyException(e));
+        }
+    } else { // real storage
+      // read storage info, lock data dirs and transition fs state if necessary
+      storage.recoverTransitionRead(nsInfo, dataDirs, startOpt);
+      // adjust
+      this.dnRegistration.setStorageInfo(storage);
+      // initialize data node internal structure
+      this.data = new FSDataset(storage, conf);
+    }
+
+      
+    // find free port
+    ServerSocket ss = (socketWriteTimeout > 0) ? 
+          ServerSocketChannel.open().socket() : new ServerSocket();
+    Server.bind(ss, socAddr, 0);
+    ss.setReceiveBufferSize(DEFAULT_DATA_SOCKET_SIZE); 
+    // adjust machine name with the actual port
+    tmpPort = ss.getLocalPort();
+    selfAddr = new InetSocketAddress(ss.getInetAddress().getHostAddress(),
+                                     tmpPort);
+    this.dnRegistration.setName(machineName + ":" + tmpPort);
+    LOG.info("Opened info server at " + tmpPort);
+      
+    this.threadGroup = new ThreadGroup("dataXceiverServer");
+    this.dataXceiverServer = new Daemon(threadGroup, 
+        new DataXceiverServer(ss, conf, this));
+    this.threadGroup.setDaemon(true); // auto destroy when empty
+
+    this.blockReportInterval =
+      conf.getLong("dfs.blockreport.intervalMsec", BLOCKREPORT_INTERVAL);
+    this.initialBlockReportDelay = conf.getLong("dfs.blockreport.initialDelay",
+                                            BLOCKREPORT_INITIAL_DELAY)* 1000L; 
+    if (this.initialBlockReportDelay >= blockReportInterval) {
+      this.initialBlockReportDelay = 0;
+      LOG.info("dfs.blockreport.initialDelay is greater than " +
+        "dfs.blockreport.intervalMsec." + " Setting initial delay to 0 msec:");
+    }
+    this.heartBeatInterval = conf.getLong("dfs.heartbeat.interval", HEARTBEAT_INTERVAL) * 1000L;
+    //initialize periodic block scanner
+    String reason = null;
+    if (conf.getInt("dfs.datanode.scan.period.hours", 0) < 0) {
+      reason = "verification is turned off by configuration";
+    } else if ( !(data instanceof FSDataset) ) {
+      reason = "verifcation is supported only with FSDataset";
+    } 
+    if ( reason == null ) {
+      blockScanner = new DataBlockScanner(this, (FSDataset)data, conf);
+    } else {
+      LOG.info("Periodic Block Verification is disabled because " +
+               reason + ".");
+    }
+
+    //create a servlet to serve full-file content
+    String infoAddr = 
+      NetUtils.getServerAddress(conf, 
+                              "dfs.datanode.info.bindAddress", 
+                              "dfs.datanode.info.port",
+                              "dfs.datanode.http.address");
+    InetSocketAddress infoSocAddr = NetUtils.createSocketAddr(infoAddr);
+    String infoHost = infoSocAddr.getHostName();
+    int tmpInfoPort = infoSocAddr.getPort();
+    this.infoServer = new HttpServer("datanode", infoHost, tmpInfoPort,
+        tmpInfoPort == 0, conf);
+    if (conf.getBoolean("dfs.https.enable", false)) {
+      boolean needClientAuth = conf.getBoolean("dfs.https.need.client.auth", false);
+      InetSocketAddress secInfoSocAddr = NetUtils.createSocketAddr(conf.get(
+          "dfs.datanode.https.address", infoHost + ":" + 0));
+      Configuration sslConf = new Configuration(false);
+      sslConf.addResource(conf.get("dfs.https.server.keystore.resource",
+          "ssl-server.xml"));
+      this.infoServer.addSslListener(secInfoSocAddr, sslConf, needClientAuth);
+    }
+    this.infoServer.addInternalServlet(null, "/streamFile/*", StreamFile.class);
+    this.infoServer.addInternalServlet(null, "/getFileChecksum/*",
+        FileChecksumServlets.GetServlet.class);
+    this.infoServer.setAttribute("datanode.blockScanner", blockScanner);
+    this.infoServer.addServlet(null, "/blockScannerReport", 
+                               DataBlockScanner.Servlet.class);
+    this.infoServer.start();
+    // adjust info port
+    this.dnRegistration.setInfoPort(this.infoServer.getPort());
+    myMetrics = new DataNodeMetrics(conf, dnRegistration.getStorageID());
+    
+    // set service-level authorization security policy
+    if (conf.getBoolean(
+          ServiceAuthorizationManager.SERVICE_AUTHORIZATION_CONFIG, false)) {
+      PolicyProvider policyProvider = 
+        (PolicyProvider)(ReflectionUtils.newInstance(
+            conf.getClass(PolicyProvider.POLICY_PROVIDER_CONFIG, 
+                HDFSPolicyProvider.class, PolicyProvider.class), 
+            conf));
+      SecurityUtil.setPolicy(new ConfiguredPolicy(conf, policyProvider));
+    }
+
+    //init ipc server
+    InetSocketAddress ipcAddr = NetUtils.createSocketAddr(
+        conf.get("dfs.datanode.ipc.address"));
+    ipcServer = RPC.getServer(this, ipcAddr.getHostName(), ipcAddr.getPort(), 
+        conf.getInt("dfs.datanode.handler.count", 3), false, conf);
+    ipcServer.start();
+    dnRegistration.setIpcPort(ipcServer.getListenerAddress().getPort());
+
+    LOG.info("dnRegistration = " + dnRegistration);
+  }
+
+  // connect to both name node if possible. 
+  // If doWait is true, then return only when at least one handshake is
+  // successful.
+  //
+  private synchronized NamespaceInfo handshake(boolean doWait) throws IOException {
+    NamespaceInfo nsInfo = null;
+    do {
+      try {
+        if (namenode1 == null) {
+          namenode1 = (DatanodeProtocol) 
+                           RPC.getProxy(DatanodeProtocol.class,
+                             DatanodeProtocol.versionID,
+                           nameAddr1, 
+                           getConf());
+          ((DatanodeProtocols)namenode).setDatanodeProtocol(namenode1, 0);
+        }
+        if (avatarnode1 == null) {
+          avatarnode1 = (AvatarProtocol) 
+                           RPC.getProxy(AvatarProtocol.class,
+                             AvatarProtocol.versionID,
+                           avatarAddr1, 
+                           getConf());
+        }
+        nsInfo = handshake(namenode1, nameAddr1);
+      } catch(ConnectException se) {  // namenode has not been started
+        LOG.info("Server at " + nameAddr1 + " not available yet, Zzzzz...");
+      } catch(SocketTimeoutException te) {  // namenode is busy
+        LOG.info("Problem connecting to server timeout. " + nameAddr1);
+      }
+      try {
+        if (namenode2 == null) {
+          namenode2 = (DatanodeProtocol) 
+                           RPC.getProxy(DatanodeProtocol.class,
+                           DatanodeProtocol.versionID,
+                           nameAddr2, 
+                           getConf());
+          ((DatanodeProtocols)namenode).setDatanodeProtocol(namenode2, 1);
+        }
+        if (avatarnode2 == null) {
+          avatarnode2 = (AvatarProtocol) 
+                           RPC.getProxy(AvatarProtocol.class,
+                             AvatarProtocol.versionID,
+                           avatarAddr2, 
+                           getConf());
+        }
+        nsInfo = handshake(namenode2, nameAddr2);
+      } catch(ConnectException se) {  // namenode has not been started
+        LOG.info("Server at " + nameAddr2 + " not available yet, Zzzzz...");
+      } catch(SocketTimeoutException te) {  // namenode is busy
+        LOG.info("Problem connecting to server timeout. " + nameAddr2);
+      }
+    } while (doWait && nsInfo == null);
+    return nsInfo;
+  }
+
+  private NamespaceInfo handshake(DatanodeProtocol node,
+                                  InetSocketAddress machine) throws IOException {
+    NamespaceInfo nsInfo = new NamespaceInfo();
+    while (shouldRun) {
+      try {
+        nsInfo = node.versionRequest();
+        break;
+      } catch(SocketTimeoutException e) {  // namenode is busy
+        LOG.info("Problem connecting to server: " + machine);
+        try {
+          Thread.sleep(1000);
+        } catch (InterruptedException ie) {}
+      }
+    }
+    String errorMsg = null;
+    // do not fail on incompatible build version
+    if( ! nsInfo.getBuildVersion().equals( Storage.getBuildVersion() )) {
+      errorMsg = "Incompatible build versions: namenode BV = " 
+        + nsInfo.getBuildVersion() + "; datanode BV = "
+        + Storage.getBuildVersion();
+      LOG.warn( errorMsg );
+    }
+    if (FSConstants.LAYOUT_VERSION != nsInfo.getLayoutVersion()) {
+      errorMsg = "Data-node and name-node layout versions must be the same."
+                  + "Expected: "+ FSConstants.LAYOUT_VERSION + 
+                  " actual "+ nsInfo.getLayoutVersion();
+      LOG.fatal(errorMsg);
+      try {
+        node.errorReport(dnRegistration,
+                         DatanodeProtocol.NOTIFY, errorMsg );
+      } catch( SocketTimeoutException e ) {  // namenode is busy        
+        LOG.info("Problem connecting to server: " + machine);
+      }
+      throw new IOException(errorMsg);
+    }
+    return nsInfo;
+  }
+
+  /**
+   * Returns true if we are able to successfully register with namenode
+   */
+  boolean register(DatanodeProtocol node, InetSocketAddress machine) 
+    throws IOException {
+    if (dnRegistration.getStorageID().equals("")) {
+      setNewStorageID(dnRegistration);
+    }
+
+    DatanodeRegistration tmp = new DatanodeRegistration(dnRegistration.getName());
+    tmp.setInfoPort(dnRegistration.getInfoPort());
+    tmp.setIpcPort(dnRegistration.getIpcPort());
+    tmp.setStorageInfo(storage);
+
+    // reset name to machineName. Mainly for web interface.
+    tmp.name = machineName + ":" + dnRegistration.getPort();
+    try {
+      tmp = node.register(tmp);
+      // if we successded registering for the first time, then we update
+      // the global registration objct
+      if (!doneRegister1 && !doneRegister2) {
+        dnRegistration = tmp;
+      }
+    } catch(SocketTimeoutException e) {  // namenode is busy
+      LOG.info("Problem connecting to server: " + machine);
+      return false;
+    }
+
+    assert ("".equals(storage.getStorageID()) 
+            && !"".equals(dnRegistration.getStorageID()))
+            || storage.getStorageID().equals(dnRegistration.getStorageID()) :
+            "New storageID can be assigned only if data-node is not formatted";
+    if (storage.getStorageID().equals("")) {
+      storage.setStorageID(dnRegistration.getStorageID());
+      storage.writeAll();
+      LOG.info("New storage id " + dnRegistration.getStorageID()
+          + " is assigned to data-node " + dnRegistration.getName());
+    }
+    if(! storage.getStorageID().equals(dnRegistration.getStorageID())) {
+      throw new IOException("Inconsistent storage IDs. Name-node returned "
+          + dnRegistration.getStorageID() 
+          + ". Expecting " + storage.getStorageID());
+    }
+    return  true;
+  }
+
+  @Override
+  public void run() {
+    LOG.info(dnRegistration + "In AvatarDataNode.run, data = " + data);
+
+    // start dataXceiveServer
+    dataXceiverServer.start();
+
+    while (shouldRun) {
+      try {
+
+        // try handshaking with any namenode that we have not yet tried
+        handshake(false);
+
+        if (namenode1 != null && !doneRegister1 &&
+            register(namenode1, nameAddr1)) {
+          doneRegister1 = true;
+          offerService1 = new OfferService(this, namenode1, nameAddr1, 
+                                           avatarnode1, avatarAddr1);
+          of1 = new Thread(offerService1, "OfferService1 " + nameAddr1);
+          of1.start();
+        }
+        if (namenode2 != null && !doneRegister2 &&
+            register(namenode2, nameAddr2)) {
+          doneRegister2 = true;
+          offerService2 = new OfferService(this, namenode2, nameAddr2,
+                                           avatarnode2, avatarAddr2);
+          of2 = new Thread(offerService2, "OfferService2 " + nameAddr2);
+          of2.start();
+        }
+
+        startDistributedUpgradeIfNeeded();
+
+        // start block scanner
+        if (blockScanner != null && blockScannerThread == null &&
+            upgradeManager.isUpgradeCompleted()) {
+          LOG.info("Starting Periodic block scanner.");
+          blockScannerThread = new Daemon(blockScanner);
+          blockScannerThread.start();
+        }
+      } catch (Exception ex) {
+        LOG.error("Exception: " + StringUtils.stringifyException(ex));
+      }
+      if (shouldRun) {
+        try {
+          Thread.sleep(5000);
+        } catch (InterruptedException ie) {
+        }
+      }
+    }
+
+    LOG.info(dnRegistration + ":Finishing AvatarDataNode in: "+data);
+    shutdown();
+  }
+
+  /**
+   * Notify both namenode(s) that we have received a block
+   */
+  @Override
+  protected void notifyNamenodeReceivedBlock(Block block, String delHint) {
+    if (offerService1 != null) {
+      offerService1.notifyNamenodeReceivedBlock(block, delHint);
+    }
+    if (offerService2 != null) {
+      offerService2.notifyNamenodeReceivedBlock(block, delHint);
+    }
+  }
+
+  /**
+   * Notify both namenode(s) that we have deleted a block
+   */
+  void removeReceivedBlocks(Block[] list) {
+    if (offerService1 != null) {
+      offerService1.removeReceivedBlocks(list);
+    }
+    if (offerService2 != null) {
+      offerService2.removeReceivedBlocks(list);
+    }
+  }
+
+  /**
+   * Start distributed upgrade if it should be initiated by the data-node.
+   */
+  private void startDistributedUpgradeIfNeeded() throws IOException {
+    UpgradeManagerDatanode um = DataNode.getDataNode().upgradeManager;
+    assert um != null : "DataNode.upgradeManager is null.";
+    if(!um.getUpgradeState())
+      return;
+    um.setUpgradeState(false, um.getUpgradeVersion());
+    um.startUpgrade();
+    return;
+  }
+
+  void transferBlocks(Block blocks[], DatanodeInfo xferTargets[][]) {
+    for (int i = 0; i < blocks.length; i++) {
+      try {
+        transferBlockMethod.invoke(this, blocks[i], xferTargets[i]);
+      } catch (java.lang.IllegalAccessException ie) {
+        LOG.warn("Failed to transfer block " + blocks[i], ie);
+      } catch (java.lang.reflect.InvocationTargetException ie) {
+        LOG.warn("Failed to transfer block " + blocks[i], ie);
+      }
+    }
+  }
+
+ /**
+   * Shut down this instance of the datanode.
+   * Returns only after shutdown is complete.
+   * This can be called from one of the Offer services thread
+   * or from the DataNode main thread.
+   */
+  @Override
+  public synchronized void shutdown() {
+    if (of1 != null && Thread.currentThread() != of1) {
+      offerService1.stop();
+      try {
+        of1.join();
+      } catch (InterruptedException ie) {
+      }
+    }
+    if (of2 != null && Thread.currentThread() != of2) {
+      offerService2.stop();
+      try {
+        of2.join();
+      } catch (InterruptedException ie) {
+      }
+    }
+    if (infoServer != null) {
+      try {
+        infoServer.stop();
+      } catch (Exception e) {
+        LOG.warn("Exception shutting down DataNode", e);
+      }
+    }
+    super.shutdown();
+    if (storage != null) {
+      try {
+        this.storage.unlockAll();
+      } catch (IOException ie) {
+      }
+    }
+    if (dataNodeThread != null) {
+      dataNodeThread.interrupt();
+      try {
+        dataNodeThread.join();
+      } catch (InterruptedException ie) {
+      }
+    }
+  }
+
+  public static void runDatanodeDaemon(AvatarDataNode dn) throws IOException {
+    if (dn != null) {
+      dn.dataNodeThread = new Thread(dn, dnThreadName);
+      dn.dataNodeThread.setDaemon(true); // needed for JUnit testing
+      dn.dataNodeThread.start();
+    }
+  }
+
+  void join() {
+    if (dataNodeThread != null) {
+      try {
+        dataNodeThread.join();
+      } catch (InterruptedException e) {}
+    }
+  }
+
+  DataStorage getStorage() {
+    return storage;
+  }
+
+  private static void printUsage() {
+    System.err.println("Usage: java DataNode");
+    System.err.println("           [-rollback]");
+  }
+
+  /**
+   * Parse and verify command line arguments and set configuration parameters.
+   *
+   * @return false if passed argements are incorrect
+   */
+  private static boolean parseArguments(String args[],
+                                        Configuration conf) {
+    int argsLen = (args == null) ? 0 : args.length;
+    StartupOption startOpt = StartupOption.REGULAR;
+    for(int i=0; i < argsLen; i++) {
+      String cmd = args[i];
+      if ("-r".equalsIgnoreCase(cmd) || "--rack".equalsIgnoreCase(cmd)) {
+        LOG.error("-r, --rack arguments are not supported anymore. RackID " +
+            "resolution is handled by the NameNode.");
+        System.exit(-1);
+      } else if ("-rollback".equalsIgnoreCase(cmd)) {
+        startOpt = StartupOption.ROLLBACK;
+      } else if ("-regular".equalsIgnoreCase(cmd)) {
+        startOpt = StartupOption.REGULAR;
+      } else
+        return false;
+    }
+    setStartupOption(conf, startOpt);
+    return true;
+  }
+
+  private static void setStartupOption(Configuration conf, StartupOption opt) {
+    conf.set("dfs.datanode.startup", opt.toString());
+  }
+
+  /**
+   * Returns the IP address of the namenode
+   */
+  private static InetSocketAddress getNameNodeAddress(Configuration conf,
+                                                      String cname, String cname2) {
+    String fs = conf.get(cname);
+    String fs2 = conf.get(cname2);
+    Configuration newconf = new Configuration(conf);
+    newconf.set("fs.default.name", fs);
+    if (fs2 != null) {
+      newconf.set("dfs.namenode.dn-address", fs2);
+    }
+    return NameNode.getAddress(newconf);
+  }
+
+  @Override
+  public InetSocketAddress getNameNodeAddr() {
+    return NameNode.getAddress(getConf());
+  }
+
+  /**
+   * Returns the IP:port address of the avatar node
+   */
+  private static InetSocketAddress getAvatarNodeAddress(Configuration conf,
+                                                        String cname) {
+    String fs = conf.get(cname);
+    Configuration newconf = new Configuration(conf);
+    newconf.set("fs.default.name", fs);
+    return AvatarNode.getAddress(newconf);
+  }
+
+  public static AvatarDataNode makeInstance(String[] dataDirs, Configuration conf)
+    throws IOException {
+    ArrayList<File> dirs = new ArrayList<File>();
+    for (int i = 0; i < dataDirs.length; i++) {
+      File data = new File(dataDirs[i]);
+      try {
+        DiskChecker.checkDir(data);
+        dirs.add(data);
+      } catch(DiskErrorException e) {
+        LOG.warn("Invalid directory in dfs.data.dir: " + e.getMessage());
+      }
+    }
+    if (dirs.size() > 0)
+      return new AvatarDataNode(conf, dirs);
+    LOG.error("All directories in dfs.data.dir are invalid.");
+    return null;
+  }
+
+  /** Instantiate a single datanode object. This must be run by invoking
+   *  {@link DataNode#runDatanodeDaemon(DataNode)} subsequently. 
+   */
+  public static AvatarDataNode instantiateDataNode(String args[],
+                                      Configuration conf) throws IOException {
+    if (conf == null)
+      conf = new Configuration();
+    if (!parseArguments(args, conf)) {
+      printUsage();
+      return null;
+    }
+    if (conf.get("dfs.network.script") != null) {
+      LOG.error("This configuration for rack identification is not supported" +
+          " anymore. RackID resolution is handled by the NameNode.");
+      System.exit(-1);
+    }
+    String[] dataDirs = conf.getStrings("dfs.data.dir");
+    dnThreadName = "AvatarDataNode: [" +
+                        StringUtils.arrayToString(dataDirs) + "]";
+    return makeInstance(dataDirs, conf);
+  }
+
+  public static AvatarDataNode createDataNode(String args[],
+                                 Configuration conf) throws IOException {
+    AvatarDataNode dn = instantiateDataNode(args, conf);
+    runDatanodeDaemon(dn);
+    return dn;
+  }
+
+  public static void main(String argv[]) {
+    try {
+      StringUtils.startupShutdownMessage(AvatarDataNode.class, argv, LOG);
+      AvatarDataNode avatarnode = createDataNode(argv, null);
+      if (avatarnode != null)
+        avatarnode.join();
+    } catch (Throwable e) {
+      LOG.error(StringUtils.stringifyException(e));
+      System.exit(-1);
+    }
+  }
+
+
+}
Index: src/contrib/highavailability/src/java/org/apache/hadoop/hdfs/server/namenode/AvatarNode.java
===================================================================
--- src/contrib/highavailability/src/java/org/apache/hadoop/hdfs/server/namenode/AvatarNode.java	(revision 0)
+++ src/contrib/highavailability/src/java/org/apache/hadoop/hdfs/server/namenode/AvatarNode.java	(revision 0)
@@ -0,0 +1,972 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hdfs.server.namenode;
+
+import java.io.IOException;
+import java.io.File;
+import java.io.FileOutputStream;
+import java.io.DataOutputStream;
+import java.io.DataInputStream;
+import java.io.FileInputStream;
+import java.net.InetSocketAddress;
+import java.util.Date;
+import java.util.List;
+import java.util.ArrayList;
+import java.util.Iterator;
+import java.util.Collection;
+import java.text.SimpleDateFormat;
+
+import org.apache.hadoop.ipc.*;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.FileUtil;
+import org.apache.hadoop.io.DataOutputBuffer;
+import org.apache.hadoop.hdfs.protocol.FSConstants;
+import org.apache.hadoop.util.ReflectionUtils;
+import org.apache.hadoop.util.StringUtils;
+import org.apache.hadoop.hdfs.protocol.AvatarProtocol;
+import org.apache.hadoop.hdfs.protocol.AvatarConstants.Avatar;
+import org.apache.hadoop.hdfs.protocol.AvatarConstants.StartupOption;
+import org.apache.hadoop.hdfs.protocol.AvatarConstants.InstanceId;
+import org.apache.hadoop.hdfs.protocol.Block;
+import org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration;
+import org.apache.hadoop.hdfs.server.namenode.NameNode;
+import org.apache.hadoop.hdfs.server.namenode.BlocksMap.BlockInfo;
+
+/**
+ * This is an implementation of the AvatarNode, a hot
+ * standby for the NameNode.
+ * This is really cool, believe me!
+ * The AvatarNode has two avatars.. the Standby avatar and the Active
+ * avatar.
+ * 
+ * In the Standby avatar, the AvatarNode is consuming transaction logs
+ * generated by the primary (via a transaction log stored in a shared device).
+ * Typically, the primary Namenode is writing transactions to a NFS filesystem
+ * and the Standby is reading the log from the same NFS filesystem. The 
+ * Standby is also making periodic checkpoints to the primary namenode.
+ * 
+ * A manual command can switch the AvatarNode from the Standby avatar
+ * to the Active avatar. In the Active avatar, the AvatarNode performs precisely
+ * the same functionality as a real usual Namenode. The switching from 
+ * Standby avatar to the Active avatar is fast and can typically occur 
+ * within seconds.
+ *
+ * Typically, an adminstrator will run require two shared mount points for
+ * transaction logs. It has to be set in fs.name.dir.shared0 and
+ * fs.name.dir.shared1 (similarly for edits). Then the adminstrator starts
+ * the AvatarNode on two different machines as follows:
+ *
+ * bin/hadoop org.apache.hadoop.hdfs.server.namenode.AvatarNode -zero -active
+ * bin/hadoop org.apache.hadoop.hdfs.server.namenode.AvatarNode -one -standby
+ * The first  AvatarNode uses  fs.name.dir.shared0 while the second
+ * AvatarNode uses fs.name.dir.shared1 to write its transaction logs.
+ * Also, at startup, the first instance is the primary Namenode and the
+ * second instance is the Standby
+ *
+ * After a while, the adminstrator decides to change the avatar of the
+ * second instance to Active. In this case, he/she has to first ensure that the
+ * first instance is really really dead. This code does not handle the
+ * split-brain scenario where there are two active namenodes in one cluster.
+ *
+ */
+
+public class AvatarNode extends NameNode implements AvatarProtocol {
+
+  public static final Log LOG = LogFactory.getLog(AvatarNode.class.getName());
+  private static boolean syncAtStartup = false;
+  private static final String STORAGE_FILE_LOCK     = "in_use.lock";
+  private static final String EDITSFILE     = "/current/edits";
+  private static final String EDITSNEW     = "/current/edits.new";
+  private static final String TIMEFILE     = "/current/fstime";
+  private static final String IMAGENEW     ="/current/fsimage.ckpt";
+  static final SimpleDateFormat dateForm =
+    new SimpleDateFormat("yyyy-MM-dd-HH:mm:ss");
+
+  // The instanceId is assigned at startuptime and does not change for
+  // the lifetime of the Node. The adminstrator has to name each instance
+  // of the AvatarNode with a different instanceId. The node number is used 
+  // by the AvaterNode to determine which shared devices it should use to
+  // checkpoint the image.
+  //
+  private static InstanceId instance = InstanceId.NODEZERO;
+
+  // The time when (and if) the fsimage was sync-ed from the remote AvatarNode
+  volatile private static long startCheckpointTime;
+
+  // Should the AvatarNode restart itself?
+  volatile static boolean doRestart;
+
+  private Server server;                   /** RPC server */
+  private InetSocketAddress serverAddress; /** RPC server address */
+  private Avatar currentAvatar;            // the current incarnation of this node
+  private Standby standby;                 // the standby object
+  private Configuration confg;             // config for the standby namenode
+  private Configuration startupConf;       // config for the namenode
+  private Thread standbyThread;            // the standby daemon thread
+
+
+  AvatarNode(Configuration conf) throws IOException {
+    super(conf);
+    initialize(conf);
+  }
+
+  /**
+   * The startup Conf is the original configuration of the AvatarNode. It is used by the
+   * secondary namenode to talk to the primary namenode.
+   * The conf is the modified configuration that is used by the standby namenode
+   */
+  AvatarNode(Configuration startupConf, Configuration conf, Avatar avatar) throws IOException {
+    super(conf);
+    initialize(conf);
+    currentAvatar = avatar;
+    this.startupConf = startupConf;
+    this.confg = conf;
+
+    if (avatar == Avatar.STANDBY) {
+      //
+      // If we are starting as a Hot Standby, then put namenode in 
+      // safemode. This prevents this instance of the NameNode from 
+      // doing active replication of blocks.
+      //
+      setSafeMode(SafeModeAction.SAFEMODE_ENTER);
+
+      // Create a standby object which does the actual work of 
+      // processing transactions from the primary and checkpointing
+      standby = new Standby(this, startupConf, confg); 
+      standbyThread = new Thread(standby);
+      standbyThread.start();
+    }
+  }
+
+  /**
+   * Wait for the StandbyNode to exit. If it does, then stop the underlying namenode.
+   */
+  void waitForRestart() {
+    if (standbyThread != null) {
+      try {
+        // if this is the standby avatarnode, then wait for the Standby to exit
+        standbyThread.join();
+      } catch (InterruptedException ie) {
+        //eat it up
+      }
+      standbyThread = null;
+      LOG.info("waitForRestart Standby thread exited.");
+
+      // if we are still in standbymode, that means we need to restart from scratch.
+      if (getAvatar() == Avatar.STANDBY) {
+        LOG.info("waitForRestart Stopping encapsulated namenode.");
+        super.stop();            // terminate encapsulated namenode
+        super.join();            // wait for encapsulated namenode to exit
+
+        if (server != null) {    // shutdown the AvatarNode
+          LOG.info("waitForRestart Stopping avatarnode rpcserver.");
+          server.stop();
+          try {
+            server.join();
+          } catch (InterruptedException ie) {
+            //eat it up
+          }
+        }
+      }
+      LOG.info("waitForRestart exiting");
+      return;
+    }
+    super.join();            // wait for encapsulated namenode
+
+    // The stop on a RPC Server does not shutdown the handler threads synchronously.
+    // So, we wait for some time before restarting the server.
+    try {
+      Thread.sleep(60000);
+    } catch (InterruptedException e) {
+    }
+  }
+
+  /**
+   * Initialize AvatarNode
+   * @param conf the configuration
+   */
+  private void initialize(Configuration conf) throws IOException {
+    InetSocketAddress socAddr = AvatarNode.getAddress(conf);
+    int handlerCount = conf.getInt("hdfs.avatarnode.handler.count", 3);
+
+    // create rpc server 
+    this.server = RPC.getServer(this, socAddr.getHostName(), 
+                                socAddr.getPort(),
+                                handlerCount, false, conf);
+
+    // The rpc-server port can be ephemeral... ensure we have the 
+    // correct info
+    this.serverAddress = this.server.getListenerAddress();
+    LOG.info("AvatarNode up at: " + this.serverAddress);
+    this.server.start();
+  }
+
+  /**
+   * If the specified protocol is AvatarProtocol, then return the
+   * AvatarProtocol version id, otherwise delegate to the underlying
+   * namenode.
+   */
+  public long getProtocolVersion(String protocol,
+                                 long clientVersion) throws IOException {
+    if (protocol.equals(AvatarProtocol.class.getName())) {
+      return AvatarProtocol.versionID;
+    } else {
+      return super.getProtocolVersion(protocol, clientVersion);
+    } 
+  }
+
+  //
+  // methods to support Avatar Protocol
+  //
+
+  /**
+   * @inheritDoc
+   */
+  public synchronized Avatar getAvatar() {
+    return currentAvatar;
+  }
+
+  /**
+   * @inheritDoc
+   */
+  public synchronized void setAvatar(Avatar avatar) throws IOException {
+    if (avatar == currentAvatar) {
+      LOG.info("Trying to change avatar to " + avatar +
+               " but am already in that state.");
+      return;
+    }
+    if (avatar == Avatar.STANDBY) {   // ACTIVE to STANDBY
+      String msg = "Changing state from active to standby is not allowed." +
+                   "If you really want to pause your primary, put it in safemode.";
+      LOG.warn(msg);
+      throw new IOException(msg);
+    } else {                  // STANDBY to ACTIVE
+      // Check to see if the primary is somehow checkpointing itself. If so, then 
+      // refuse to switch to active mode. This check is not foolproof but is a
+      // defensive mechanism to prevent administrator errors.
+      if (standby.hasStaleCheckpoint()) {
+        String msg = "Failed to change avatar from " + currentAvatar + 
+                     " to " + avatar +
+                     " because the Standby has not yet consumed all transactions.";
+        LOG.warn(msg);
+        throw new IOException(msg);
+      }
+      standby.quiesce();
+      setSafeMode(SafeModeAction.SAFEMODE_LEAVE);
+    }
+    LOG.info("Changed avatar from " + currentAvatar + 
+             " to " + avatar);
+    currentAvatar = avatar;
+  }
+
+  /**
+   * @inheritDoc
+   */
+   public Block[] blockReceivedNew(DatanodeRegistration nodeReg,
+                                   Block blocks[],
+                                   String delHints[]) throws IOException {
+    super.blockReceived(nodeReg, blocks, delHints);
+    List<Block> failed = new ArrayList<Block>(); 
+    for (int i = 0; i < blocks.length; i++) {
+      Block block = blocks[i];
+      synchronized(namesystem) {
+        BlockInfo storedBlock = namesystem.blocksMap.getStoredBlock(block);
+        if (storedBlock == null || storedBlock.getINode() == null) {
+          // If this block does not belong to anyfile, then record it.
+          LOG.info("blockReceived request received for "
+                                   + block + " on " + nodeReg.getName()
+                                   + " size " + block.getNumBytes()
+                                   + " But it does not belong to any file."
+                                   + " Retry later.");
+          failed.add(block);
+        }
+      }
+    }
+    return failed.toArray(new Block[failed.size()]);
+  }
+
+  /**
+   * Returns the hostname:port for the AvatarNode. The default
+   * port for the AvatarNode is one more than the port of the
+   * underlying namenode.
+   */
+  public static InetSocketAddress getAddress(Configuration conf) {
+    InetSocketAddress u = NameNode.getAddress(conf);
+    int port = conf.getInt("dfs.avatarnode.port", u.getPort() + 1);
+    return new InetSocketAddress(u.getHostName(), port);
+  }
+
+ /**
+  * Help message for a user
+  */
+  private static void printUsage() {
+    System.err.println(
+      "Usage: java AvatareNode [" +
+      StartupOption.STANDBY.getName() + "] | [" +
+      StartupOption.SYNC.getName() + "] | [" +
+      StartupOption.NODEZERO.getName() + "] | [" +
+      StartupOption.NODEONE.getName() + "] | [" +
+      StartupOption.FORMAT.getName() + "] | [" +
+      StartupOption.UPGRADE.getName() + "] | [" +
+      StartupOption.ROLLBACK.getName() + "] | [" +
+      StartupOption.FINALIZE.getName() + "] | [" +
+      StartupOption.IMPORT.getName() + "]");
+  }
+
+  /**
+   * validates command line arguments
+   */
+  static void validateStartupOptions(StartupOption startOpt) throws IOException {
+    // sync cannot be specified along with format or finalize
+    if (syncAtStartup) {
+      if (startOpt == StartupOption.FORMAT ||
+          startOpt == StartupOption.FINALIZE) {
+        String msg = "Option " + StartupOption.SYNC +
+                     " cannot be specified along with " +
+                     startOpt;
+        LOG.warn(msg);
+        throw new IOException(msg);
+      }
+    }
+  }
+
+  /**
+   * Analyze the command line options
+   */
+  private static StartupOption parseArguments(String args[]) {
+    int argsLen = (args == null) ? 0 : args.length;
+    StartupOption startOpt = StartupOption.ACTIVE;
+    for (int i=0; i < argsLen; i++) {
+      String cmd = args[i];
+      if (StartupOption.STANDBY.getName().equalsIgnoreCase(cmd)) {
+        startOpt = StartupOption.STANDBY;
+      } else if (StartupOption.SYNC.getName().equalsIgnoreCase(cmd)) {
+        syncAtStartup = true;
+      } else if (StartupOption.NODEZERO.getName().equalsIgnoreCase(cmd)) {
+        instance = InstanceId.NODEZERO;
+      } else if (StartupOption.NODEONE.getName().equalsIgnoreCase(cmd)) {
+        instance = InstanceId.NODEONE;
+      } else if (StartupOption.FORMAT.getName().equalsIgnoreCase(cmd)) {
+        startOpt = StartupOption.FORMAT;
+      } else if (StartupOption.REGULAR.getName().equalsIgnoreCase(cmd)) {
+        startOpt = StartupOption.REGULAR;
+      } else if (StartupOption.UPGRADE.getName().equalsIgnoreCase(cmd)) {
+        startOpt = StartupOption.UPGRADE;
+      } else if (StartupOption.ROLLBACK.getName().equalsIgnoreCase(cmd)) {
+        startOpt = StartupOption.ROLLBACK;
+      } else if (StartupOption.FINALIZE.getName().equalsIgnoreCase(cmd)) {
+        startOpt = StartupOption.FINALIZE;
+      } else if (StartupOption.IMPORT.getName().equalsIgnoreCase(cmd)) {
+        startOpt = StartupOption.IMPORT;
+      } else {
+        return null;
+      }
+    }
+    return startOpt;
+  }
+
+  /**
+   * Records the startup command in the configuration
+   */
+  private static void setStartupOption(Configuration conf, StartupOption opt) {
+    conf.set("dfs.avatarnode.startup", opt.toString());
+  }
+
+  public static AvatarNode createAvatarNode(String argv[],
+                                 Configuration conf) throws IOException {
+    if (conf == null) {
+      conf = new Configuration();
+    }
+    Configuration startupConf = conf;   // save configuration at startup
+    StartupOption startOpt = parseArguments(argv);
+    if (startOpt == null) {
+      printUsage();
+      return null;
+    }
+    setStartupOption(conf, startOpt);
+
+    // sync cannot be specified along with format or finalize
+    validateStartupOptions(startOpt);
+
+    // If sync is requested, then we copy only the fsimage
+    //  (and not the transaction logs) from the other node. 
+    // If we are NODEONE, then modify the configuration to 
+    // set fs.name.dir, fs.default.name and dfs.http.address.
+    //
+    conf = copyFsImage(startupConf);
+
+    // namenode options.
+    switch (startOpt) {
+      case FORMAT:
+        boolean aborted = format(conf, true);
+        System.exit(aborted ? 1 : 0);
+      case FINALIZE:
+        aborted = finalize(conf, true);
+        System.exit(aborted ? 1 : 0);
+      default:
+    }
+
+    return new AvatarNode(startupConf, conf, startOpt.toAvatar());
+  }
+
+  /**
+   * Return the configuration that should be used by this instance of AvatarNode
+   * Copy fsimages from the remote shared device. 
+   */
+  static Configuration copyFsImage(Configuration conf)
+    throws IOException {
+    String img0 = conf.get("dfs.name.dir.shared0");
+    String img1 = conf.get("dfs.name.dir.shared1");
+    String edit0 = conf.get("dfs.name.edits.dir.shared0");
+    String edit1 = conf.get("dfs.name.edits.dir.shared1");
+    Collection<String> namedirs = conf.getStringCollection("dfs.name.dir");
+    Collection<String> editsdir = conf.getStringCollection("dfs.name.edits.dir");
+    String msg = "";
+
+    if (img0 == null || img0.isEmpty()) {
+      msg += "No values specified in dfs.name.dir.share0";
+    }
+    if (img1 == null || img1.isEmpty()) {
+      msg += " No values specified in dfs.name.dir.share1";
+    }
+    if (edit0 == null || edit0.isEmpty()) {
+      msg += " No values specified in dfs.name.edits.dir.share0";
+    }
+    if (edit1 == null || edit1.isEmpty()) {
+      msg += " No values specified in dfs.name.edits.dir.share1";
+    }
+    if (msg.length() != 0) {
+      LOG.info(msg);
+      throw new IOException(msg);
+    }
+
+    // verify that the shared dirctories are not specified as dfs.name.dir
+    for (String str : namedirs) {
+      if (str.equalsIgnoreCase(img0)) {
+        msg = "The name specified in dfs.name.dir.shared0 " +
+              img0 + " is already part of dfs.name.dir ";
+      }
+      if (str.equalsIgnoreCase(img1)) {
+        msg += " The name specified in dfs.name.dir.shared1 " +
+              img1 + " is already part of dfs.name.dir ";
+      }
+    }
+    if (msg.length() != 0) {
+      LOG.info(msg);
+      throw new IOException(msg);
+    }
+    // verify that the shared edits directories are not specified as dfs.name.edits.dir
+    for (String str : editsdir) {
+      if (str.equalsIgnoreCase(edit0)) {
+        msg = "The name specified in dfs.name.edits.dir.shared0 " +
+              img0 + " is already part of dfs.name.dir ";
+      }
+      if (str.equalsIgnoreCase(edit1)) {
+        msg += " The name specified in dfs.name.edits.dir.shared1 " +
+              img1 + " is already part of dfs.name.dir ";
+      }
+    }
+    if (msg.length() != 0) {
+      LOG.info(msg);
+      throw new IOException(msg);
+    }
+
+    // record the fstime of the checkpoint that we are about to sync from
+    setStartCheckpointTime(conf);
+
+    File primary = new File(img0);
+    File standby = new File(img1);
+    String mdate = dateForm.format(new Date(now()));
+    FileSystem localFs = FileSystem.getLocal(conf).getRaw();
+    File src = null;
+    File dest = null;
+    File srcedit = null;
+    File destedit = null;
+
+    //
+    // if we are instance one then copy from primary to secondary
+    // otherwise copy from secondary to primary.
+    //
+    if (instance == InstanceId.NODEONE) {
+      src = primary;
+      dest = standby;
+      srcedit = new File(edit0);
+      destedit = new File(edit1);
+    } else if (instance == InstanceId.NODEZERO) {
+      dest = primary;
+      src = standby;
+      destedit = new File(edit0);
+      srcedit = new File(edit1);
+    }
+
+    // copy fsimage directory if needed
+    if (src.exists() && syncAtStartup) {
+      if (dest.exists()) {
+        File tmp = new File (dest + File.pathSeparator + mdate);
+        if (!dest.renameTo(tmp)) {
+          throw new IOException("Unable to rename " + dest +
+                                " to " +  tmp);
+        }
+        LOG.info("Moved aside " + dest + " as " + tmp);
+      }
+      if (!FileUtil.copy(localFs, new Path(src.toString()), 
+                        localFs, new Path(dest.toString()), 
+                        false, conf)) {
+        msg = "Error copying " + src + " to " + dest;
+        LOG.error(msg);
+        throw new IOException(msg);
+      }
+      LOG.info("Copied " + src + " into " + dest);
+
+      // Remove the lock file from the newly synced directory
+      File lockfile = new File(dest, STORAGE_FILE_LOCK);
+      lockfile.delete();
+
+      // Remove fsimage.ckpt if it exists.
+      File ckptfile = new File(dest.toString() + IMAGENEW);
+      ckptfile.delete();
+
+      // Now, copy from the now-updated shared directory to all other
+      // local dirs specified in fs.name.dir
+      src = dest;
+      if (!namedirs.isEmpty()) {
+        for (String str : namedirs) {
+          dest = new File(str);
+          if (dest.exists()) {
+            File tmp = new File (dest + File.pathSeparator + mdate);
+            if (!dest.renameTo(tmp)) {
+              throw new IOException("Unable to rename " + dest +
+                                    " to " +  tmp);
+            }
+            LOG.info("Moved aside " + dest + " as " + tmp);
+          }
+          if (!FileUtil.copy(localFs, new Path(src.toString()), 
+                             localFs, new Path(dest.toString()), 
+                             false, conf)) {
+            msg = "Error copying " + src + " to " + dest;
+            LOG.error(msg);
+            throw new IOException(msg);
+          }
+          LOG.info("Copied " + src + " into " + dest);
+        }
+      }
+    }
+
+    // copy edits directory if needed
+    if (srcedit.exists() && syncAtStartup) {
+      if (destedit.exists()) {
+        File tmp = new File (destedit + File.pathSeparator + mdate);
+        if (!destedit.renameTo(tmp)) {
+          throw new IOException("Unable to rename " + destedit +
+                                " to " +  tmp);
+        }
+        LOG.info("Moved aside " + destedit + " as " + tmp);
+      }
+      if (!FileUtil.copy(localFs, new Path(srcedit.toString()), 
+                         localFs, new Path(destedit.toString()), 
+                         false, conf)) {
+        msg = "Error copying " + srcedit + " to " + destedit;
+        LOG.error(msg);
+        throw new IOException(msg);
+      }
+      LOG.info("Copied " + srcedit + " into " + destedit);
+
+      // Remove the lock file from the newly synced directory
+      File lockfile = new File(destedit, STORAGE_FILE_LOCK);
+      if (lockfile.delete() == false) {
+        throw new IOException("Unable to delete lock file " + lockfile);
+      }
+
+      // Remove edits and edits.new. Create empty edits file.
+      File efile = new File(destedit.toString() + EDITSFILE);
+      if (efile.delete() == false) {
+        throw new IOException("Unable to delete edits file " + efile);
+      }
+      efile = new File(destedit + EDITSNEW);
+      efile.delete();
+      createEditsFile(destedit.toString());
+
+      // Now, copy from the now-updated shared directory to all other
+      // local dirs specified in fs.name.edits.dir
+      srcedit = destedit;
+      if (!editsdir.isEmpty()) {
+        for (String str : editsdir) {
+          destedit = new File(str);
+          if (destedit.exists()) {
+            File tmp = new File (destedit + File.pathSeparator + mdate);
+            if (!destedit.renameTo(tmp)) {
+              throw new IOException("Unable to rename " + destedit +
+                                    " to " +  tmp);
+            }
+            LOG.info("Moved aside " + destedit + " as " + tmp);
+          }
+          if (!FileUtil.copy(localFs, new Path(srcedit.toString()), 
+                             localFs, new Path(destedit.toString()), 
+                             false, conf)) {
+            msg = "Error copying " + srcedit + " to " + destedit;
+            LOG.error(msg);
+            throw new IOException(msg);
+          }
+          LOG.info("Copied " + srcedit + " into " + destedit);
+        }
+      }
+    }
+
+    // allocate a new configuration and update fs.name.dir approprately
+    // The shared device should be the first in the list.
+    Configuration newconf = new Configuration(conf);
+    StringBuffer buf = new StringBuffer();
+    if (instance == InstanceId.NODEONE) {
+      buf.append(img1);
+    } else if (instance == InstanceId.NODEZERO) {
+      buf.append(img0);
+    }
+    for (String str : namedirs) {
+      buf.append(",");
+      buf.append(str);
+    }
+    newconf.set("dfs.name.dir", buf.toString());
+    buf = null;
+
+    // update fs.name.edits.dir approprately in the new configuration
+    // The shared device should be the first in the list.
+    StringBuffer buf1 = new StringBuffer();
+    if (instance == InstanceId.NODEONE) {
+      buf1.append(edit1);
+    } else if (instance == InstanceId.NODEZERO) {
+      buf1.append(edit0);
+    }
+    for (String str : editsdir) {
+      buf1.append(",");
+      buf1.append(str);
+    }
+    newconf.set("dfs.name.edits.dir", buf1.toString());
+
+    // if we are starting as the other namenode, then change the 
+    // default URL to make the namenode attach to the appropriate URL
+    if (instance == InstanceId.NODEZERO) {
+      String fs = conf.get("fs.default.name0");
+      if (fs != null) {
+        newconf.set("fs.default.name", fs);
+      }
+      fs = conf.get("dfs.http.address0");
+      if (fs != null) {
+        newconf.set("dfs.http.address", fs);
+      }
+      fs = conf.get("dfs.namenode.dn-address0");
+      if (fs != null) {
+        newconf.set("dfs.namenode.dn-address", fs);
+      }
+    }
+    if (instance == InstanceId.NODEONE) {
+      String fs = conf.get("fs.default.name1");
+      if (fs != null) {
+        newconf.set("fs.default.name", fs);
+      }
+      fs = conf.get("dfs.http.address1");
+      if (fs != null) {
+        newconf.set("dfs.http.address", fs);
+      }
+      fs = conf.get("dfs.namenode.dn-address1");
+      if (fs != null) {
+        newconf.set("dfs.namenode.dn-address", fs);
+      }
+    }
+    return newconf;
+  }
+
+  /**
+   * Returns the address of the remote namenode
+   */
+  static InetSocketAddress getRemoteNamenodeAddress(Configuration conf) 
+    throws IOException {
+    String fs = null;
+    if (instance == InstanceId.NODEZERO) {
+      fs = conf.get("fs.default.name1");
+    } else if (instance == InstanceId.NODEONE) {
+      fs = conf.get("fs.default.name0");
+    } else {
+      throw new IOException("Unknown instance " + instance);
+    }
+    if (fs != null) {
+      conf = new Configuration(conf);
+      conf.set("fs.default.name", fs);
+    }
+    return NameNode.getAddress(conf);
+  }
+
+  /**
+   * Returns the name of the http server of the local namenode
+   */
+  static String getRemoteNamenodeHttpName(Configuration conf) 
+    throws IOException {
+    if (instance == InstanceId.NODEZERO) {
+      return conf.get("dfs.http.address1");
+    } else if (instance == InstanceId.NODEONE) {
+      return conf.get("dfs.http.address0");
+    } else {
+      throw new IOException("Unknown instance " + instance);
+    }
+  }
+
+  /**
+   * Create an empty edits log
+   */
+  static void createEditsFile(String editDir) throws IOException {
+    File editfile = new File(editDir + EDITSFILE);
+    FileOutputStream fp = new FileOutputStream(editfile);
+    DataOutputBuffer buf = new DataOutputBuffer(1024);
+    buf.writeInt(FSConstants.LAYOUT_VERSION);
+    buf.writeTo(fp);
+    buf.close();
+    fp.close();
+  }
+
+  /**
+   * Return the edits file of the remote NameNode
+   */
+  File getRemoteEditsFile(Configuration conf) throws IOException {
+    String edit = null;
+    if (instance == InstanceId.NODEZERO) {
+      edit = conf.get("dfs.name.edits.dir.shared1");
+    } else if (instance == InstanceId.NODEONE) {
+      edit = conf.get("dfs.name.edits.dir.shared0");
+    } else {
+      LOG.info("Instance is invalid. " + instance);
+      throw new IOException("Instance is invalid. " + instance);
+    }
+    return new File(edit + EDITSFILE);
+  }
+
+  /**
+   * Return the edits.new file of the remote NameNode
+   */
+  File getRemoteEditsFileNew(Configuration conf) throws IOException {
+    String edit = null;
+    if (instance == InstanceId.NODEZERO) {
+      edit = conf.get("dfs.name.edits.dir.shared1");
+    } else if (instance == InstanceId.NODEONE) {
+      edit = conf.get("dfs.name.edits.dir.shared0");
+    } else {
+      LOG.info("Instance is invalid. " + instance);
+      throw new IOException("Instance is invalid. " + instance);
+    }
+    return new File(edit + EDITSNEW);
+  }
+
+  /**
+   * Return the fstime file of the remote NameNode
+   */
+  File getRemoteTimeFile(Configuration conf) throws IOException {
+    String edit = null;
+    if (instance == InstanceId.NODEZERO) {
+      edit = conf.get("dfs.name.edits.dir.shared1");
+    } else if (instance == InstanceId.NODEONE) {
+      edit = conf.get("dfs.name.edits.dir.shared0");
+    } else {
+      LOG.info("Instance is invalid. " + instance);
+      throw new IOException("Instance is invalid. " + instance);
+    }
+    return new File(edit + TIMEFILE);
+  }
+
+  /**
+   * Reads the timestamp of the last checkpoint from the remote fstime file.
+   */
+  static long readRemoteFstime(Configuration conf) throws IOException {
+    String edit = null;
+    if (instance == InstanceId.NODEZERO) {
+      edit = conf.get("dfs.name.edits.dir.shared1");
+    } else if (instance == InstanceId.NODEONE) {
+      edit = conf.get("dfs.name.edits.dir.shared0");
+    } else {
+      LOG.info("Instance is invalid. " + instance);
+      throw new IOException("Instance is invalid. " + instance);
+    }
+    File timeFile = new File(edit + TIMEFILE);
+    long timeStamp = 0L;
+    DataInputStream in = null;
+    try {
+      in = new DataInputStream(new FileInputStream(timeFile));
+      timeStamp = in.readLong();
+    } catch (IOException e) {
+      if (!timeFile.exists()) {
+        String msg = "Error reading checkpoint time file " + timeFile +
+                     " file does not exist.";
+        LOG.error(msg);
+        throw new IOException(msg + e);
+      } else if (!timeFile.canRead()) {
+        String msg = "Error reading checkpoint time file " + timeFile +
+                     " cannot read file of size " + timeFile.length() +
+                     " last modified " + 
+                     dateForm.format(new Date(timeFile.lastModified()));
+        LOG.error(msg);
+        throw new IOException(msg + e);
+      } else {
+        String msg = "Error reading checkpoint time file " + timeFile;
+        LOG.error(msg);
+        throw new IOException(msg + e);
+      }
+    } finally {
+      if (in != null) {
+        in.close();
+      }
+    }
+    return timeStamp;
+  }
+
+  /**
+   * Returns the starting checkpoint time of this AvatarNode
+   */
+  static long getStartCheckpointTime() {
+    return startCheckpointTime;
+  }
+
+  /**
+   * Sets the starting checkpoint time of this AvatarNode
+   */
+  static void setStartCheckpointTime(Configuration conf) throws IOException {
+    startCheckpointTime = readRemoteFstime(conf);
+  }
+
+  /**
+   * Indicates that the AvatarNode shoudl restart
+   */
+  static void doRestart() {
+    doRestart = true; 
+  }
+
+  /**
+   * Returns true if both edits and edits.new for the
+   * remote namenode exists.
+   */
+  boolean twoEditsFile(Configuration conf) throws IOException{
+    File f1 = getRemoteEditsFile(conf);
+    File f2 = getRemoteEditsFileNew(conf);
+    return f1.exists() && f2.exists();
+  }
+
+  /**
+   * Returns the size of the edits file for the remote
+   * namenode.
+   */
+  long editSize(Configuration conf) throws IOException{
+    return getRemoteEditsFile(conf).length();
+  }
+
+  /**
+   * Current system time.
+   * @return current time in msec.
+   */
+  static long now() {
+    return System.currentTimeMillis();
+  }
+
+  /**
+   * Verify that configured directories exist, then
+   * Interactively confirm that formatting is desired 
+   * for each existing directory and format them.
+   * 
+   * @param conf
+   * @param isConfirmationNeeded
+   * @return true if formatting was aborted, false otherwise
+   * @throws IOException
+   */
+  private static boolean format(Configuration conf,
+                                boolean isConfirmationNeeded
+                                ) throws IOException {
+    boolean allowFormat = conf.getBoolean("dfs.namenode.support.allowformat", 
+                                          true);
+    if (!allowFormat) {
+      throw new IOException("The option dfs.namenode.support.allowformat is "
+                            + "set to false for this filesystem, so it "
+                            + "cannot be formatted. You will need to set "
+                            + "dfs.namenode.support.allowformat parameter "
+                            + "to true in order to format this filesystem");
+    }
+    Collection<File> dirsToFormat = FSNamesystem.getNamespaceDirs(conf);
+    Collection<File> editDirsToFormat = 
+                 FSNamesystem.getNamespaceEditsDirs(conf);
+    for(Iterator<File> it = dirsToFormat.iterator(); it.hasNext();) {
+      File curDir = it.next();
+      if (!curDir.exists())
+        continue;
+      if (isConfirmationNeeded) {
+        System.err.print("Re-format filesystem in " + curDir +" ? (Y or N) ");
+        if (!(System.in.read() == 'Y')) {
+          System.err.println("Format aborted in "+ curDir);
+          return true;
+        }
+        while(System.in.read() != '\n'); // discard the enter-key
+      }
+    }
+
+    FSNamesystem nsys = new FSNamesystem(new FSImage(dirsToFormat,
+                                         editDirsToFormat), conf);
+    nsys.dir.fsImage.format();
+    return false;
+  }
+
+  private static boolean finalize(Configuration conf,
+                               boolean isConfirmationNeeded
+                               ) throws IOException {
+    Collection<File> dirsToFormat = FSNamesystem.getNamespaceDirs(conf);
+    Collection<File> editDirsToFormat = 
+                               FSNamesystem.getNamespaceEditsDirs(conf);
+    FSNamesystem nsys = new FSNamesystem(new FSImage(dirsToFormat,
+                                         editDirsToFormat), conf);
+    System.err.print(
+        "\"finalize\" will remove the previous state of the files system.\n"
+        + "Recent upgrade will become permanent.\n"
+        + "Rollback option will not be available anymore.\n");
+    if (isConfirmationNeeded) {
+      System.err.print("Finalize filesystem state ? (Y or N) ");
+      if (!(System.in.read() == 'Y')) {
+        System.err.println("Finalize aborted.");
+        return true;
+      }
+      while(System.in.read() != '\n'); // discard the enter-key
+    }
+    nsys.dir.fsImage.finalizeUpgrade();
+    return false;
+  }
+
+  /**
+   */
+  public static void main(String argv[]) throws Exception {
+    do {
+      doRestart = false;
+      try {
+        StringUtils.startupShutdownMessage(AvatarNode.class, argv, LOG);
+        AvatarNode avatarnode = createAvatarNode(argv, null);
+        if (avatarnode != null) {
+          avatarnode.waitForRestart();
+        }
+      } catch (Throwable e) {
+        LOG.error(StringUtils.stringifyException(e));
+        if (doRestart) {
+          LOG.error("AvatarNode restarting...");
+        }
+      }
+    } while (doRestart == true);
+  }
+}
Index: src/contrib/highavailability/src/java/org/apache/hadoop/hdfs/server/namenode/Standby.java
===================================================================
--- src/contrib/highavailability/src/java/org/apache/hadoop/hdfs/server/namenode/Standby.java	(revision 0)
+++ src/contrib/highavailability/src/java/org/apache/hadoop/hdfs/server/namenode/Standby.java	(revision 0)
@@ -0,0 +1,410 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hdfs.server.namenode;
+
+import java.io.IOException;
+import java.io.EOFException;
+import java.io.File;
+import java.io.FileOutputStream;
+import java.io.DataOutputStream;
+import java.io.BufferedInputStream;
+import java.io.FileInputStream;
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.DataInputStream;
+import java.util.Date;
+import java.util.Collection;
+import java.text.SimpleDateFormat;
+import java.lang.Thread;
+import java.net.URI;
+import java.net.InetSocketAddress;
+import java.net.InetAddress;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.FileUtil;
+import org.apache.hadoop.io.*;
+import org.apache.hadoop.ipc.*;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.permission.*;
+import org.apache.hadoop.net.NetUtils;
+import org.apache.hadoop.hdfs.protocol.FSConstants;
+import org.apache.hadoop.hdfs.protocol.Block;
+import org.apache.hadoop.hdfs.protocol.DatanodeID;
+import org.apache.hadoop.util.ReflectionUtils;
+import org.apache.hadoop.util.StringUtils;
+import org.apache.hadoop.hdfs.protocol.AvatarProtocol;
+import org.apache.hadoop.hdfs.protocol.AvatarConstants.Avatar;
+import org.apache.hadoop.hdfs.protocol.AvatarConstants.InstanceId;
+import org.apache.hadoop.hdfs.server.protocol.NamenodeProtocol;
+import org.apache.hadoop.hdfs.server.namenode.NameNode;
+import org.apache.hadoop.hdfs.server.namenode.FSNamesystem;
+import org.apache.hadoop.hdfs.server.namenode.FSImage;
+import org.apache.hadoop.hdfs.server.namenode.FSEditLog;
+import org.apache.hadoop.hdfs.server.namenode.AvatarNode;
+import org.apache.hadoop.hdfs.server.common.Storage;
+import org.apache.hadoop.http.HttpServer;
+
+/**
+ * This class drives the ingest of transaciton logs from primary.
+ * It also implements periodic checkpointing of the  primary namenode.
+ */
+
+public class Standby implements Runnable {
+
+  public static final Log LOG = AvatarNode.LOG;
+  private static final long CHECKPOINT_DELAY = 10000; // 10 seconds
+  private AvatarNode avatarNode;
+  private Configuration confg; // configuration of local standby namenode
+  private Configuration startupConf; // original configuration of AvatarNode
+  private FSImage fsImage; // fsImage of the current namenode.
+  private FSNamesystem fsnamesys; // fsnamesystem of the local standby namenode
+  volatile private Ingest ingest;   // object that processes transaction logs from primary
+  volatile private Thread ingestThread;  // thread that is procesing the transaction log
+  volatile private boolean running;
+
+  //
+  // These are for the Secondary NameNode.
+  //
+  private String fsName;                    // local namenode http name
+  private InetSocketAddress nameNodeAddr;   // remote primary namenode address
+  private NamenodeProtocol primaryNamenode; // remote primary namenode
+  private HttpServer infoServer;
+  private int infoPort;
+  private String infoBindAddress;
+  private long checkpointPeriod;        // in seconds
+  private long checkpointSize;    // size (in MB) of current Edit Log
+  private long lastCheckpointTime;
+  private long earlyScheduledCheckpointTime = Long.MAX_VALUE;
+  private long sleepBetweenErrors;
+  volatile private Thread backgroundThread;  // thread for secondary namenode 
+
+  // The Standby can either be processing transaction logs
+  // from the primary namenode or it could be doing a checkpoint to upload a merged
+  // fsimage to the primary.
+  // The startupConf is the original configuration that was used to start the
+  // AvatarNode. It is used by the secondary namenode to talk to the primary.
+  // The "conf" is the configuration of the local standby namenode.
+  //
+  Standby(AvatarNode avatarNode, Configuration startupConf, Configuration conf) 
+    throws IOException {
+    this.running = true;
+    this.avatarNode = avatarNode;
+    this.confg = conf;
+    this.startupConf = startupConf;
+    this.fsImage = avatarNode.getFSImage();
+    this.fsnamesys = avatarNode.getNamesystem();
+    this.sleepBetweenErrors = startupConf.getInt("hdfs.avatarnode.sleep", 5000);
+    initSecondary(startupConf); // start webserver for secondary namenode
+  }
+
+  public void run() {
+    backgroundThread = Thread.currentThread();
+    while (running) {
+      try {
+        // if the checkpoint periodicity or the checkpoint size has
+        // exceeded the configured parameters, then also we have to checkpoint
+        //
+        long now = AvatarNode.now();
+
+        // Check to see if the primary is somehow checkpointing itself. If so, then 
+        // exit the StandbyNode, we cannot have two daemons checkpointing the same
+        // namespace at the same time
+        if (hasStaleCheckpoint()) {
+          backgroundThread = null;
+          quiesce();
+          break;
+        }
+
+        if (lastCheckpointTime == 0 ||
+            (lastCheckpointTime + 1000 * checkpointPeriod < now) ||
+            (earlyScheduledCheckpointTime < now) ||
+            avatarNode.editSize(confg) > checkpointSize) {
+          // schedule an early checkpoint if this current one fails.
+          earlyScheduledCheckpointTime = now + CHECKPOINT_DELAY;
+          doCheckpoint();
+          earlyScheduledCheckpointTime = Long.MAX_VALUE;
+          lastCheckpointTime = now;
+
+          // set the last expected checkpoint time on the primary.
+          AvatarNode.setStartCheckpointTime(startupConf);
+        }
+
+        // if edit and edits.new both exists, then we schedule a checkpoint
+        // to occur very soon.
+        // Only reschedule checkpoint if it is not scheduled to occur even sooner
+        if (avatarNode.twoEditsFile(startupConf) &&
+                (earlyScheduledCheckpointTime > now + CHECKPOINT_DELAY)) {
+          LOG.warn("Standby: edits and edits.new found, scheduling early checkpoint.");
+          earlyScheduledCheckpointTime = now + CHECKPOINT_DELAY;
+        }
+
+        // if the checkpoint creation has switched off ingesting, then we restart the
+        // ingestion here.
+        if (ingest == null) {
+          ingest = new Ingest(this, confg, avatarNode.getRemoteEditsFile(startupConf));
+          ingestThread = new Thread(ingest);
+          ingestThread.start(); // start thread to process transaction logs
+        }
+        try {
+          Thread.sleep(sleepBetweenErrors);
+        } catch (InterruptedException e) {
+          // give a change to exit this thread, if necessary
+        }
+      } catch (IOException e) {
+        LOG.warn("Standby: encounter exception " + StringUtils.stringifyException(e));
+        try {
+          Thread.sleep(sleepBetweenErrors);
+        } catch (InterruptedException e1) {
+          // give a change to exit this thread, if necessary
+        }
+
+        // since we had an error, we have to cleanup the ingest thread
+        if (ingest != null) {
+          ingest.stop();
+          try {
+            ingestThread.join();
+            LOG.info("Standby: error cleanup Ingest thread exited.");
+          } catch (InterruptedException em) {
+            String msg = "Standby: error cleanup Ingest thread did not exit. " + em;
+            LOG.info(msg);
+            throw new RuntimeException(msg);
+          }
+          ingest = null;
+          ingestThread = null;
+        }
+      }
+    }
+  }
+
+  //
+  // stop checkpointing, read edit and edits.new(if it exists) 
+  // into local namenode
+  //
+  synchronized void quiesce() throws IOException {
+    // have to wait for the main thread to exit here
+    // first stop the main thread before stopping the ingest thread
+    LOG.info("Standby: Quiescing.");
+    running = false;
+    try {
+      if (backgroundThread != null) {
+        backgroundThread.join();
+        backgroundThread = null;
+      }
+    } catch (InterruptedException e) {
+      LOG.info("Standby: quiesce interrupted.");
+      throw new IOException(e.getMessage());
+    }
+    try {
+      if (infoServer != null) {
+        infoServer.stop();
+        infoServer= null;
+      }
+    } catch (Exception e) {
+      LOG.error(StringUtils.stringifyException(e));
+    }
+
+    // Ingest till end of edits log
+    if (ingest == null) {
+      ingest = new Ingest(this, confg, avatarNode.getRemoteEditsFile(startupConf));
+      ingestThread = new Thread(ingest);
+      ingestThread.start(); // start thread to process edits.new
+    }
+    ingest.quiesce(); // process everything till end of transaction log
+    try {
+      ingestThread.join();
+      LOG.info("Standby: quiesce Ingest thread exited.");
+    } catch (InterruptedException e) {
+      LOG.info("Standby: quiesce interrupted.");
+      throw new IOException(e.getMessage());
+    }
+
+    // verify that the entire transaction log was truly consumed
+    if (!ingest.getIngestStatus()) {
+      String emsg = "Standby: quiesce could not successfully ingest transaction log.";
+      LOG.warn(emsg);
+      throw new IOException(emsg);
+    }
+    ingest = null;
+    ingestThread = null;
+
+    // if edits.new exists, then read it in too
+    File editnew = avatarNode.getRemoteEditsFileNew(startupConf);
+    if (editnew.exists()) {
+      ingest = new Ingest(this, confg, editnew);
+      ingestThread = new Thread(ingest);
+      ingestThread.start(); // start thread to process edits.new
+      ingest.quiesce();     // read till end of edits.new
+      try {
+        ingestThread.join(); // wait till ingestion is complete
+        LOG.info("Standby: quiesce Ingest thread for edits.new exited");
+      } catch (InterruptedException e) {
+        LOG.info("Standby: quiesce interrupted.");
+        throw new IOException(e.getMessage());
+      }
+      if (!ingest.getIngestStatus()) {
+        String emsg = "Standby: quiesce could not ingest transaction log for edits.new";
+        LOG.warn(emsg);
+         throw new IOException(emsg);
+      }
+      ingest = null;
+      ingestThread = null;
+    }
+  }
+
+  /**
+   * Check to see if the remote namenode is doing its own checkpointing. This can happen 
+   * when the remote namenode is restarted. This method returns true if the remote 
+   * namenode has done an unexpected checkpoint. This method retrieves the fstime of the
+   * remote namenode and matches it against the fstime of the checkpoint when this
+   * AvatarNode did a full-sync of the edits log.
+   */
+  boolean hasStaleCheckpoint() throws IOException {
+    long remotefsTime = AvatarNode.readRemoteFstime(startupConf);
+    long localfsTime = AvatarNode.getStartCheckpointTime();
+    if (remotefsTime != localfsTime) {
+      LOG.warn("Standby: The remote active namenode might have been restarted.");
+      LOG.warn("Standby: The fstime of checkpoint from which the Standby was created is " +
+               AvatarNode.dateForm.format(new Date(localfsTime)) +
+               " but remote fstime is " + 
+               AvatarNode.dateForm.format(new Date(remotefsTime)));
+      AvatarNode.doRestart();
+      return true;
+    }
+    return false;
+  }
+
+  /**
+   * writes the in memory image of the local namenode to the fsimage
+   * and tnen uploads this image to the primary namenode. The transaction 
+   * log on the primary is purged too.
+   */
+  private void doCheckpoint() throws IOException {
+    // Tell the remote namenode to start logging transactions in a new edit file
+    // Retuns a token that would be used to upload the merged image.
+    LOG.info("Standby: startCheckpoint Roll edits logs of primary namenode " +  nameNodeAddr);
+    CheckpointSignature sig = (CheckpointSignature)primaryNamenode.rollEditLog();
+
+    // Ingest till end of edits log
+    if (ingest == null) {
+      LOG.info("Standby: creating ingest thread to process all transactions.");
+      ingest = new Ingest(this, confg, avatarNode.getRemoteEditsFile(startupConf));
+      ingestThread = new Thread(ingest);
+      ingestThread.start(); // start thread to process edits.new
+    }
+    // make a last pass over the edits and then quit ingestion
+    ingest.quiesce(sig);
+    try {
+      ingestThread.join();
+      LOG.info("Standby: finished quitting ingest thread just before ckpt.");
+    } catch (InterruptedException e) {
+      LOG.info("Standby: quiesce interrupted.");
+      throw new IOException(e.getMessage());
+    }
+    if (!ingest.getIngestStatus()) {
+      ingest = null;
+      ingestThread = null;
+      String emsg = "Standby: doCheckpoint could not ingest transaction log.";
+      emsg += " This is real bad because we do not know how much edits we have consumed.";
+      emsg += " It is better to exit the AvatarNode here.";
+      LOG.error(emsg);
+      throw new RuntimeException(emsg);
+    }
+    ingest = null;
+    ingestThread = null;
+
+    synchronized(fsnamesys) {
+      // roll transaction logs on local namenode
+      LOG.info("Standby: Roll edits logs to local namenode.");
+      fsImage.rollEditLog();
+
+      // save a checkpoint of the current namespace of the local Namenode
+      LOG.info("Standby: Save fsimage on local namenode.");
+      fsImage.saveNamespace(true);
+    }
+
+    // copy image to primary namenode
+    LOG.info("Standby: Upload fsimage to remote namenode.");
+    putFSImage(sig);
+
+    // make transaction to primary namenode to switch edit logs
+    LOG.info("Standby: Roll fsimage on primary namenode.");
+    primaryNamenode.rollFsImage();
+
+    LOG.info("Standby: Checkpoint done. New Image Size: " +
+             fsImage.getFsImageName().length());
+  }
+
+  /**
+   * Initialize the webserver so that the primary namenode can fetch
+   * transaction logs from standby via http.
+   */
+  void initSecondary(Configuration conf) throws IOException {
+
+    nameNodeAddr = AvatarNode.getRemoteNamenodeAddress(conf);
+    this.primaryNamenode =
+        (NamenodeProtocol) RPC.waitForProxy(NamenodeProtocol.class,
+            NamenodeProtocol.versionID, nameNodeAddr, conf);
+
+    fsName = AvatarNode.getRemoteNamenodeHttpName(conf);
+
+    // Initialize other scheduling parameters from the configuration
+    checkpointPeriod = conf.getLong("fs.checkpoint.period", 3600);
+    checkpointSize = conf.getLong("fs.checkpoint.size", 4194304);
+
+    // initialize the webserver for uploading files.
+    String infoAddr = 
+      NetUtils.getServerAddress(conf,
+                                "dfs.secondary.info.bindAddress",
+                                "dfs.secondary.info.port",
+                                "dfs.secondary.http.address");
+    InetSocketAddress infoSocAddr = NetUtils.createSocketAddr(infoAddr);
+    infoBindAddress = infoSocAddr.getHostName();
+    int tmpInfoPort = infoSocAddr.getPort();
+    infoServer = new HttpServer("secondary", infoBindAddress, tmpInfoPort,
+        tmpInfoPort == 0, conf);
+    infoServer.setAttribute("name.system.image", fsImage);
+    this.infoServer.setAttribute("name.conf", conf);
+    infoServer.addInternalServlet("getimage", "/getimage", GetImageServlet.class);
+    infoServer.start();
+
+    // The web-server port can be ephemeral... ensure we have the correct info
+    infoPort = infoServer.getPort();
+    conf.set("dfs.secondary.http.address", infoBindAddress + ":" +infoPort);
+    LOG.info("Secondary Web-server up at: " + infoBindAddress + ":" +infoPort);
+    LOG.warn("Checkpoint Period   :" + checkpointPeriod + " secs " +
+             "(" + checkpointPeriod/60 + " min)");
+    LOG.warn("Log Size Trigger    :" + checkpointSize + " bytes " +
+             "(" + checkpointSize/1024 + " KB)");
+  }
+
+  /**
+   * Copy the new fsimage into the NameNode
+   */
+  private void putFSImage(CheckpointSignature sig) throws IOException {
+    String fileid = "putimage=1&port=" + infoPort +
+      "&machine=" +
+      InetAddress.getLocalHost().getHostAddress() +
+      "&token=" + sig.toString();
+    LOG.info("Standby: Posted URL " + fsName + fileid);
+    TransferFsImage.getFileClient(fsName, fileid, (File[])null);
+  }
+}
Index: src/contrib/highavailability/src/java/org/apache/hadoop/hdfs/server/namenode/Ingest.java
===================================================================
--- src/contrib/highavailability/src/java/org/apache/hadoop/hdfs/server/namenode/Ingest.java	(revision 0)
+++ src/contrib/highavailability/src/java/org/apache/hadoop/hdfs/server/namenode/Ingest.java	(revision 0)
@@ -0,0 +1,720 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hdfs.server.namenode;
+
+import java.io.IOException;
+import java.io.EOFException;
+import java.io.File;
+import java.io.FileOutputStream;
+import java.io.DataOutputStream;
+import java.io.BufferedInputStream;
+import java.io.InputStream;
+import java.io.FileInputStream;
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.DataInputStream;
+import java.io.RandomAccessFile;
+import java.nio.channels.FileChannel;
+import java.lang.Thread;
+import java.util.Date;
+import java.util.Collection;
+import java.text.SimpleDateFormat;
+
+import java.util.concurrent.locks.ReadWriteLock;
+import java.util.concurrent.locks.ReentrantReadWriteLock;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.FileUtil;
+import org.apache.hadoop.io.*;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.permission.*;
+import org.apache.hadoop.hdfs.protocol.FSConstants;
+import org.apache.hadoop.hdfs.protocol.Block;
+import org.apache.hadoop.hdfs.protocol.DatanodeID;
+import org.apache.hadoop.util.ReflectionUtils;
+import org.apache.hadoop.util.StringUtils;
+import org.apache.hadoop.hdfs.protocol.AvatarProtocol;
+import org.apache.hadoop.hdfs.protocol.AvatarConstants.Avatar;
+import org.apache.hadoop.hdfs.protocol.AvatarConstants.InstanceId;
+import org.apache.hadoop.hdfs.server.namenode.AvatarNode;
+import org.apache.hadoop.hdfs.server.namenode.NameNode;
+import org.apache.hadoop.hdfs.server.namenode.FSNamesystem;
+import org.apache.hadoop.hdfs.server.namenode.FSImage;
+import org.apache.hadoop.hdfs.server.namenode.FSEditLog;
+import org.apache.hadoop.hdfs.server.common.Storage;
+
+/**
+ * This class reads transaction logs from the primary's shared device
+ * and feeds it to the standby NameNode.
+ */
+
+public class Ingest implements Runnable {
+
+  public static final Log LOG = AvatarNode.LOG;
+  static final SimpleDateFormat DATE_FORM =
+    new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");
+
+  private Standby standby;
+  private Configuration confg;  // configuration of local namenode
+  private File ingestFile;
+  volatile private boolean running = true;
+  volatile private boolean lastScan = false; // is this the last scan?
+  private CheckpointSignature lastSignature;
+  volatile private boolean success = false;  // not successfully ingested yet
+  RandomAccessFile rp;  // the file to read transactions from.
+  FileInputStream fp;
+  FileChannel fc ;
+  long currentPosition; // current offset in the transaction log
+
+  Ingest(Standby standby, Configuration conf, File edits) throws IOException {
+    this.standby = standby;
+    this.confg = conf;
+    this.ingestFile = edits;
+  }
+
+  public void run() {
+    while (running) {
+      try {
+        // keep ingesting transactions from remote edits log.
+        loadFSEdits(ingestFile);
+      } catch (Exception e) {
+        LOG.warn("Ingest: Exception while processing transactions (" + 
+                 running + ") " + StringUtils.stringifyException(e));
+        throw new RuntimeException("Ingest: failure", e);
+      } finally {
+        LOG.warn("Ingest: Processing transactions has a hiccup. " + running);
+      }
+    }
+    success = true;      // successful ingest.
+  }
+
+  /**
+   * Immediately Stop the ingestion of transaction logs
+   */
+  void stop() {
+    running = false;
+  }
+
+  /**
+   * Indicate that this is the last pass over the transaction log
+   */
+  void quiesce() {
+    lastScan = true;
+  }
+
+  /**
+   * Indicate that this is the last pass over the transaction log
+   * Verify that file modification time of the edits log matches
+   * that of the signature.
+   */
+  synchronized void quiesce(CheckpointSignature sig) {
+    lastSignature = sig;
+    lastScan = true;
+  }
+
+  private synchronized CheckpointSignature getLastCheckpointSignature() {
+    return this.lastSignature;
+  }
+
+  /**
+   * Indicate whether ingest was successful or not.
+   * Returns true on success, else false.
+   */
+  boolean getIngestStatus() {
+    return success;
+  }
+
+  /**
+   * Load an edit log, and continue applying the changes to the in-memory 
+   * structure. This is where we ingest transactions into the standby.
+   */
+  private int loadFSEdits(File edits) throws IOException {
+    FSNamesystem fsNamesys = FSNamesystem.getFSNamesystem();
+    FSDirectory fsDir = fsNamesys.dir;
+    int numEdits = 0;
+    int logVersion = 0;
+    String clientName = null;
+    String clientMachine = null;
+    String path = null;
+    int numOpAdd = 0, numOpClose = 0, numOpDelete = 0,
+        numOpRename = 0, numOpSetRepl = 0, numOpMkDir = 0,
+        numOpSetPerm = 0, numOpSetOwner = 0, numOpSetGenStamp = 0,
+        numOpTimes = 0, numOpOther = 0;
+    long startTime = FSNamesystem.now();
+
+    LOG.info("Ingest: Consuming transactions from file " + edits +
+             " of size " + edits.length());
+    rp = new RandomAccessFile(edits, "r");
+    fp = new FileInputStream(rp.getFD()); // open for reads
+    fc = rp.getChannel();
+
+    DataInputStream in = new DataInputStream(fp);
+    try {
+      // Read log file version. Could be missing. 
+      in.mark(4);
+      // If edits log is greater than 2G, available method will return negative
+      // numbers, so we avoid having to call available
+      boolean available = true;
+      try {
+        logVersion = in.readByte();
+      } catch (EOFException e) {
+        available = false;
+      }
+      if (available) {
+        fc.position(0);          // reset
+        in = new DataInputStream(fp);
+        logVersion = in.readInt();
+        if (logVersion != FSConstants.LAYOUT_VERSION) // future version
+          throw new IOException(
+                          "Ingest: Unexpected version of the file system log file: "
+                          + logVersion + ". Current version = " 
+                          + FSConstants.LAYOUT_VERSION + ".");
+      }
+      assert logVersion <= Storage.LAST_UPGRADABLE_LAYOUT_VERSION :
+                            "Unsupported version " + logVersion;
+      currentPosition = fc.position();
+      numEdits = ingestFSEdits(edits, in, logVersion); // continue to ingest 
+    } finally {
+      LOG.info("Ingest: Closing transactions file " + edits);
+      fp.close();
+    }
+    LOG.info("Ingest: Edits file " + edits.getName() 
+        + " of size " + edits.length() + " edits # " + numEdits 
+        + " loaded in " + (FSNamesystem.now()-startTime)/1000 + " seconds.");
+
+    if (LOG.isDebugEnabled()) {
+      LOG.debug("Ingest: numOpAdd = " + numOpAdd + " numOpClose = " + numOpClose 
+          + " numOpDelete = " + numOpDelete + " numOpRename = " + numOpRename 
+          + " numOpSetRepl = " + numOpSetRepl + " numOpMkDir = " + numOpMkDir
+          + " numOpSetPerm = " + numOpSetPerm 
+          + " numOpSetOwner = " + numOpSetOwner
+          + " numOpSetGenStamp = " + numOpSetGenStamp 
+          + " numOpTimes = " + numOpTimes
+          + " numOpOther = " + numOpOther);
+    }
+
+    if (logVersion != FSConstants.LAYOUT_VERSION) // other version
+      numEdits++; // save this image asap
+    return numEdits;
+  }
+
+  /**
+   * Continue to ingest transaction logs until the currentState is 
+   * no longer INGEST. If lastScan is set to true, then we process 
+   * till the end of the file and return.
+   */
+  int ingestFSEdits(File fname, DataInputStream in, 
+                    int logVersion) throws IOException {
+    FSNamesystem fsNamesys = FSNamesystem.getFSNamesystem();
+    FSDirectory fsDir = fsNamesys.dir;
+    int numEdits = 0;
+    String clientName = null;
+    String clientMachine = null;
+    String path = null;
+    int numOpAdd = 0, numOpClose = 0, numOpDelete = 0,
+        numOpRename = 0, numOpSetRepl = 0, numOpMkDir = 0,
+        numOpSetPerm = 0, numOpSetOwner = 0, numOpSetGenStamp = 0,
+        numOpTimes = 0, numOpOther = 0;
+    long startTime = FSNamesystem.now();
+    boolean error = false;
+    boolean quitAfterScan = false;
+
+    while (running && !quitAfterScan) {
+
+      // if the application requested that we make a final pass over 
+      // the transaction log, then we remember it here. We close and
+      // reopen the file to ensure that we can see all the data in the
+      // file, one reason being that NFS has open-to-close cache
+      // coherancy and the edit log could be stored in NFS.
+      //
+      if (lastScan) {
+        LOG.info("Ingest: Starting last scan of transaction log " + fname);
+        quitAfterScan = true;
+        fp.close();
+        rp = new RandomAccessFile(fname, "r");
+        fp = new FileInputStream(rp.getFD()); // open for reads
+        fc = rp.getChannel();
+
+        // discard older buffers and start a fresh one.
+        fc.position(currentPosition);
+        in = new DataInputStream(fp);
+      }
+
+      //
+      // Verify that signature of file matches. This is imporatant in the
+      // case when the Primary NN was configured to write transactions to 
+      // to devices (local and NFS) and the Primary had encountered errors
+      // to the NFS device and has continued writing transactions to its
+      // device only. In this case, the rollEditLog() RPC would return the
+      // modtime of the edits file of the Primary's local device and will
+      // not match with the timestamp of our local log from where we are
+      // ingesting.
+      //
+      CheckpointSignature signature = getLastCheckpointSignature();
+      if (signature != null) {
+        long localtime = fname.lastModified();
+        if (localtime == signature.editsTime) {
+          LOG.debug("Ingest: Matched modification time of edits log. ");
+        } else if (localtime < signature.editsTime) {
+          LOG.info("Ingest: Timestamp of transaction log on local machine is " +
+                   localtime +
+                   " and on remote namenode is " + signature.editsTime);
+          String msg = "Ingest: Timestamp of transaction log on local machine is " + 
+                       DATE_FORM.format(new Date(localtime)) +
+                       " and on remote namenode is " +
+                       DATE_FORM.format(new Date(signature.editsTime));
+          LOG.info(msg);
+          throw new IOException(msg);
+        } else {
+          LOG.info("Ingest: Timestamp of transaction log on local machine is " +
+                   localtime +
+                   " and on remote namenode is " + signature.editsTime);
+          String msg = "Ingest: Timestamp of transaction log on localmachine is " + 
+                       DATE_FORM.format(new Date(localtime)) +
+                       " and on remote namenode is " +
+                       DATE_FORM.format(new Date(signature.editsTime)) +
+                       ". But this can never happen.";
+          LOG.info(msg);
+          throw new IOException(msg);
+        }
+      }
+
+      //
+      // Process all existing transactions till end of file
+      //
+      while (running) {
+        currentPosition = fc.position(); // record the current file offset.
+
+        try {
+          long timestamp = 0;
+          long mtime = 0;
+          long atime = 0;
+          long blockSize = 0;
+          byte opcode = -1;
+          error = false;
+          try {
+            opcode = in.readByte();
+            if (opcode == OP_INVALID) {
+              FSNamesystem.LOG.debug("Ingest: Invalid opcode, reached end of log " +
+                                     "Number of transactions found " + 
+                                     numEdits);
+              break; // No more transactions.
+            }
+          } catch (EOFException e) {
+            break; // No more transactions.
+          }
+          switch (opcode) {
+          case OP_ADD:
+          case OP_CLOSE: {
+          // versions > 0 support per file replication
+          // get name and replication
+          int length = in.readInt();
+          if (-7 == logVersion && length != 3||
+              -17 < logVersion && logVersion < -7 && length != 4 ||
+              logVersion <= -17 && length != 5) {
+              throw new IOException("Ingest: Incorrect data format."  +
+                                    " logVersion is " + logVersion +
+                                    " but writables.length is " +
+                                    length + ". ");
+          }
+          path = FSImage.readString(in);
+          short replication = readShort(in);
+          mtime = readLong(in);
+          if (logVersion <= -17) {
+            atime = readLong(in);
+          }
+          if (logVersion < -7) {
+            blockSize = readLong(in);
+          }
+          // get blocks
+          Block blocks[] = null;
+          if (logVersion <= -14) {
+            blocks = readBlocks(in);
+          } else {
+            BlockTwo oldblk = new BlockTwo();
+            int num = in.readInt();
+            blocks = new Block[num];
+            for (int i = 0; i < num; i++) {
+              oldblk.readFields(in);
+              blocks[i] = new Block(oldblk.blkid, oldblk.len, 
+                                    Block.GRANDFATHER_GENERATION_STAMP);
+            }
+          }
+
+          // Older versions of HDFS does not store the block size in inode.
+          // If the file has more than one block, use the size of the
+          // first block as the blocksize. Otherwise use the default
+          // block size.
+          if (-8 <= logVersion && blockSize == 0) {
+            if (blocks.length > 1) {
+              blockSize = blocks[0].getNumBytes();
+            } else {
+              long first = ((blocks.length == 1)? blocks[0].getNumBytes(): 0);
+              blockSize = Math.max(fsNamesys.getDefaultBlockSize(), first);
+            }
+          }
+           
+          PermissionStatus permissions = fsNamesys.getUpgradePermission();
+          if (logVersion <= -11) {
+            permissions = PermissionStatus.read(in);
+          }
+
+          // clientname, clientMachine and block locations of last block.
+          if (opcode == OP_ADD && logVersion <= -12) {
+            clientName = FSImage.readString(in);
+            clientMachine = FSImage.readString(in);
+            if (-13 <= logVersion) {
+              readDatanodeDescriptorArray(in);
+            }
+          } else {
+            clientName = "";
+            clientMachine = "";
+          }
+
+          // The open lease transaction re-creates a file if necessary.
+          // Delete the file if it already exists.
+          if (FSNamesystem.LOG.isDebugEnabled()) {
+            FSNamesystem.LOG.debug(opcode + ": " + path + 
+                                   " numblocks : " + blocks.length +
+                                   " clientHolder " +  clientName +
+                                   " clientMachine " + clientMachine);
+          }
+
+          fsDir.unprotectedDelete(path, mtime);
+
+          // add to the file tree
+          INodeFile node = (INodeFile)fsDir.unprotectedAddFile(
+                                                    path, permissions,
+                                                    blocks, replication, 
+                                                    mtime, atime, blockSize);
+          if (opcode == OP_ADD) {
+            numOpAdd++;
+            //
+            // Replace current node with a INodeUnderConstruction.
+            // Recreate in-memory lease record.
+            //
+            INodeFileUnderConstruction cons = new INodeFileUnderConstruction(
+                                      node.getLocalNameBytes(),
+                                      node.getReplication(), 
+                                      node.getModificationTime(),
+                                      node.getPreferredBlockSize(),
+                                      node.getBlocks(),
+                                      node.getPermissionStatus(),
+                                      clientName, 
+                                      clientMachine, 
+                                      null);
+            fsDir.replaceNode(path, node, cons);
+            fsNamesys.leaseManager.addLease(cons.clientName, path);
+            }
+            break;
+          } 
+          case OP_SET_REPLICATION: {
+          numOpSetRepl++;
+          path = FSImage.readString(in);
+          short replication = readShort(in);
+          fsDir.unprotectedSetReplication(path, replication, null);
+          break;
+          } 
+          case OP_RENAME: {
+          numOpRename++;
+          int length = in.readInt();
+          if (length != 3) {
+            throw new IOException("Ingest: Incorrect data format. " 
+                                  + "Mkdir operation.");
+          }
+          String s = FSImage.readString(in);
+          String d = FSImage.readString(in);
+          timestamp = readLong(in);
+          FileStatus dinfo = fsDir.getFileInfo(d);
+          fsDir.unprotectedRenameTo(s, d, timestamp);
+          fsNamesys.changeLease(s, d, dinfo);
+          break;
+          }
+          case OP_DELETE: {
+          numOpDelete++;
+          int length = in.readInt();
+          if (length != 2) {
+            throw new IOException("Ingest: Incorrect data format. " 
+                                  + "delete operation.");
+          }
+          path = FSImage.readString(in);
+          timestamp = readLong(in);
+          fsDir.unprotectedDelete(path, timestamp);
+          break;
+          }
+          case OP_MKDIR: {
+          numOpMkDir++;
+          PermissionStatus permissions = fsNamesys.getUpgradePermission();
+          int length = in.readInt();
+          if (-17 < logVersion && length != 2 ||
+              logVersion <= -17 && length != 3) {
+            throw new IOException("Ingest: Incorrect data format. " 
+                                  + "Mkdir operation.");
+          }
+          path = FSImage.readString(in);
+          timestamp = readLong(in);
+
+          // The disk format stores atimes for directories as well.
+          // However, currently this is not being updated/used because of
+          // performance reasons.
+          if (logVersion <= -17) {
+            atime = readLong(in);
+          }
+
+          if (logVersion <= -11) {
+            permissions = PermissionStatus.read(in);
+          }
+          fsDir.unprotectedMkdir(path, permissions, timestamp);
+          break;
+          }
+          case OP_SET_GENSTAMP: {
+          numOpSetGenStamp++;
+          long lw = in.readLong();
+          fsDir.namesystem.setGenerationStamp(lw);
+          break;
+          } 
+          case OP_DATANODE_ADD: {
+          numOpOther++;
+          FSImage.DatanodeImage nodeimage = new FSImage.DatanodeImage();
+          nodeimage.readFields(in);
+          //Datnodes are not persistent any more.
+          break;
+          }
+          case OP_DATANODE_REMOVE: {
+          numOpOther++;
+          DatanodeID nodeID = new DatanodeID();
+          nodeID.readFields(in);
+          //Datanodes are not persistent any more.
+          break;
+          }
+          case OP_SET_PERMISSIONS: {
+          numOpSetPerm++;
+          if (logVersion > -11)
+            throw new IOException("Ingest: Unexpected opcode " + opcode
+                                  + " for version " + logVersion);
+          fsDir.unprotectedSetPermission(
+              FSImage.readString(in), FsPermission.read(in));
+          break;
+          }
+          case OP_SET_OWNER: {
+          numOpSetOwner++;
+          if (logVersion > -11)
+            throw new IOException("Ingest: Unexpected opcode " + opcode
+                                  + " for version " + logVersion);
+          fsDir.unprotectedSetOwner(FSImage.readString(in),
+              FSImage.readString_EmptyAsNull(in),
+              FSImage.readString_EmptyAsNull(in));
+          break;
+          }
+          case OP_SET_NS_QUOTA: {
+          if (logVersion > -16) {
+            throw new IOException("Ingest: Unexpected opcode " + opcode
+                + " for version " + logVersion);
+          }
+          fsDir.unprotectedSetQuota(FSImage.readString(in), 
+                                    readLongWritable(in), 
+                                    FSConstants.QUOTA_DONT_SET);
+          break;
+          }
+          case OP_CLEAR_NS_QUOTA: {
+          if (logVersion > -16) {
+            throw new IOException("Ingest: Unexpected opcode " + opcode
+                + " for version " + logVersion);
+          }
+          fsDir.unprotectedSetQuota(FSImage.readString(in),
+                                    FSConstants.QUOTA_RESET,
+                                    FSConstants.QUOTA_DONT_SET);
+          break;
+          }
+
+          case OP_SET_QUOTA:
+          fsDir.unprotectedSetQuota(FSImage.readString(in),
+                                    readLongWritable(in),
+                                    readLongWritable(in));
+                                      
+          break;
+
+          case OP_TIMES: {
+          numOpTimes++;
+          int length = in.readInt();
+          if (length != 3) {
+            throw new IOException("Ingest: Incorrect data format. " 
+                                  + "times operation.");
+          }
+          path = FSImage.readString(in);
+          mtime = readLong(in);
+          atime = readLong(in);
+          fsDir.unprotectedSetTimes(path, mtime, atime, true);
+          break;
+          }
+          default: {
+          throw new IOException("Ingest: Never seen opcode " + opcode);
+          }
+          }
+          numEdits++;
+          LOG.info("Ingest: Processed transaction from " + fname + " opcode " + opcode +
+                   " file offset " + currentPosition);
+        }
+        catch (IOException e) {
+          error = true; // if we haven't reached eof, then error.
+          break;
+        }
+      }
+   
+      // if we failed to read the entire transaction from disk, 
+      // then roll back to the offset where there was a last good 
+      // read, sleep for sometime for new transaction to
+      // appear in the file and then continue;
+      //
+      if (error || running) {
+
+        // discard older buffers and start a fresh one.
+        fc.position(currentPosition);
+        in = new DataInputStream(fp);
+
+        if (error) {
+          LOG.info("Ingest: Incomplete transaction record at offset " + 
+                   fc.position() +
+                   " but the file is of size " + fc.size() + 
+                   ". Continuing....");
+        }
+
+        if (running && !lastScan) {
+          try {
+            Thread.sleep(1000); // sleep for a second
+          } catch (InterruptedException e) {
+            // break out of waiting if we receive an interrupt.
+          }
+        }
+      }
+    }
+    LOG.info("Ingest: Edits file " + fname.getName() +
+      " numedits " + numEdits +
+      " loaded in " + (FSNamesystem.now()-startTime)/1000 + " seconds.");
+
+    // If the last Scan was completed, then stop the Ingest thread.
+    if (lastScan && quitAfterScan) {
+      LOG.info("Ingest: lastScan completed.");
+      running = false;
+    }
+    return numEdits; // total transactions consumed
+  }
+
+  // a place holder for reading a long
+  private static final LongWritable longWritable = new LongWritable();
+
+  /** Read an integer from an input stream */
+  private static long readLongWritable(DataInputStream in) throws IOException {
+    synchronized (longWritable) {
+      longWritable.readFields(in);
+      return longWritable.get();
+    }
+  }
+
+  /**
+   * A class to read in blocks stored in the old format. The only two
+   * fields in the block were blockid and length.
+   */
+  static class BlockTwo implements Writable {
+    long blkid;
+    long len;
+
+    static {                                      // register a ctor
+      WritableFactories.setFactory
+        (BlockTwo.class,
+         new WritableFactory() {
+           public Writable newInstance() { return new BlockTwo(); }
+         });
+    }
+
+
+    BlockTwo() {
+      blkid = 0;
+      len = 0;
+    }
+
+    /////////////////////////////////////
+    // Writable
+    /////////////////////////////////////
+    public void write(DataOutput out) throws IOException {
+      out.writeLong(blkid);
+      out.writeLong(len);
+    }
+
+    public void readFields(DataInput in) throws IOException {
+      this.blkid = in.readLong();
+      this.len = in.readLong();
+    }
+  }
+
+  //
+  // These methods are copied from FsEdits/FsImage because they are defined as
+  // private scope in those files.
+  //
+  static private DatanodeDescriptor[] readDatanodeDescriptorArray(DataInput in
+      ) throws IOException {
+    DatanodeDescriptor[] locations = new DatanodeDescriptor[in.readInt()];
+    for (int i = 0; i < locations.length; i++) {
+      locations[i] = new DatanodeDescriptor();
+      locations[i].readFieldsFromFSEditLog(in);
+    }
+    return locations;
+  }
+
+  static private short readShort(DataInputStream in) throws IOException {
+    return Short.parseShort(FSImage.readString(in));
+  }
+
+  static private long readLong(DataInputStream in) throws IOException {
+    return Long.parseLong(FSImage.readString(in));
+  }
+
+  static private Block[] readBlocks(DataInputStream in) throws IOException {
+    int numBlocks = in.readInt();
+    Block[] blocks = new Block[numBlocks];
+    for (int i = 0; i < numBlocks; i++) {
+      blocks[i] = new Block();
+      blocks[i].readFields(in);
+    }
+    return blocks;
+  }
+
+  private static final byte OP_INVALID = -1;
+  private static final byte OP_ADD = 0;
+  private static final byte OP_RENAME = 1;  // rename
+  private static final byte OP_DELETE = 2;  // delete
+  private static final byte OP_MKDIR = 3;   // create directory
+  private static final byte OP_SET_REPLICATION = 4; // set replication
+  //the following two are used only for backward compatibility :
+  @Deprecated private static final byte OP_DATANODE_ADD = 5;
+  @Deprecated private static final byte OP_DATANODE_REMOVE = 6;
+  private static final byte OP_SET_PERMISSIONS = 7;
+  private static final byte OP_SET_OWNER = 8;
+  private static final byte OP_CLOSE = 9;    // close after write
+  private static final byte OP_SET_GENSTAMP = 10;    // store genstamp
+  /* The following two are not used any more. Should be removed once
+   * LAST_UPGRADABLE_LAYOUT_VERSION is -17 or newer. */
+  private static final byte OP_SET_NS_QUOTA = 11; // set namespace quota
+  private static final byte OP_CLEAR_NS_QUOTA = 12; // clear namespace quota
+  private static final byte OP_TIMES = 13; // sets mod & access time on a file
+  private static final byte OP_SET_QUOTA = 14; // sets name and disk quotas.
+}
Index: src/contrib/highavailability/src/java/org/apache/hadoop/hdfs/AvatarClient.java
===================================================================
--- src/contrib/highavailability/src/java/org/apache/hadoop/hdfs/AvatarClient.java	(revision 0)
+++ src/contrib/highavailability/src/java/org/apache/hadoop/hdfs/AvatarClient.java	(revision 0)
@@ -0,0 +1,3370 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hdfs;
+
+import org.apache.hadoop.io.*;
+import org.apache.hadoop.io.retry.RetryPolicies;
+import org.apache.hadoop.io.retry.RetryPolicy;
+import org.apache.hadoop.io.retry.RetryProxy;
+import org.apache.hadoop.fs.*;
+import org.apache.hadoop.fs.permission.FsPermission;
+import org.apache.hadoop.ipc.*;
+import org.apache.hadoop.net.NetUtils;
+import org.apache.hadoop.net.NodeBase;
+import org.apache.hadoop.conf.*;
+import org.apache.hadoop.hdfs.DistributedFileSystem.DiskStatus;
+import org.apache.hadoop.hdfs.protocol.*;
+import org.apache.hadoop.hdfs.server.common.HdfsConstants;
+import org.apache.hadoop.hdfs.server.common.UpgradeStatusReport;
+import org.apache.hadoop.hdfs.server.datanode.DataNode;
+import org.apache.hadoop.hdfs.server.namenode.NameNode;
+import org.apache.hadoop.hdfs.server.namenode.NotReplicatedYetException;
+import org.apache.hadoop.security.AccessControlException;
+import org.apache.hadoop.security.UnixUserGroupInformation;
+import org.apache.hadoop.util.*;
+
+import org.apache.commons.logging.*;
+
+import java.io.*;
+import java.net.*;
+import java.util.*;
+import java.util.zip.CRC32;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.ConcurrentHashMap;
+import java.nio.BufferOverflowException;
+import java.nio.ByteBuffer;
+
+import javax.net.SocketFactory;
+import javax.security.auth.login.LoginException;
+
+/********************************************************
+ * AvatarClient can connect to a AvatarNode and 
+ * perform basic file tasks.  It uses the AvatarProtocol
+ * to communicate with a AvatarNode daemon
+ *
+ ********************************************************/
+public class AvatarClient implements FSConstants, java.io.Closeable {
+  public static final Log LOG = LogFactory.getLog(AvatarClient.class);
+  public static final int MAX_BLOCK_ACQUIRE_FAILURES = 3;
+  private static final int TCP_WINDOW_SIZE = 128 * 1024; // 128 KB
+  public final AvatarProtocol namenode;
+  private final AvatarProtocol rpcNamenode;
+  final UnixUserGroupInformation ugi;
+  volatile boolean clientRunning = true;
+  Random r = new Random();
+  final String clientName;
+  final LeaseChecker leasechecker = new LeaseChecker();
+  private Configuration conf;
+  private long defaultBlockSize;
+  private short defaultReplication;
+  private SocketFactory socketFactory;
+  private int socketTimeout;
+  private int datanodeWriteTimeout;
+  final int writePacketSize;
+  private final FileSystem.Statistics stats;
+  private int maxBlockAcquireFailures;
+    
+ 
+  public static AvatarProtocol createNamenode(Configuration conf) throws IOException {
+    return createNamenode(NameNode.getAddress(conf), conf);
+  }
+
+  public static AvatarProtocol createNamenode( InetSocketAddress nameNodeAddr,
+      Configuration conf) throws IOException {
+    try {
+      return createNamenode(createRPCNamenode(nameNodeAddr, conf,
+        UnixUserGroupInformation.login(conf, true)));
+    } catch (LoginException e) {
+      throw (IOException)(new IOException().initCause(e));
+    }
+  }
+
+  private static AvatarProtocol createRPCNamenode(InetSocketAddress nameNodeAddr,
+      Configuration conf, UnixUserGroupInformation ugi) 
+    throws IOException {
+    return (AvatarProtocol)RPC.getProxy(AvatarProtocol.class,
+        AvatarProtocol.versionID, nameNodeAddr, ugi, conf,
+        NetUtils.getSocketFactory(conf, AvatarProtocol.class));
+  }
+
+  private static AvatarProtocol createNamenode(AvatarProtocol rpcNamenode)
+    throws IOException {
+    RetryPolicy createPolicy = RetryPolicies.retryUpToMaximumCountWithFixedSleep(
+        5, LEASE_SOFTLIMIT_PERIOD, TimeUnit.MILLISECONDS);
+    
+    Map<Class<? extends Exception>,RetryPolicy> remoteExceptionToPolicyMap =
+      new HashMap<Class<? extends Exception>, RetryPolicy>();
+    remoteExceptionToPolicyMap.put(AlreadyBeingCreatedException.class, createPolicy);
+
+    Map<Class<? extends Exception>,RetryPolicy> exceptionToPolicyMap =
+      new HashMap<Class<? extends Exception>, RetryPolicy>();
+    exceptionToPolicyMap.put(RemoteException.class, 
+        RetryPolicies.retryByRemoteException(
+            RetryPolicies.TRY_ONCE_THEN_FAIL, remoteExceptionToPolicyMap));
+    RetryPolicy methodPolicy = RetryPolicies.retryByException(
+        RetryPolicies.TRY_ONCE_THEN_FAIL, exceptionToPolicyMap);
+    Map<String,RetryPolicy> methodNameToPolicyMap = new HashMap<String,RetryPolicy>();
+    
+    methodNameToPolicyMap.put("create", methodPolicy);
+
+    return (AvatarProtocol) RetryProxy.create(AvatarProtocol.class,
+        rpcNamenode, methodNameToPolicyMap);
+  }
+
+  static ClientDatanodeProtocol createClientDatanodeProtocolProxy (
+      DatanodeID datanodeid, Configuration conf) throws IOException {
+    InetSocketAddress addr = NetUtils.createSocketAddr(
+      datanodeid.getHost() + ":" + datanodeid.getIpcPort());
+    if (ClientDatanodeProtocol.LOG.isDebugEnabled()) {
+      ClientDatanodeProtocol.LOG.info("ClientDatanodeProtocol addr=" + addr);
+    }
+    return (ClientDatanodeProtocol)RPC.getProxy(ClientDatanodeProtocol.class,
+        ClientDatanodeProtocol.versionID, addr, conf);
+  }
+        
+  /**
+   * Same as this(NameNode.getAddress(conf), conf);
+   * @see #AvatarClient(InetSocketAddress, Configuration)
+   */
+  public AvatarClient(Configuration conf) throws IOException {
+    this(NameNode.getAddress(conf), conf);
+  }
+
+  /**
+   * Same as this(nameNodeAddr, conf, null);
+   * @see #AvatarClient(InetSocketAddress, Configuration, org.apache.hadoop.fs.FileSystem.Statistics)
+   */
+  public AvatarClient(InetSocketAddress nameNodeAddr, Configuration conf
+      ) throws IOException {
+    this(nameNodeAddr, conf, null);
+  }
+
+  /**
+   * Same as this(nameNodeAddr, null, conf, stats);
+   * @see #AvatarClient(InetSocketAddress, AvatarProtocol, Configuration, org.apache.hadoop.fs.FileSystem.Statistics) 
+   */
+  public AvatarClient(InetSocketAddress nameNodeAddr, Configuration conf,
+                   FileSystem.Statistics stats)
+    throws IOException {
+    this(nameNodeAddr, null, conf, stats);
+  }
+
+  /** 
+   * Create a new AvatarClient connected to the given nameNodeAddr or rpcNamenode.
+   * Exactly one of nameNodeAddr or rpcNamenode must be null.
+   */
+  AvatarClient(InetSocketAddress nameNodeAddr, AvatarProtocol rpcNamenode,
+      Configuration conf, FileSystem.Statistics stats)
+    throws IOException {
+    this.conf = conf;
+    this.stats = stats;
+    this.socketTimeout = conf.getInt("dfs.socket.timeout", 
+                                     HdfsConstants.READ_TIMEOUT);
+    this.datanodeWriteTimeout = conf.getInt("dfs.datanode.socket.write.timeout",
+                                            HdfsConstants.WRITE_TIMEOUT);
+    this.socketFactory = NetUtils.getSocketFactory(conf, AvatarProtocol.class);
+    // dfs.write.packet.size is an internal config variable
+    this.writePacketSize = conf.getInt("dfs.write.packet.size", 64*1024);
+    this.maxBlockAcquireFailures = 
+                          conf.getInt("dfs.client.max.block.acquire.failures",
+                                      MAX_BLOCK_ACQUIRE_FAILURES);
+    
+    try {
+      this.ugi = UnixUserGroupInformation.login(conf, true);
+    } catch (LoginException e) {
+      throw (IOException)(new IOException().initCause(e));
+    }
+
+    String taskId = conf.get("mapred.task.id");
+    if (taskId != null) {
+      this.clientName = "AvatarClient_" + taskId; 
+    } else {
+      this.clientName = "AvatarClient_" + r.nextInt();
+    }
+    defaultBlockSize = conf.getLong("dfs.block.size", DEFAULT_BLOCK_SIZE);
+    defaultReplication = (short) conf.getInt("dfs.replication", 3);
+
+    if (nameNodeAddr != null && rpcNamenode == null) {
+      this.rpcNamenode = createRPCNamenode(nameNodeAddr, conf, ugi);
+      this.namenode = createNamenode(this.rpcNamenode);
+    } else if (nameNodeAddr == null && rpcNamenode != null) {
+      //This case is used for testing.
+      this.namenode = this.rpcNamenode = rpcNamenode;
+    } else {
+      throw new IllegalArgumentException(
+          "Expecting exactly one of nameNodeAddr and rpcNamenode being null: "
+          + "nameNodeAddr=" + nameNodeAddr + ", rpcNamenode=" + rpcNamenode);
+    }
+  }
+
+  private void checkOpen() throws IOException {
+    if (!clientRunning) {
+      IOException result = new IOException("Filesystem closed");
+      throw result;
+    }
+  }
+    
+  //
+  // If the call stack does not have FsShell.delete(), then invoke
+  // FsShell.delete. This ensures that files always goes thru Trash.
+  // Returns 0 if the file is successfully deleted by this method,
+  // Returns -1 if the file is not being deleted by this method
+  // Returns 1 if this method tried deleting the file but failed.
+  //
+  private int deleteUsingTrash(String file, boolean recursive) throws IOException {
+
+    // The configuration parameter specifies the class name to match.
+    // Typically, this is set to org.apache.hadoop.fs.FsShell.delete
+    String className = conf.get("fs.shell.delete.classname");
+    if (className == null) {
+      className = "org.apache.hadoop.fs.FsShell.delete";
+    }
+
+    // find the stack trace of this thread
+    StringWriter str = new StringWriter();
+    PrintWriter pr = new PrintWriter(str);
+    try {
+      throw new Throwable();
+    } catch (Throwable t) {
+      t.printStackTrace(pr);
+    }
+
+    // if the specified class does not appear in the calling thread's
+    // stack trace, and if this file is not in "/tmp",
+    // then invoke FsShell.delete()
+    if (str.toString().indexOf(className) == -1 &&
+        file.indexOf("/tmp") != 0) {
+      String errmsg = "File " + file + " is beng deleted only through Trash " +
+                      className +
+                      " because all deletes must go through Trash.";
+      LOG.warn(errmsg);
+      FsShell fh = new FsShell(conf);
+      Path p = new Path(file);
+      fh.init();
+      try {
+        fh.delete(p, p.getFileSystem(conf), recursive, false);
+        return 0;  // successful deletion
+      } catch (RemoteException rex) {
+        throw rex.unwrapRemoteException(AccessControlException.class);
+      } catch (AccessControlException ace) {
+        throw ace;
+      } catch (IOException e) {
+        return 1;                 // deletion unsuccessful
+      }
+    }
+    return -1;                     // deletion not attempted
+  }
+
+  /**
+   * Close the file system, abandoning all of the leases and files being
+   * created and close connections to the namenode.
+   */
+  public synchronized void close() throws IOException {
+    if(clientRunning) {
+      leasechecker.close();
+      clientRunning = false;
+      try {
+        leasechecker.interruptAndJoin();
+      } catch (InterruptedException ie) {
+      }
+  
+      // close connections to the namenode
+      RPC.stopProxy(rpcNamenode);
+    }
+  }
+
+  /**
+   * Get the default block size for this cluster
+   * @return the default block size in bytes
+   */
+  public long getDefaultBlockSize() {
+    return defaultBlockSize;
+  }
+    
+  public long getBlockSize(String f) throws IOException {
+    try {
+      return namenode.getPreferredBlockSize(f);
+    } catch (IOException ie) {
+      LOG.warn("Problem getting block size: " + 
+          StringUtils.stringifyException(ie));
+      throw ie;
+    }
+  }
+
+  /**
+   * Report corrupt blocks that were discovered by the client.
+   */
+  public void reportBadBlocks(LocatedBlock[] blocks) throws IOException {
+    namenode.reportBadBlocks(blocks);
+  }
+  
+  public short getDefaultReplication() {
+    return defaultReplication;
+  }
+    
+  /**
+   *  @deprecated Use getBlockLocations instead
+   *
+   * Get hints about the location of the indicated block(s).
+   * 
+   * getHints() returns a list of hostnames that store data for
+   * a specific file region.  It returns a set of hostnames for 
+   * every block within the indicated region.
+   *
+   * This function is very useful when writing code that considers
+   * data-placement when performing operations.  For example, the
+   * MapReduce system tries to schedule tasks on the same machines
+   * as the data-block the task processes. 
+   */
+  @Deprecated
+  public String[][] getHints(String src, long start, long length) 
+    throws IOException {
+    BlockLocation[] blkLocations = getBlockLocations(src, start, length);
+    if ((blkLocations == null) || (blkLocations.length == 0)) {
+      return new String[0][];
+    }
+    int blkCount = blkLocations.length;
+    String[][]hints = new String[blkCount][];
+    for (int i=0; i < blkCount ; i++) {
+      String[] hosts = blkLocations[i].getHosts();
+      hints[i] = new String[hosts.length];
+      hints[i] = hosts;
+    }
+    return hints;
+  }
+
+  private static LocatedBlocks callGetBlockLocations(AvatarProtocol namenode,
+      String src, long start, long length) throws IOException {
+    try {
+      return namenode.getBlockLocations(src, start, length);
+    } catch(RemoteException re) {
+      throw re.unwrapRemoteException(AccessControlException.class,
+                                    FileNotFoundException.class);
+    }
+  }
+
+  /**
+   * Get block location info about file
+   * 
+   * getBlockLocations() returns a list of hostnames that store 
+   * data for a specific file region.  It returns a set of hostnames
+   * for every block within the indicated region.
+   *
+   * This function is very useful when writing code that considers
+   * data-placement when performing operations.  For example, the
+   * MapReduce system tries to schedule tasks on the same machines
+   * as the data-block the task processes. 
+   */
+  public BlockLocation[] getBlockLocations(String src, long start, 
+    long length) throws IOException {
+    LocatedBlocks blocks = callGetBlockLocations(namenode, src, start, length);
+    if (blocks == null) {
+      return new BlockLocation[0];
+    }
+    int nrBlocks = blocks.locatedBlockCount();
+    BlockLocation[] blkLocations = new BlockLocation[nrBlocks];
+    int idx = 0;
+    for (LocatedBlock blk : blocks.getLocatedBlocks()) {
+      assert idx < nrBlocks : "Incorrect index";
+      DatanodeInfo[] locations = blk.getLocations();
+      String[] hosts = new String[locations.length];
+      String[] names = new String[locations.length];
+      String[] racks = new String[locations.length];
+      for (int hCnt = 0; hCnt < locations.length; hCnt++) {
+        hosts[hCnt] = locations[hCnt].getHostName();
+        names[hCnt] = locations[hCnt].getName();
+        NodeBase node = new NodeBase(names[hCnt], 
+                                     locations[hCnt].getNetworkLocation());
+        racks[hCnt] = node.toString();
+      }
+      blkLocations[idx] = new BlockLocation(names, hosts, racks,
+                                            blk.getStartOffset(),
+                                            blk.getBlockSize());
+      idx++;
+    }
+    return blkLocations;
+  }
+
+  public DFSInputStream open(String src) throws IOException {
+    return open(src, conf.getInt("io.file.buffer.size", 4096), true, null);
+  }
+
+  /**
+   * Create an input stream that obtains a nodelist from the
+   * namenode, and then reads from all the right places.  Creates
+   * inner subclass of InputStream that does the right out-of-band
+   * work.
+   */
+  DFSInputStream open(String src, int buffersize, boolean verifyChecksum,
+                      FileSystem.Statistics stats
+      ) throws IOException {
+    checkOpen();
+    //    Get block info from namenode
+    return new DFSInputStream(src, buffersize, verifyChecksum);
+  }
+
+  /**
+   * Create a new dfs file and return an output stream for writing into it. 
+   * 
+   * @param src stream name
+   * @param overwrite do not check for file existence if true
+   * @return output stream
+   * @throws IOException
+   */
+  public OutputStream create(String src, 
+                             boolean overwrite
+                             ) throws IOException {
+    return create(src, overwrite, defaultReplication, defaultBlockSize, null);
+  }
+    
+  /**
+   * Create a new dfs file and return an output stream for writing into it
+   * with write-progress reporting. 
+   * 
+   * @param src stream name
+   * @param overwrite do not check for file existence if true
+   * @return output stream
+   * @throws IOException
+   */
+  public OutputStream create(String src, 
+                             boolean overwrite,
+                             Progressable progress
+                             ) throws IOException {
+    return create(src, overwrite, defaultReplication, defaultBlockSize, null);
+  }
+    
+  /**
+   * Create a new dfs file with the specified block replication 
+   * and return an output stream for writing into the file.  
+   * 
+   * @param src stream name
+   * @param overwrite do not check for file existence if true
+   * @param replication block replication
+   * @return output stream
+   * @throws IOException
+   */
+  public OutputStream create(String src, 
+                             boolean overwrite, 
+                             short replication,
+                             long blockSize
+                             ) throws IOException {
+    return create(src, overwrite, replication, blockSize, null);
+  }
+
+  
+  /**
+   * Create a new dfs file with the specified block replication 
+   * with write-progress reporting and return an output stream for writing
+   * into the file.  
+   * 
+   * @param src stream name
+   * @param overwrite do not check for file existence if true
+   * @param replication block replication
+   * @return output stream
+   * @throws IOException
+   */
+  public OutputStream create(String src, 
+                             boolean overwrite, 
+                             short replication,
+                             long blockSize,
+                             Progressable progress
+                             ) throws IOException {
+    return create(src, overwrite, replication, blockSize, progress,
+        conf.getInt("io.file.buffer.size", 4096));
+  }
+  /**
+   * Call
+   * {@link #create(String,FsPermission,boolean,short,long,Progressable,int)}
+   * with default permission.
+   * @see FsPermission#getDefault()
+   */
+  public OutputStream create(String src,
+      boolean overwrite,
+      short replication,
+      long blockSize,
+      Progressable progress,
+      int buffersize
+      ) throws IOException {
+    return create(src, FsPermission.getDefault(),
+        overwrite, replication, blockSize, progress, buffersize);
+  }
+  /**
+   * Create a new dfs file with the specified block replication 
+   * with write-progress reporting and return an output stream for writing
+   * into the file.  
+   * 
+   * @param src stream name
+   * @param permission The permission of the directory being created.
+   * If permission == null, use {@link FsPermission#getDefault()}.
+   * @param overwrite do not check for file existence if true
+   * @param replication block replication
+   * @return output stream
+   * @throws IOException
+   * @see AvatarProtocol#create(String, FsPermission, String, boolean, short, long)
+   */
+  public OutputStream create(String src, 
+                             FsPermission permission,
+                             boolean overwrite, 
+                             short replication,
+                             long blockSize,
+                             Progressable progress,
+                             int buffersize
+                             ) throws IOException {
+    checkOpen();
+    if (permission == null) {
+      permission = FsPermission.getDefault();
+    }
+    FsPermission masked = permission.applyUMask(FsPermission.getUMask(conf));
+    LOG.debug(src + ": masked=" + masked);
+    OutputStream result = new DFSOutputStream(src, masked,
+        overwrite, replication, blockSize, progress, buffersize,
+        conf.getInt("io.bytes.per.checksum", 512));
+    leasechecker.put(src, result);
+    return result;
+  }
+
+  /**
+   * Append to an existing HDFS file.  
+   * 
+   * @param src file name
+   * @param buffersize buffer size
+   * @param progress for reporting write-progress
+   * @return an output stream for writing into the file
+   * @throws IOException
+   * @see AvatarProtocol#append(String, String)
+   */
+  OutputStream append(String src, int buffersize, Progressable progress
+      ) throws IOException {
+    checkOpen();
+    FileStatus stat = null;
+    LocatedBlock lastBlock = null;
+    try {
+      stat = getFileInfo(src);
+      lastBlock = namenode.append(src, clientName);
+    } catch(RemoteException re) {
+      throw re.unwrapRemoteException(FileNotFoundException.class,
+                                     AccessControlException.class,
+                                     NSQuotaExceededException.class,
+                                     DSQuotaExceededException.class);
+    }
+    OutputStream result = new DFSOutputStream(src, buffersize, progress,
+        lastBlock, stat, conf.getInt("io.bytes.per.checksum", 512));
+    leasechecker.put(src, result);
+    return result;
+  }
+
+  /**
+   * Set replication for an existing file.
+   * 
+   * @see AvatarProtocol#setReplication(String, short)
+   * @param replication
+   * @throws IOException
+   * @return true is successful or false if file does not exist 
+   */
+  public boolean setReplication(String src, 
+                                short replication
+                                ) throws IOException {
+    try {
+      return namenode.setReplication(src, replication);
+    } catch(RemoteException re) {
+      throw re.unwrapRemoteException(AccessControlException.class,
+                                     NSQuotaExceededException.class,
+                                     DSQuotaExceededException.class);
+    }
+  }
+
+  /**
+   * Rename file or directory.
+   * See {@link AvatarProtocol#rename(String, String)}. 
+   */
+  public boolean rename(String src, String dst) throws IOException {
+    checkOpen();
+    try {
+      return namenode.rename(src, dst);
+    } catch(RemoteException re) {
+      throw re.unwrapRemoteException(AccessControlException.class,
+                                     NSQuotaExceededException.class,
+                                     DSQuotaExceededException.class);
+    }
+  }
+
+  /**
+   * Delete file or directory.
+   * See {@link AvatarProtocol#delete(String)}. 
+   */
+  @Deprecated
+  public boolean delete(String src) throws IOException {
+    checkOpen();
+    int val = deleteUsingTrash(src, true);           // allow deletion only from FsShell
+    if (val == 0) {
+      return true;
+    } else if (val == 1) {
+      return false;
+    }
+    return namenode.delete(src, true);
+  }
+
+  /**
+   * delete file or directory.
+   * delete contents of the directory if non empty and recursive 
+   * set to true
+   */
+  public boolean delete(String src, boolean recursive) throws IOException {
+    checkOpen();
+    int val = deleteUsingTrash(src, recursive);           // allow deletion only from FsShell
+    if (val == 0) {
+      return true;
+    } else if (val == 1) {
+      return false;
+    }
+    try {
+      return namenode.delete(src, recursive);
+    } catch(RemoteException re) {
+      throw re.unwrapRemoteException(AccessControlException.class);
+    }
+  }
+  
+  /** Implemented using getFileInfo(src)
+   */
+  public boolean exists(String src) throws IOException {
+    checkOpen();
+    return getFileInfo(src) != null;
+  }
+
+  /** @deprecated Use getFileStatus() instead */
+  @Deprecated
+  public boolean isDirectory(String src) throws IOException {
+    FileStatus fs = getFileInfo(src);
+    if (fs != null)
+      return fs.isDir();
+    else
+      throw new FileNotFoundException("File does not exist: " + src);
+  }
+
+  /**
+   */
+  public FileStatus[] listPaths(String src) throws IOException {
+    checkOpen();
+    try {
+      return namenode.getListing(src);
+    } catch(RemoteException re) {
+      throw re.unwrapRemoteException(AccessControlException.class);
+    }
+  }
+
+  public FileStatus getFileInfo(String src) throws IOException {
+    checkOpen();
+    try {
+      return namenode.getFileInfo(src);
+    } catch(RemoteException re) {
+      throw re.unwrapRemoteException(AccessControlException.class);
+    }
+  }
+
+  /**
+   * Get the checksum of a file.
+   * @param src The file path
+   * @return The checksum 
+   * @see DistributedFileSystem#getFileChecksum(Path)
+   */
+  MD5MD5CRC32FileChecksum getFileChecksum(String src) throws IOException {
+    checkOpen();
+    return getFileChecksum(src, namenode, socketFactory, socketTimeout);    
+  }
+
+  /**
+   * Get the checksum of a file.
+   * @param src The file path
+   * @return The checksum 
+   */
+  public static MD5MD5CRC32FileChecksum getFileChecksum(String src,
+      AvatarProtocol namenode, SocketFactory socketFactory, int socketTimeout
+      ) throws IOException {
+    //get all block locations
+    final List<LocatedBlock> locatedblocks
+        = callGetBlockLocations(namenode, src, 0, Long.MAX_VALUE).getLocatedBlocks();
+    final DataOutputBuffer md5out = new DataOutputBuffer();
+    int bytesPerCRC = 0;
+    long crcPerBlock = 0;
+
+    //get block checksum for each block
+    for(int i = 0; i < locatedblocks.size(); i++) {
+      LocatedBlock lb = locatedblocks.get(i);
+      final Block block = lb.getBlock();
+      final DatanodeInfo[] datanodes = lb.getLocations();
+      
+      //try each datanode location of the block
+      final int timeout = 3000 * datanodes.length + socketTimeout;
+      boolean done = false;
+      for(int j = 0; !done && j < datanodes.length; j++) {
+        //connect to a datanode
+        final Socket sock = socketFactory.createSocket();
+        NetUtils.connect(sock, 
+                         NetUtils.createSocketAddr(datanodes[j].getName()),
+                         timeout);
+        sock.setSoTimeout(timeout);
+
+        DataOutputStream out = new DataOutputStream(
+            new BufferedOutputStream(NetUtils.getOutputStream(sock), 
+                                     DataNode.SMALL_BUFFER_SIZE));
+        DataInputStream in = new DataInputStream(NetUtils.getInputStream(sock));
+
+        // get block MD5
+        try {
+          if (LOG.isDebugEnabled()) {
+            LOG.debug("write to " + datanodes[j].getName() + ": "
+                + DataTransferProtocol.OP_BLOCK_CHECKSUM +
+                ", block=" + block);
+          }
+          out.writeShort(DataTransferProtocol.DATA_TRANSFER_VERSION);
+          out.write(DataTransferProtocol.OP_BLOCK_CHECKSUM);
+          out.writeLong(block.getBlockId());
+          out.writeLong(block.getGenerationStamp());
+          out.flush();
+         
+          final short reply = in.readShort();
+          if (reply != DataTransferProtocol.OP_STATUS_SUCCESS) {
+            throw new IOException("Bad response " + reply + " for block "
+                + block + " from datanode " + datanodes[j].getName());
+          }
+
+          //read byte-per-checksum
+          final int bpc = in.readInt(); 
+          if (i == 0) { //first block
+            bytesPerCRC = bpc;
+          }
+          else if (bpc != bytesPerCRC) {
+            throw new IOException("Byte-per-checksum not matched: bpc=" + bpc
+                + " but bytesPerCRC=" + bytesPerCRC);
+          }
+          
+          //read crc-per-block
+          final long cpb = in.readLong();
+          if (locatedblocks.size() > 1 && i == 0) {
+            crcPerBlock = cpb;
+          }
+
+          //read md5
+          final MD5Hash md5 = MD5Hash.read(in);
+          md5.write(md5out);
+          
+          done = true;
+
+          if (LOG.isDebugEnabled()) {
+            if (i == 0) {
+              LOG.debug("set bytesPerCRC=" + bytesPerCRC
+                  + ", crcPerBlock=" + crcPerBlock);
+            }
+            LOG.debug("got reply from " + datanodes[j].getName()
+                + ": md5=" + md5);
+          }
+        } catch (IOException ie) {
+          LOG.warn("src=" + src + ", datanodes[" + j + "].getName()="
+              + datanodes[j].getName(), ie);
+        } finally {
+          IOUtils.closeStream(in);
+          IOUtils.closeStream(out);
+          IOUtils.closeSocket(sock);        
+        }
+      }
+
+      if (!done) {
+        throw new IOException("Fail to get block MD5 for " + block);
+      }
+    }
+
+    //compute file MD5
+    final MD5Hash fileMD5 = MD5Hash.digest(md5out.getData()); 
+    return new MD5MD5CRC32FileChecksum(bytesPerCRC, crcPerBlock, fileMD5);
+  }
+
+  /**
+   * Set permissions to a file or directory.
+   * @param src path name.
+   * @param permission
+   * @throws <code>FileNotFoundException</code> is file does not exist.
+   */
+  public void setPermission(String src, FsPermission permission
+                            ) throws IOException {
+    checkOpen();
+    try {
+      namenode.setPermission(src, permission);
+    } catch(RemoteException re) {
+      throw re.unwrapRemoteException(AccessControlException.class,
+                                     FileNotFoundException.class);
+    }
+  }
+
+  /**
+   * Set file or directory owner.
+   * @param src path name.
+   * @param username user id.
+   * @param groupname user group.
+   * @throws <code>FileNotFoundException</code> is file does not exist.
+   */
+  public void setOwner(String src, String username, String groupname
+                      ) throws IOException {
+    checkOpen();
+    try {
+      namenode.setOwner(src, username, groupname);
+    } catch(RemoteException re) {
+      throw re.unwrapRemoteException(AccessControlException.class,
+                                     FileNotFoundException.class);
+    }
+  }
+
+  public DiskStatus getDiskStatus() throws IOException {
+    long rawNums[] = namenode.getStats();
+    return new DiskStatus(rawNums[0], rawNums[1], rawNums[2]);
+  }
+  /**
+   */
+  public long totalRawCapacity() throws IOException {
+    long rawNums[] = namenode.getStats();
+    return rawNums[0];
+  }
+
+  /**
+   */
+  public long totalRawUsed() throws IOException {
+    long rawNums[] = namenode.getStats();
+    return rawNums[1];
+  }
+
+  /**
+   * Returns count of blocks with no good replicas left. Normally should be 
+   * zero.
+   * @throws IOException
+   */ 
+  public long getMissingBlocksCount() throws IOException {
+    return namenode.getStats()[AvatarProtocol.GET_STATS_MISSING_BLOCKS_IDX];
+  }
+  
+  /**
+   * Returns count of blocks with one of more replica missing.
+   * @throws IOException
+   */ 
+  public long getUnderReplicatedBlocksCount() throws IOException {
+    return namenode.getStats()[AvatarProtocol.GET_STATS_UNDER_REPLICATED_IDX];
+  }
+  
+  /**
+   * Returns count of blocks with at least one replica marked corrupt. 
+   * @throws IOException
+   */ 
+  public long getCorruptBlocksCount() throws IOException {
+    return namenode.getStats()[AvatarProtocol.GET_STATS_CORRUPT_BLOCKS_IDX];
+  }
+  
+  public DatanodeInfo[] datanodeReport(DatanodeReportType type)
+  throws IOException {
+    return namenode.getDatanodeReport(type);
+  }
+    
+  /**
+   * Enter, leave or get safe mode.
+   * See {@link AvatarProtocol#setSafeMode(FSConstants.SafeModeAction)} 
+   * for more details.
+   * 
+   * @see AvatarProtocol#setSafeMode(FSConstants.SafeModeAction)
+   */
+  public boolean setSafeMode(SafeModeAction action) throws IOException {
+    return namenode.setSafeMode(action);
+  }
+
+  /**
+   * Save namespace image.
+   * See {@link AvatarProtocol#saveNamespace()} 
+   * for more details.
+   * 
+   * @see AvatarProtocol#saveNamespace()
+   */
+  void saveNamespace() throws AccessControlException, IOException {
+    try {
+      namenode.saveNamespace();
+    } catch(RemoteException re) {
+      throw re.unwrapRemoteException(AccessControlException.class);
+    }
+  }
+
+  /**
+   * Refresh the hosts and exclude files.  (Rereads them.)
+   * See {@link AvatarProtocol#refreshNodes()} 
+   * for more details.
+   * 
+   * @see AvatarProtocol#refreshNodes()
+   */
+  public void refreshNodes() throws IOException {
+    namenode.refreshNodes();
+  }
+
+  /**
+   * Dumps DFS data structures into specified file.
+   * See {@link AvatarProtocol#metaSave(String)} 
+   * for more details.
+   * 
+   * @see AvatarProtocol#metaSave(String)
+   */
+  public void metaSave(String pathname) throws IOException {
+    namenode.metaSave(pathname);
+  }
+    
+  /**
+   * @see AvatarProtocol#finalizeUpgrade()
+   */
+  public void finalizeUpgrade() throws IOException {
+    namenode.finalizeUpgrade();
+  }
+
+  /**
+   * @see AvatarProtocol#distributedUpgradeProgress(FSConstants.UpgradeAction)
+   */
+  public UpgradeStatusReport distributedUpgradeProgress(UpgradeAction action
+                                                        ) throws IOException {
+    return namenode.distributedUpgradeProgress(action);
+  }
+
+  /**
+   */
+  public boolean mkdirs(String src) throws IOException {
+    return mkdirs(src, null);
+  }
+
+  /**
+   * Create a directory (or hierarchy of directories) with the given
+   * name and permission.
+   *
+   * @param src The path of the directory being created
+   * @param permission The permission of the directory being created.
+   * If permission == null, use {@link FsPermission#getDefault()}.
+   * @return True if the operation success.
+   * @see AvatarProtocol#mkdirs(String, FsPermission)
+   */
+  public boolean mkdirs(String src, FsPermission permission)throws IOException{
+    checkOpen();
+    if (permission == null) {
+      permission = FsPermission.getDefault();
+    }
+    FsPermission masked = permission.applyUMask(FsPermission.getUMask(conf));
+    LOG.debug(src + ": masked=" + masked);
+    try {
+      return namenode.mkdirs(src, masked);
+    } catch(RemoteException re) {
+      throw re.unwrapRemoteException(AccessControlException.class,
+                                     NSQuotaExceededException.class,
+                                     DSQuotaExceededException.class);
+    }
+  }
+
+  ContentSummary getContentSummary(String src) throws IOException {
+    try {
+      return namenode.getContentSummary(src);
+    } catch(RemoteException re) {
+      throw re.unwrapRemoteException(AccessControlException.class,
+                                     FileNotFoundException.class);
+    }
+  }
+
+  /**
+   * Sets or resets quotas for a directory.
+   * @see org.apache.hadoop.hdfs.protocol.AvatarProtocol#setQuota(String, long, long)
+   */
+  void setQuota(String src, long namespaceQuota, long diskspaceQuota) 
+                                                 throws IOException {
+    // sanity check
+    if ((namespaceQuota <= 0 && namespaceQuota != FSConstants.QUOTA_DONT_SET &&
+         namespaceQuota != FSConstants.QUOTA_RESET) ||
+        (diskspaceQuota <= 0 && diskspaceQuota != FSConstants.QUOTA_DONT_SET &&
+         diskspaceQuota != FSConstants.QUOTA_RESET)) {
+      throw new IllegalArgumentException("Invalid values for quota : " +
+                                         namespaceQuota + " and " + 
+                                         diskspaceQuota);
+                                         
+    }
+    
+    try {
+      namenode.setQuota(src, namespaceQuota, diskspaceQuota);
+    } catch(RemoteException re) {
+      throw re.unwrapRemoteException(AccessControlException.class,
+                                     FileNotFoundException.class,
+                                     NSQuotaExceededException.class,
+                                     DSQuotaExceededException.class);
+    }
+  }
+
+  /**
+   * set the modification and access time of a file
+   * @throws FileNotFoundException if the path is not a file
+   */
+  public void setTimes(String src, long mtime, long atime) throws IOException {
+    checkOpen();
+    try {
+      namenode.setTimes(src, mtime, atime);
+    } catch(RemoteException re) {
+      throw re.unwrapRemoteException(AccessControlException.class,
+                                     FileNotFoundException.class);
+    }
+  }
+
+  /**
+   * Pick the best node from which to stream the data.
+   * Entries in <i>nodes</i> are already in the priority order
+   */
+  private DatanodeInfo bestNode(DatanodeInfo nodes[], 
+                                AbstractMap<DatanodeInfo, DatanodeInfo> deadNodes)
+                                throws IOException {
+    if (nodes != null) { 
+      for (int i = 0; i < nodes.length; i++) {
+        if (!deadNodes.containsKey(nodes[i])) {
+          return nodes[i];
+        }
+      }
+    }
+    throw new IOException("No live nodes contain current block");
+  }
+
+  boolean isLeaseCheckerStarted() {
+    return leasechecker.daemon != null;
+  }
+
+  /** Lease management*/
+  class LeaseChecker implements Runnable {
+    /** A map from src -> DFSOutputStream of files that are currently being
+     * written by this client.
+     */
+    private final SortedMap<String, OutputStream> pendingCreates
+        = new TreeMap<String, OutputStream>();
+
+    private Daemon daemon = null;
+    
+    synchronized void put(String src, OutputStream out) {
+      if (clientRunning) {
+        if (daemon == null) {
+          daemon = new Daemon(this);
+          daemon.start();
+        }
+        pendingCreates.put(src, out);
+      }
+    }
+    
+    synchronized void remove(String src) {
+      pendingCreates.remove(src);
+    }
+    
+    void interruptAndJoin() throws InterruptedException {
+      Daemon daemonCopy = null;
+      synchronized (this) {
+        if (daemon != null) {
+          daemon.interrupt();
+          daemonCopy = daemon;
+        }
+      }
+     
+      if (daemonCopy != null) {
+        LOG.debug("Wait for lease checker to terminate");
+        daemonCopy.join();
+      }
+    }
+
+    synchronized void close() {
+      while (!pendingCreates.isEmpty()) {
+        String src = pendingCreates.firstKey();
+        OutputStream out = pendingCreates.remove(src);
+        if (out != null) {
+          try {
+            out.close();
+          } catch (IOException ie) {
+            LOG.error("Exception closing file " + src+ " : " + ie, ie);
+          }
+        }
+      }
+    }
+
+    private void renew() throws IOException {
+      synchronized(this) {
+        if (pendingCreates.isEmpty()) {
+          return;
+        }
+      }
+      namenode.renewLease(clientName);
+    }
+
+    /**
+     * Periodically check in with the namenode and renew all the leases
+     * when the lease period is half over.
+     */
+    public void run() {
+      long lastRenewed = 0;
+      while (clientRunning && !Thread.interrupted()) {
+        if (System.currentTimeMillis() - lastRenewed > (LEASE_SOFTLIMIT_PERIOD / 2)) {
+          try {
+            renew();
+            lastRenewed = System.currentTimeMillis();
+          } catch (IOException ie) {
+            LOG.warn("Problem renewing lease for " + clientName, ie);
+          }
+        }
+
+        try {
+          Thread.sleep(1000);
+        } catch (InterruptedException ie) {
+          if (LOG.isDebugEnabled()) {
+            LOG.debug(this + " is interrupted.", ie);
+          }
+          return;
+        }
+      }
+    }
+
+    /** {@inheritDoc} */
+    public String toString() {
+      String s = getClass().getSimpleName();
+      if (LOG.isTraceEnabled()) {
+        return s + "@" + AvatarClient.this + ": "
+               + StringUtils.stringifyException(new Throwable("for testing"));
+      }
+      return s;
+    }
+  }
+
+  /** Utility class to encapsulate data node info and its ip address. */
+  private static class DNAddrPair {
+    DatanodeInfo info;
+    InetSocketAddress addr;
+    DNAddrPair(DatanodeInfo info, InetSocketAddress addr) {
+      this.info = info;
+      this.addr = addr;
+    }
+  }
+
+  /** This is a wrapper around connection to datadone
+   * and understands checksum, offset etc
+   */
+  public static class BlockReader extends FSInputChecker {
+
+    private Socket dnSock; //for now just sending checksumOk.
+    private DataInputStream in;
+    private DataChecksum checksum;
+    private long lastChunkOffset = -1;
+    private long lastChunkLen = -1;
+    private long lastSeqNo = -1;
+
+    private long startOffset;
+    private long firstChunkOffset;
+    private int bytesPerChecksum;
+    private int checksumSize;
+    private boolean gotEOS = false;
+    
+    byte[] skipBuf = null;
+    ByteBuffer checksumBytes = null;
+    int dataLeft = 0;
+    boolean isLastPacket = false;
+    
+    /* FSInputChecker interface */
+    
+    /* same interface as inputStream java.io.InputStream#read()
+     * used by DFSInputStream#read()
+     * This violates one rule when there is a checksum error:
+     * "Read should not modify user buffer before successful read"
+     * because it first reads the data to user buffer and then checks
+     * the checksum.
+     */
+    @Override
+    public synchronized int read(byte[] buf, int off, int len) 
+                                 throws IOException {
+      
+      //for the first read, skip the extra bytes at the front.
+      if (lastChunkLen < 0 && startOffset > firstChunkOffset && len > 0) {
+        // Skip these bytes. But don't call this.skip()!
+        int toSkip = (int)(startOffset - firstChunkOffset);
+        if ( skipBuf == null ) {
+          skipBuf = new byte[bytesPerChecksum];
+        }
+        if ( super.read(skipBuf, 0, toSkip) != toSkip ) {
+          // should never happen
+          throw new IOException("Could not skip required number of bytes");
+        }
+      }
+      
+      boolean eosBefore = gotEOS;
+      int nRead = super.read(buf, off, len);
+      
+      // if gotEOS was set in the previous read and checksum is enabled :
+      if (gotEOS && !eosBefore && nRead >= 0 && needChecksum()) {
+        //checksum is verified and there are no errors.
+        checksumOk(dnSock);
+      }
+      return nRead;
+    }
+
+    @Override
+    public synchronized long skip(long n) throws IOException {
+      /* How can we make sure we don't throw a ChecksumException, at least
+       * in majority of the cases?. This one throws. */  
+      if ( skipBuf == null ) {
+        skipBuf = new byte[bytesPerChecksum]; 
+      }
+
+      long nSkipped = 0;
+      while ( nSkipped < n ) {
+        int toSkip = (int)Math.min(n-nSkipped, skipBuf.length);
+        int ret = read(skipBuf, 0, toSkip);
+        if ( ret <= 0 ) {
+          return nSkipped;
+        }
+        nSkipped += ret;
+      }
+      return nSkipped;
+    }
+
+    @Override
+    public int read() throws IOException {
+      throw new IOException("read() is not expected to be invoked. " +
+                            "Use read(buf, off, len) instead.");
+    }
+    
+    @Override
+    public boolean seekToNewSource(long targetPos) throws IOException {
+      /* Checksum errors are handled outside the BlockReader. 
+       * DFSInputStream does not always call 'seekToNewSource'. In the 
+       * case of pread(), it just tries a different replica without seeking.
+       */ 
+      return false;
+    }
+    
+    @Override
+    public void seek(long pos) throws IOException {
+      throw new IOException("Seek() is not supported in BlockInputChecker");
+    }
+
+    @Override
+    protected long getChunkPosition(long pos) {
+      throw new RuntimeException("getChunkPosition() is not supported, " +
+                                 "since seek is not required");
+    }
+    
+    /**
+     * Makes sure that checksumBytes has enough capacity 
+     * and limit is set to the number of checksum bytes needed 
+     * to be read.
+     */
+    private void adjustChecksumBytes(int dataLen) {
+      int requiredSize = 
+        ((dataLen + bytesPerChecksum - 1)/bytesPerChecksum)*checksumSize;
+      if (checksumBytes == null || requiredSize > checksumBytes.capacity()) {
+        checksumBytes =  ByteBuffer.wrap(new byte[requiredSize]);
+      } else {
+        checksumBytes.clear();
+      }
+      checksumBytes.limit(requiredSize);
+    }
+    
+    @Override
+    protected synchronized int readChunk(long pos, byte[] buf, int offset, 
+                                         int len, byte[] checksumBuf) 
+                                         throws IOException {
+      // Read one chunk.
+      
+      if ( gotEOS ) {
+        if ( startOffset < 0 ) {
+          //This is mainly for debugging. can be removed.
+          throw new IOException( "BlockRead: already got EOS or an error" );
+        }
+        startOffset = -1;
+        return -1;
+      }
+      
+      // Read one DATA_CHUNK.
+      long chunkOffset = lastChunkOffset;
+      if ( lastChunkLen > 0 ) {
+        chunkOffset += lastChunkLen;
+      }
+      
+      if ( (pos + firstChunkOffset) != chunkOffset ) {
+        throw new IOException("Mismatch in pos : " + pos + " + " + 
+                              firstChunkOffset + " != " + chunkOffset);
+      }
+
+      // Read next packet if the previous packet has been read completely.
+      if (dataLeft <= 0) {
+        //Read packet headers.
+        int packetLen = in.readInt();
+        long offsetInBlock = in.readLong();
+        long seqno = in.readLong();
+        boolean lastPacketInBlock = in.readBoolean();
+      
+        if (LOG.isDebugEnabled()) {
+          LOG.debug("AvatarClient readChunk got seqno " + seqno +
+                    " offsetInBlock " + offsetInBlock +
+                    " lastPacketInBlock " + lastPacketInBlock +
+                    " packetLen " + packetLen);
+        }
+        
+        int dataLen = in.readInt();
+      
+        // Sanity check the lengths
+        if ( dataLen < 0 || 
+             ( (dataLen % bytesPerChecksum) != 0 && !lastPacketInBlock ) ||
+             (seqno != (lastSeqNo + 1)) ) {
+             throw new IOException("BlockReader: error in packet header" +
+                                   "(chunkOffset : " + chunkOffset + 
+                                   ", dataLen : " + dataLen +
+                                   ", seqno : " + seqno + 
+                                   " (last: " + lastSeqNo + "))");
+        }
+        
+        lastSeqNo = seqno;
+        isLastPacket = lastPacketInBlock;
+        dataLeft = dataLen;
+        adjustChecksumBytes(dataLen);
+        if (dataLen > 0) {
+          IOUtils.readFully(in, checksumBytes.array(), 0,
+                            checksumBytes.limit());
+        }
+      }
+
+      int chunkLen = Math.min(dataLeft, bytesPerChecksum);
+      
+      if ( chunkLen > 0 ) {
+        // len should be >= chunkLen
+        IOUtils.readFully(in, buf, offset, chunkLen);
+        checksumBytes.get(checksumBuf, 0, checksumSize);
+      }
+      
+      dataLeft -= chunkLen;
+      lastChunkOffset = chunkOffset;
+      lastChunkLen = chunkLen;
+      
+      if ((dataLeft == 0 && isLastPacket) || chunkLen == 0) {
+        gotEOS = true;
+      }
+      if ( chunkLen == 0 ) {
+        return -1;
+      }
+      
+      return chunkLen;
+    }
+    
+    private BlockReader( String file, long blockId, DataInputStream in, 
+                         DataChecksum checksum, boolean verifyChecksum,
+                         long startOffset, long firstChunkOffset, 
+                         Socket dnSock ) {
+      super(new Path("/blk_" + blockId + ":of:" + file)/*too non path-like?*/,
+            1, verifyChecksum,
+            checksum.getChecksumSize() > 0? checksum : null, 
+            checksum.getBytesPerChecksum(),
+            checksum.getChecksumSize());
+      
+      this.dnSock = dnSock;
+      this.in = in;
+      this.checksum = checksum;
+      this.startOffset = Math.max( startOffset, 0 );
+
+      this.firstChunkOffset = firstChunkOffset;
+      lastChunkOffset = firstChunkOffset;
+      lastChunkLen = -1;
+
+      bytesPerChecksum = this.checksum.getBytesPerChecksum();
+      checksumSize = this.checksum.getChecksumSize();
+    }
+
+    public static BlockReader newBlockReader(Socket sock, String file, long blockId, 
+        long genStamp, long startOffset, long len, int bufferSize) throws IOException {
+      return newBlockReader(sock, file, blockId, genStamp, startOffset, len, bufferSize,
+          true);
+    }
+
+    /** Java Doc required */
+    public static BlockReader newBlockReader( Socket sock, String file, long blockId, 
+                                       long genStamp,
+                                       long startOffset, long len,
+                                       int bufferSize, boolean verifyChecksum)
+                                       throws IOException {
+      return newBlockReader(sock, file, blockId, genStamp, startOffset,
+                            len, bufferSize, verifyChecksum, "");
+    }
+
+    public static BlockReader newBlockReader( Socket sock, String file,
+                                       long blockId, 
+                                       long genStamp,
+                                       long startOffset, long len,
+                                       int bufferSize, boolean verifyChecksum,
+                                       String clientName)
+                                       throws IOException {
+      // in and out will be closed when sock is closed (by the caller)
+      DataOutputStream out = new DataOutputStream(
+        new BufferedOutputStream(NetUtils.getOutputStream(sock,HdfsConstants.WRITE_TIMEOUT)));
+
+      //write the header.
+      out.writeShort( DataTransferProtocol.DATA_TRANSFER_VERSION );
+      out.write( DataTransferProtocol.OP_READ_BLOCK );
+      out.writeLong( blockId );
+      out.writeLong( genStamp );
+      out.writeLong( startOffset );
+      out.writeLong( len );
+      Text.writeString(out, clientName);
+      out.flush();
+      
+      //
+      // Get bytes in block, set streams
+      //
+
+      DataInputStream in = new DataInputStream(
+          new BufferedInputStream(NetUtils.getInputStream(sock), 
+                                  bufferSize));
+      
+      if ( in.readShort() != DataTransferProtocol.OP_STATUS_SUCCESS ) {
+        throw new IOException("Got error in response to OP_READ_BLOCK " +
+                              "for file " + file + 
+                              " for block " + blockId);
+      }
+      DataChecksum checksum = DataChecksum.newDataChecksum( in );
+      //Warning when we get CHECKSUM_NULL?
+      
+      // Read the first chunk offset.
+      long firstChunkOffset = in.readLong();
+      
+      if ( firstChunkOffset < 0 || firstChunkOffset > startOffset ||
+          firstChunkOffset >= (startOffset + checksum.getBytesPerChecksum())) {
+        throw new IOException("BlockReader: error in first chunk offset (" +
+                              firstChunkOffset + ") startOffset is " + 
+                              startOffset + " for file " + file);
+      }
+
+      return new BlockReader( file, blockId, in, checksum, verifyChecksum,
+                              startOffset, firstChunkOffset, sock );
+    }
+
+    @Override
+    public synchronized void close() throws IOException {
+      startOffset = -1;
+      checksum = null;
+      // in will be closed when its Socket is closed.
+    }
+    
+    /** kind of like readFully(). Only reads as much as possible.
+     * And allows use of protected readFully().
+     */
+    public int readAll(byte[] buf, int offset, int len) throws IOException {
+      return readFully(this, buf, offset, len);
+    }
+    
+    /* When the reader reaches end of a block and there are no checksum
+     * errors, we send OP_STATUS_CHECKSUM_OK to datanode to inform that 
+     * checksum was verified and there was no error.
+     */ 
+    private void checksumOk(Socket sock) {
+      try {
+        OutputStream out = NetUtils.getOutputStream(sock, HdfsConstants.WRITE_TIMEOUT);
+        byte buf[] = { (DataTransferProtocol.OP_STATUS_CHECKSUM_OK >>> 8) & 0xff,
+                       (DataTransferProtocol.OP_STATUS_CHECKSUM_OK) & 0xff };
+        out.write(buf);
+        out.flush();
+      } catch (IOException e) {
+        // its ok not to be able to send this.
+        LOG.debug("Could not write to datanode " + sock.getInetAddress() +
+                  ": " + e.getMessage());
+      }
+    }
+  }
+    
+  /****************************************************************
+   * DFSInputStream provides bytes from a named file.  It handles 
+   * negotiation of the namenode and various datanodes as necessary.
+   ****************************************************************/
+  class DFSInputStream extends FSInputStream {
+    private Socket s = null;
+    private boolean closed = false;
+
+    private String src;
+    private long prefetchSize = 10 * defaultBlockSize;
+    private BlockReader blockReader = null;
+    private boolean verifyChecksum;
+    private LocatedBlocks locatedBlocks = null;
+    private DatanodeInfo currentNode = null;
+    private Block currentBlock = null;
+    private long pos = 0;
+    private long blockEnd = -1;
+    private int failures = 0;
+    private int timeWindow = 3000; // wait time window (in msec) if BlockMissingException is caught
+
+    /* XXX Use of CocurrentHashMap is temp fix. Need to fix 
+     * parallel accesses to DFSInputStream (through ptreads) properly */
+    private ConcurrentHashMap<DatanodeInfo, DatanodeInfo> deadNodes = 
+               new ConcurrentHashMap<DatanodeInfo, DatanodeInfo>();
+    private int buffersize = 1;
+    
+    private byte[] oneByteBuf = new byte[1]; // used for 'int read()'
+    
+    void addToDeadNodes(DatanodeInfo dnInfo) {
+      deadNodes.put(dnInfo, dnInfo);
+    }
+    
+    DFSInputStream(String src, int buffersize, boolean verifyChecksum
+                   ) throws IOException {
+      this.verifyChecksum = verifyChecksum;
+      this.buffersize = buffersize;
+      this.src = src;
+      prefetchSize = conf.getLong("dfs.read.prefetch.size", prefetchSize);
+      timeWindow = conf.getInt("dfs.client.baseTimeWindow.waitOn.BlockMissingException", timeWindow);
+      openInfo();
+    }
+
+    /**
+     * Grab the open-file info from namenode
+     */
+    synchronized void openInfo() throws IOException {
+      LocatedBlocks newInfo = callGetBlockLocations(namenode, src, 0, prefetchSize);
+      if (newInfo == null) {
+        throw new IOException("Cannot open filename " + src);
+      }
+
+      if (locatedBlocks != null) {
+        Iterator<LocatedBlock> oldIter = locatedBlocks.getLocatedBlocks().iterator();
+        Iterator<LocatedBlock> newIter = newInfo.getLocatedBlocks().iterator();
+        while (oldIter.hasNext() && newIter.hasNext()) {
+          if (! oldIter.next().getBlock().equals(newIter.next().getBlock())) {
+            throw new IOException("Blocklist for " + src + " has changed!");
+          }
+        }
+      }
+      this.locatedBlocks = newInfo;
+      this.currentNode = null;
+    }
+    
+    public synchronized long getFileLength() {
+      return (locatedBlocks == null) ? 0 : locatedBlocks.getFileLength();
+    }
+
+    /**
+     * Returns the datanode from which the stream is currently reading.
+     */
+    public DatanodeInfo getCurrentDatanode() {
+      return currentNode;
+    }
+
+    /**
+     * Returns the block containing the target position. 
+     */
+    public Block getCurrentBlock() {
+      return currentBlock;
+    }
+
+    /**
+     * Return collection of blocks that has already been located.
+     */
+    synchronized List<LocatedBlock> getAllBlocks() throws IOException {
+      return getBlockRange(0, this.getFileLength());
+    }
+
+    /**
+     * Get block at the specified position.
+     * Fetch it from the namenode if not cached.
+     * 
+     * @param offset
+     * @return located block
+     * @throws IOException
+     */
+    private LocatedBlock getBlockAt(long offset) throws IOException {
+      assert (locatedBlocks != null) : "locatedBlocks is null";
+      // search cached blocks first
+      int targetBlockIdx = locatedBlocks.findBlock(offset);
+      if (targetBlockIdx < 0) { // block is not cached
+        targetBlockIdx = LocatedBlocks.getInsertIndex(targetBlockIdx);
+        // fetch more blocks
+        LocatedBlocks newBlocks;
+        newBlocks = callGetBlockLocations(namenode, src, offset, prefetchSize);
+        assert (newBlocks != null) : "Could not find target position " + offset;
+        locatedBlocks.insertRange(targetBlockIdx, newBlocks.getLocatedBlocks());
+      }
+      LocatedBlock blk = locatedBlocks.get(targetBlockIdx);
+      // update current position
+      this.pos = offset;
+      this.blockEnd = blk.getStartOffset() + blk.getBlockSize() - 1;
+      this.currentBlock = blk.getBlock();
+      return blk;
+    }
+
+    /**
+     * Get blocks in the specified range.
+     * Fetch them from the namenode if not cached.
+     * 
+     * @param offset
+     * @param length
+     * @return consequent segment of located blocks
+     * @throws IOException
+     */
+    private synchronized List<LocatedBlock> getBlockRange(long offset, 
+                                                          long length) 
+                                                        throws IOException {
+      assert (locatedBlocks != null) : "locatedBlocks is null";
+      List<LocatedBlock> blockRange = new ArrayList<LocatedBlock>();
+      // search cached blocks first
+      int blockIdx = locatedBlocks.findBlock(offset);
+      if (blockIdx < 0) { // block is not cached
+        blockIdx = LocatedBlocks.getInsertIndex(blockIdx);
+      }
+      long remaining = length;
+      long curOff = offset;
+      while(remaining > 0) {
+        LocatedBlock blk = null;
+        if(blockIdx < locatedBlocks.locatedBlockCount())
+          blk = locatedBlocks.get(blockIdx);
+        if (blk == null || curOff < blk.getStartOffset()) {
+          LocatedBlocks newBlocks;
+          newBlocks = callGetBlockLocations(namenode, src, curOff, remaining);
+          locatedBlocks.insertRange(blockIdx, newBlocks.getLocatedBlocks());
+          continue;
+        }
+        assert curOff >= blk.getStartOffset() : "Block not found";
+        blockRange.add(blk);
+        long bytesRead = blk.getStartOffset() + blk.getBlockSize() - curOff;
+        remaining -= bytesRead;
+        curOff += bytesRead;
+        blockIdx++;
+      }
+      return blockRange;
+    }
+
+    /**
+     * Open a DataInputStream to a DataNode so that it can be read from.
+     * We get block ID and the IDs of the destinations at startup, from the namenode.
+     */
+    private synchronized DatanodeInfo blockSeekTo(long target) throws IOException {
+      if (target >= getFileLength()) {
+        throw new IOException("Attempted to read past end of file");
+      }
+
+      if ( blockReader != null ) {
+        blockReader.close(); 
+        blockReader = null;
+      }
+      
+      if (s != null) {
+        s.close();
+        s = null;
+      }
+
+      //
+      // Compute desired block
+      //
+      LocatedBlock targetBlock = getBlockAt(target);
+      assert (target==this.pos) : "Wrong postion " + pos + " expect " + target;
+      long offsetIntoBlock = target - targetBlock.getStartOffset();
+
+      //
+      // Connect to best DataNode for desired Block, with potential offset
+      //
+      DatanodeInfo chosenNode = null;
+      failures = 0;
+      while (s == null) {
+        DNAddrPair retval = chooseDataNode(targetBlock);
+        chosenNode = retval.info;
+        InetSocketAddress targetAddr = retval.addr;
+
+        try {
+          s = socketFactory.createSocket();
+          NetUtils.connect(s, targetAddr, socketTimeout);
+          s.setSoTimeout(socketTimeout);
+          Block blk = targetBlock.getBlock();
+          
+          blockReader = BlockReader.newBlockReader(s, src, blk.getBlockId(), 
+              blk.getGenerationStamp(),
+              offsetIntoBlock, blk.getNumBytes() - offsetIntoBlock,
+              buffersize, verifyChecksum, clientName);
+          return chosenNode;
+        } catch (IOException ex) {
+          // Put chosen node into dead list, continue
+          LOG.debug("Failed to connect to " + targetAddr + ":" 
+                    + StringUtils.stringifyException(ex));
+          addToDeadNodes(chosenNode);
+          if (s != null) {
+            try {
+              s.close();
+            } catch (IOException iex) {
+            }                        
+          }
+          s = null;
+        }
+      }
+      return chosenNode;
+    }
+
+    /**
+     * Close it down!
+     */
+    @Override
+    public synchronized void close() throws IOException {
+      if (closed) {
+        return;
+      }
+      checkOpen();
+      
+      if ( blockReader != null ) {
+        blockReader.close();
+        blockReader = null;
+      }
+      
+      if (s != null) {
+        s.close();
+        s = null;
+      }
+      super.close();
+      closed = true;
+    }
+
+    @Override
+    public synchronized int read() throws IOException {
+      int ret = read( oneByteBuf, 0, 1 );
+      return ( ret <= 0 ) ? -1 : (oneByteBuf[0] & 0xff);
+    }
+
+    /* This is a used by regular read() and handles ChecksumExceptions.
+     * name readBuffer() is chosen to imply similarity to readBuffer() in
+     * ChecksuFileSystem
+     */ 
+    private synchronized int readBuffer(byte buf[], int off, int len) 
+                                                    throws IOException {
+      IOException ioe;
+      
+      /* we retry current node only once. So this is set to true only here.
+       * Intention is to handle one common case of an error that is not a
+       * failure on datanode or client : when DataNode closes the connection
+       * since client is idle. If there are other cases of "non-errors" then
+       * then a datanode might be retried by setting this to true again.
+       */
+      boolean retryCurrentNode = true;
+ 
+      while (true) {
+        // retry as many times as seekToNewSource allows.
+        try {
+          return blockReader.read(buf, off, len);
+        } catch ( ChecksumException ce ) {
+          LOG.warn("Found Checksum error for " + currentBlock + " from " +
+                   currentNode.getName() + " at " + ce.getPos());          
+          reportChecksumFailure(src, currentBlock, currentNode);
+          ioe = ce;
+          retryCurrentNode = false;
+        } catch ( IOException e ) {
+          if (!retryCurrentNode) {
+            LOG.warn("Exception while reading from " + currentBlock +
+                     " of " + src + " from " + currentNode + ": " +
+                     StringUtils.stringifyException(e));
+          }
+          ioe = e;
+        }
+        boolean sourceFound = false;
+        if (retryCurrentNode) {
+          /* possibly retry the same node so that transient errors don't
+           * result in application level failures (e.g. Datanode could have
+           * closed the connection because the client is idle for too long).
+           */ 
+          sourceFound = seekToBlockSource(pos);
+        } else {
+          addToDeadNodes(currentNode);
+          sourceFound = seekToNewSource(pos);
+        }
+        if (!sourceFound) {
+          throw ioe;
+        }
+        retryCurrentNode = false;
+      }
+    }
+
+    /**
+     * Read the entire buffer.
+     */
+    @Override
+    public synchronized int read(byte buf[], int off, int len) throws IOException {
+      checkOpen();
+      if (closed) {
+        throw new IOException("Stream closed");
+      }
+      if (pos < getFileLength()) {
+        int retries = 2;
+        while (retries > 0) {
+          try {
+            if (pos > blockEnd) {
+              currentNode = blockSeekTo(pos);
+            }
+            int realLen = Math.min(len, (int) (blockEnd - pos + 1));
+            int result = readBuffer(buf, off, realLen);
+            
+            if (result >= 0) {
+              pos += result;
+            } else {
+              // got a EOS from reader though we expect more data on it.
+              throw new IOException("Unexpected EOS from the reader");
+            }
+            if (stats != null && result != -1) {
+              stats.incrementBytesRead(result);
+            }
+            return result;
+          } catch (ChecksumException ce) {
+            throw ce;            
+          } catch (IOException e) {
+            if (retries == 1) {
+              LOG.warn("DFS Read: " + StringUtils.stringifyException(e));
+            }
+            blockEnd = -1;
+            if (currentNode != null) { addToDeadNodes(currentNode); }
+            if (--retries == 0) {
+              throw e;
+            }
+          }
+        }
+      }
+      return -1;
+    }
+
+        
+    private DNAddrPair chooseDataNode(LocatedBlock block)
+      throws IOException {
+      while (true) {
+        DatanodeInfo[] nodes = block.getLocations();
+        DatanodeInfo chosenNode = null;
+        try {
+          chosenNode = bestNode(nodes, deadNodes);
+          InetSocketAddress targetAddr = 
+                            NetUtils.createSocketAddr(chosenNode.getName());
+          return new DNAddrPair(chosenNode, targetAddr);
+        } catch (IOException ie) {
+          String blockInfo = block.getBlock() + " file=" + src;
+          if (failures >= maxBlockAcquireFailures) {
+            throw new BlockMissingException(src, "Could not obtain block: " + blockInfo, block.getStartOffset());
+          }
+          
+          if (nodes == null || nodes.length == 0) {
+            LOG.info("No node available for block: " + blockInfo);
+          }
+          LOG.info("Could not obtain block " + block.getBlock() + 
+                   " from node:  " + (chosenNode == null ? " " : chosenNode.getHostName()) +
+                   ie);
+          try {
+            // Introducing a random factor to the wait time before another retry.
+            // The wait time is dependent on # of failures and a random factor.
+            // At the first time of getting a BlockMissingException, the wait time
+            // is a random number between 0..3000 ms. If the first retry
+            // still fails, we will wait 3000 ms grace period before the 2nd retry.
+            // Also at the second retry, the waiting window is expanded to 6000 ms
+            // alleviating the request rate from the server. Similarly the 3rd retry
+            // will wait 6000ms grace period before retry and the waiting window is
+            // expanded to 9000ms. 
+            double waitTime = timeWindow * failures +       // grace period for the last round of attempt
+              timeWindow * (failures + 1) * r.nextDouble(); // expanding time window for each failure
+            LOG.warn("DFS chooseDataNode: got # " + (failures + 1) + " IOException, will wait for " + waitTime + " msec.");
+            Thread.sleep((long)waitTime);
+          } catch (InterruptedException iex) {
+          }
+          deadNodes.clear(); //2nd option is to remove only nodes[blockId]
+          openInfo();
+          block = getBlockAt(block.getStartOffset());
+          failures++;
+          continue;
+        }
+      }
+    } 
+        
+    private void fetchBlockByteRange(LocatedBlock block, long start,
+                                     long end, byte[] buf, int offset) throws IOException {
+      //
+      // Connect to best DataNode for desired Block, with potential offset
+      //
+      Socket dn = null;
+      int numAttempts = block.getLocations().length;
+      IOException ioe = null;
+      failures = 0;
+      
+      while (dn == null && numAttempts-- > 0 ) {
+        DNAddrPair retval = chooseDataNode(block);
+        DatanodeInfo chosenNode = retval.info;
+        InetSocketAddress targetAddr = retval.addr;
+        BlockReader reader = null;
+            
+        try {
+          dn = socketFactory.createSocket();
+          NetUtils.connect(dn, targetAddr, socketTimeout);
+          dn.setSoTimeout(socketTimeout);
+              
+          int len = (int) (end - start + 1);
+              
+          reader = BlockReader.newBlockReader(dn, src, 
+                                              block.getBlock().getBlockId(),
+                                              block.getBlock().getGenerationStamp(),
+                                              start, len, buffersize, 
+                                              verifyChecksum, clientName);
+          int nread = reader.readAll(buf, offset, len);
+          if (nread != len) {
+            throw new IOException("truncated return from reader.read(): " +
+                                  "excpected " + len + ", got " + nread);
+          }
+          return;
+        } catch (ChecksumException e) {
+          ioe = e;
+          LOG.warn("fetchBlockByteRange(). Got a checksum exception for " +
+                   src + " at " + block.getBlock() + ":" + 
+                   e.getPos() + " from " + chosenNode.getName());
+          reportChecksumFailure(src, block.getBlock(), chosenNode);
+        } catch (IOException e) {
+          ioe = e;
+          LOG.warn("Failed to connect to " + targetAddr + 
+                   " for file " + src + 
+                   " for block " + block.getBlock().getBlockId() + ":"  +
+                   StringUtils.stringifyException(e));
+        } finally {
+          IOUtils.closeStream(reader);
+          IOUtils.closeSocket(dn);
+          dn = null;
+        }
+        // Put chosen node into dead list, continue
+        addToDeadNodes(chosenNode);
+      }
+      throw (ioe == null) ? new IOException("Could not read data") : ioe;
+    }
+
+    /**
+     * Read bytes starting from the specified position.
+     * 
+     * @param position start read from this position
+     * @param buffer read buffer
+     * @param offset offset into buffer
+     * @param length number of bytes to read
+     * 
+     * @return actual number of bytes read
+     */
+    @Override
+    public int read(long position, byte[] buffer, int offset, int length)
+      throws IOException {
+      // sanity checks
+      checkOpen();
+      if (closed) {
+        throw new IOException("Stream closed");
+      }
+      long filelen = getFileLength();
+      if ((position < 0) || (position >= filelen)) {
+        return -1;
+      }
+      int realLen = length;
+      if ((position + length) > filelen) {
+        realLen = (int)(filelen - position);
+      }
+      
+      // determine the block and byte range within the block
+      // corresponding to position and realLen
+      List<LocatedBlock> blockRange = getBlockRange(position, realLen);
+      int remaining = realLen;
+      for (LocatedBlock blk : blockRange) {
+        long targetStart = position - blk.getStartOffset();
+        long bytesToRead = Math.min(remaining, blk.getBlockSize() - targetStart);
+        fetchBlockByteRange(blk, targetStart, 
+                            targetStart + bytesToRead - 1, buffer, offset);
+        remaining -= bytesToRead;
+        position += bytesToRead;
+        offset += bytesToRead;
+      }
+      assert remaining == 0 : "Wrong number of bytes read.";
+      if (stats != null) {
+        stats.incrementBytesRead(realLen);
+      }
+      return realLen;
+    }
+     
+    @Override
+    public long skip(long n) throws IOException {
+      if ( n > 0 ) {
+        long curPos = getPos();
+        long fileLen = getFileLength();
+        if( n+curPos > fileLen ) {
+          n = fileLen - curPos;
+        }
+        seek(curPos+n);
+        return n;
+      }
+      return n < 0 ? -1 : 0;
+    }
+
+    /**
+     * Seek to a new arbitrary location
+     */
+    @Override
+    public synchronized void seek(long targetPos) throws IOException {
+      if (targetPos > getFileLength()) {
+        throw new IOException("Cannot seek after EOF");
+      }
+      boolean done = false;
+      if (pos <= targetPos && targetPos <= blockEnd) {
+        //
+        // If this seek is to a positive position in the current
+        // block, and this piece of data might already be lying in
+        // the TCP buffer, then just eat up the intervening data.
+        //
+        int diff = (int)(targetPos - pos);
+        if (diff <= TCP_WINDOW_SIZE) {
+          try {
+            pos += blockReader.skip(diff);
+            if (pos == targetPos) {
+              done = true;
+            }
+          } catch (IOException e) {//make following read to retry
+            LOG.debug("Exception while seek to " + targetPos + " from "
+                      + currentBlock +" of " + src + " from " + currentNode + 
+                      ": " + StringUtils.stringifyException(e));
+          }
+        }
+      }
+      if (!done) {
+        pos = targetPos;
+        blockEnd = -1;
+      }
+    }
+
+    /**
+     * Same as {@link #seekToNewSource(long)} except that it does not exclude
+     * the current datanode and might connect to the same node.
+     */
+    private synchronized boolean seekToBlockSource(long targetPos)
+                                                   throws IOException {
+      currentNode = blockSeekTo(targetPos);
+      return true;
+    }
+    
+    /**
+     * Seek to given position on a node other than the current node.  If
+     * a node other than the current node is found, then returns true. 
+     * If another node could not be found, then returns false.
+     */
+    @Override
+    public synchronized boolean seekToNewSource(long targetPos) throws IOException {
+      boolean markedDead = deadNodes.containsKey(currentNode);
+      addToDeadNodes(currentNode);
+      DatanodeInfo oldNode = currentNode;
+      DatanodeInfo newNode = blockSeekTo(targetPos);
+      if (!markedDead) {
+        /* remove it from deadNodes. blockSeekTo could have cleared 
+         * deadNodes and added currentNode again. Thats ok. */
+        deadNodes.remove(oldNode);
+      }
+      if (!oldNode.getStorageID().equals(newNode.getStorageID())) {
+        currentNode = newNode;
+        return true;
+      } else {
+        return false;
+      }
+    }
+        
+    /**
+     */
+    @Override
+    public synchronized long getPos() throws IOException {
+      return pos;
+    }
+
+    /**
+     */
+    @Override
+    public synchronized int available() throws IOException {
+      if (closed) {
+        throw new IOException("Stream closed");
+      }
+      return (int) (getFileLength() - pos);
+    }
+
+    /**
+     * We definitely don't support marks
+     */
+    @Override
+    public boolean markSupported() {
+      return false;
+    }
+    @Override
+    public void mark(int readLimit) {
+    }
+    @Override
+    public void reset() throws IOException {
+      throw new IOException("Mark/reset not supported");
+    }
+  }
+    
+  static class DFSDataInputStream extends FSDataInputStream {
+    DFSDataInputStream(DFSInputStream in)
+      throws IOException {
+      super(in);
+    }
+      
+    /**
+     * Returns the datanode from which the stream is currently reading.
+     */
+    public DatanodeInfo getCurrentDatanode() {
+      return ((DFSInputStream)in).getCurrentDatanode();
+    }
+      
+    /**
+     * Returns the block containing the target position. 
+     */
+    public Block getCurrentBlock() {
+      return ((DFSInputStream)in).getCurrentBlock();
+    }
+
+    /**
+     * Return collection of blocks that has already been located.
+     */
+    synchronized List<LocatedBlock> getAllBlocks() throws IOException {
+      return ((DFSInputStream)in).getAllBlocks();
+    }
+
+  }
+
+  /****************************************************************
+   * DFSOutputStream creates files from a stream of bytes.
+   *
+   * The client application writes data that is cached internally by
+   * this stream. Data is broken up into packets, each packet is
+   * typically 64K in size. A packet comprises of chunks. Each chunk
+   * is typically 512 bytes and has an associated checksum with it.
+   *
+   * When a client application fills up the currentPacket, it is
+   * enqueued into dataQueue.  The DataStreamer thread picks up
+   * packets from the dataQueue, sends it to the first datanode in
+   * the pipeline and moves it from the dataQueue to the ackQueue.
+   * The ResponseProcessor receives acks from the datanodes. When an
+   * successful ack for a packet is received from all datanodes, the
+   * ResponseProcessor removes the corresponding packet from the
+   * ackQueue.
+   *
+   * In case of error, all outstanding packets and moved from
+   * ackQueue. A new pipeline is setup by eliminating the bad
+   * datanode from the original pipeline. The DataStreamer now
+   * starts sending packets from the dataQueue.
+  ****************************************************************/
+  class DFSOutputStream extends FSOutputSummer implements Syncable {
+    private Socket s;
+    boolean closed = false;
+  
+    private String src;
+    private DataOutputStream blockStream;
+    private DataInputStream blockReplyStream;
+    private Block block;
+    final private long blockSize;
+    private DataChecksum checksum;
+    private LinkedList<Packet> dataQueue = new LinkedList<Packet>();
+    private LinkedList<Packet> ackQueue = new LinkedList<Packet>();
+    private Packet currentPacket = null;
+    private int maxPackets = 80; // each packet 64K, total 5MB
+    // private int maxPackets = 1000; // each packet 64K, total 64MB
+    private DataStreamer streamer = new DataStreamer();;
+    private ResponseProcessor response = null;
+    private long currentSeqno = 0;
+    private long bytesCurBlock = 0; // bytes writen in current block
+    private int packetSize = 0; // write packet size, including the header.
+    private int chunksPerPacket = 0;
+    private DatanodeInfo[] nodes = null; // list of targets for current block
+    private volatile boolean hasError = false;
+    private volatile int errorIndex = 0;
+    private volatile IOException lastException = null;
+    private long artificialSlowdown = 0;
+    private long lastFlushOffset = -1; // offset when flush was invoked
+    private boolean persistBlocks = false; // persist blocks on namenode
+    private int recoveryErrorCount = 0; // number of times block recovery failed
+    private int maxRecoveryErrorCount = 5; // try block recovery 5 times
+    private volatile boolean appendChunk = false;   // appending to existing partial block
+    private long initialFileSize = 0; // at time of file open
+
+    private void setLastException(IOException e) {
+      if (lastException == null) {
+        lastException = e;
+      }
+    }
+    
+    private class Packet {
+      ByteBuffer buffer;           // only one of buf and buffer is non-null
+      byte[]  buf;
+      long    seqno;               // sequencenumber of buffer in block
+      long    offsetInBlock;       // offset in block
+      boolean lastPacketInBlock;   // is this the last packet in block?
+      int     numChunks;           // number of chunks currently in packet
+      int     maxChunks;           // max chunks in packet
+      int     dataStart;
+      int     dataPos;
+      int     checksumStart;
+      int     checksumPos;      
+  
+      // create a new packet
+      Packet(int pktSize, int chunksPerPkt, long offsetInBlock) {
+        this.lastPacketInBlock = false;
+        this.numChunks = 0;
+        this.offsetInBlock = offsetInBlock;
+        this.seqno = currentSeqno;
+        currentSeqno++;
+        
+        buffer = null;
+        buf = new byte[pktSize];
+        
+        checksumStart = DataNode.PKT_HEADER_LEN + SIZE_OF_INTEGER;
+        checksumPos = checksumStart;
+        dataStart = checksumStart + chunksPerPkt * checksum.getChecksumSize();
+        dataPos = dataStart;
+        maxChunks = chunksPerPkt;
+      }
+
+      void writeData(byte[] inarray, int off, int len) {
+        if ( dataPos + len > buf.length) {
+          throw new BufferOverflowException();
+        }
+        System.arraycopy(inarray, off, buf, dataPos, len);
+        dataPos += len;
+      }
+  
+      void  writeChecksum(byte[] inarray, int off, int len) {
+        if (checksumPos + len > dataStart) {
+          throw new BufferOverflowException();
+        }
+        System.arraycopy(inarray, off, buf, checksumPos, len);
+        checksumPos += len;
+      }
+      
+      /**
+       * Returns ByteBuffer that contains one full packet, including header.
+       */
+      ByteBuffer getBuffer() {
+        /* Once this is called, no more data can be added to the packet.
+         * setting 'buf' to null ensures that.
+         * This is called only when the packet is ready to be sent.
+         */
+        if (buffer != null) {
+          return buffer;
+        }
+        
+        //prepare the header and close any gap between checksum and data.
+        
+        int dataLen = dataPos - dataStart;
+        int checksumLen = checksumPos - checksumStart;
+        
+        if (checksumPos != dataStart) {
+          /* move the checksum to cover the gap.
+           * This can happen for the last packet.
+           */
+          System.arraycopy(buf, checksumStart, buf, 
+                           dataStart - checksumLen , checksumLen); 
+        }
+        
+        int pktLen = SIZE_OF_INTEGER + dataLen + checksumLen;
+        
+        //normally dataStart == checksumPos, i.e., offset is zero.
+        buffer = ByteBuffer.wrap(buf, dataStart - checksumPos,
+                                 DataNode.PKT_HEADER_LEN + pktLen);
+        buf = null;
+        buffer.mark();
+        
+        /* write the header and data length.
+         * The format is described in comment before DataNode.BlockSender
+         */
+        buffer.putInt(pktLen);  // pktSize
+        buffer.putLong(offsetInBlock); 
+        buffer.putLong(seqno);
+        buffer.put((byte) ((lastPacketInBlock) ? 1 : 0));
+        //end of pkt header
+        buffer.putInt(dataLen); // actual data length, excluding checksum.
+        
+        buffer.reset();
+        return buffer;
+      }
+    }
+  
+    //
+    // The DataStreamer class is responsible for sending data packets to the
+    // datanodes in the pipeline. It retrieves a new blockid and block locations
+    // from the namenode, and starts streaming packets to the pipeline of
+    // Datanodes. Every packet has a sequence number associated with
+    // it. When all the packets for a block are sent out and acks for each
+    // if them are received, the DataStreamer closes the current block.
+    //
+    private class DataStreamer extends Daemon {
+
+      private volatile boolean closed = false;
+  
+      public void run() {
+        while (!closed && clientRunning) {
+
+          // if the Responder encountered an error, shutdown Responder
+          if (hasError && response != null) {
+            try {
+              response.close();
+              response.join();
+              response = null;
+            } catch (InterruptedException  e) {
+            }
+          }
+
+          Packet one = null;
+          synchronized (dataQueue) {
+
+            // process IO errors if any
+            boolean doSleep = processDatanodeError(hasError, false);
+
+            // wait for a packet to be sent.
+            while ((!closed && !hasError && clientRunning 
+                   && dataQueue.size() == 0) || doSleep) {
+              try {
+                dataQueue.wait(1000);
+              } catch (InterruptedException  e) {
+              }
+              doSleep = false;
+            }
+            if (closed || hasError || dataQueue.size() == 0 || !clientRunning) {
+              continue;
+            }
+
+            try {
+              // get packet to be sent.
+              one = dataQueue.getFirst();
+              long offsetInBlock = one.offsetInBlock;
+  
+              // get new block from namenode.
+              if (blockStream == null) {
+                LOG.debug("Allocating new block");
+                nodes = nextBlockOutputStream(src); 
+                this.setName("DataStreamer for file " + src +
+                             " block " + block);
+                response = new ResponseProcessor(nodes);
+                response.start();
+              }
+
+              if (offsetInBlock >= blockSize) {
+                throw new IOException("BlockSize " + blockSize +
+                                      " is smaller than data size. " +
+                                      " Offset of packet in block " + 
+                                      offsetInBlock +
+                                      " Aborting file " + src);
+              }
+
+              ByteBuffer buf = one.getBuffer();
+              
+              // move packet from dataQueue to ackQueue
+              dataQueue.removeFirst();
+              dataQueue.notifyAll();
+              synchronized (ackQueue) {
+                ackQueue.addLast(one);
+                ackQueue.notifyAll();
+              } 
+              
+              // write out data to remote datanode
+              blockStream.write(buf.array(), buf.position(), buf.remaining());
+              
+              if (one.lastPacketInBlock) {
+                blockStream.writeInt(0); // indicate end-of-block 
+              }
+              blockStream.flush();
+              if (LOG.isDebugEnabled()) {
+                LOG.debug("DataStreamer block " + block +
+                          " wrote packet seqno:" + one.seqno +
+                          " size:" + buf.remaining() +
+                          " offsetInBlock:" + one.offsetInBlock + 
+                          " lastPacketInBlock:" + one.lastPacketInBlock);
+              }
+            } catch (Throwable e) {
+              LOG.warn("DataStreamer Exception: " + 
+                       StringUtils.stringifyException(e));
+              if (e instanceof IOException) {
+                setLastException((IOException)e);
+              }
+              hasError = true;
+            }
+          }
+
+          if (closed || hasError || !clientRunning) {
+            continue;
+          }
+
+          // Is this block full?
+          if (one.lastPacketInBlock) {
+            synchronized (ackQueue) {
+              while (!hasError && ackQueue.size() != 0 && clientRunning) {
+                try {
+                  ackQueue.wait();   // wait for acks to arrive from datanodes
+                } catch (InterruptedException  e) {
+                }
+              }
+            }
+            LOG.debug("Closing old block " + block);
+            this.setName("DataStreamer for file " + src);
+
+            response.close();        // ignore all errors in Response
+            try {
+              response.join();
+              response = null;
+            } catch (InterruptedException  e) {
+            }
+
+            if (closed || hasError || !clientRunning) {
+              continue;
+            }
+
+            synchronized (dataQueue) {
+              try {
+                blockStream.close();
+                blockReplyStream.close();
+              } catch (IOException e) {
+              }
+              nodes = null;
+              response = null;
+              blockStream = null;
+              blockReplyStream = null;
+            }
+          }
+          if (progress != null) { progress.progress(); }
+
+          // This is used by unit test to trigger race conditions.
+          if (artificialSlowdown != 0 && clientRunning) {
+            try { 
+              Thread.sleep(artificialSlowdown); 
+            } catch (InterruptedException e) {}
+          }
+        }
+      }
+
+      // shutdown thread
+      void close() {
+        closed = true;
+        synchronized (dataQueue) {
+          dataQueue.notifyAll();
+        }
+        synchronized (ackQueue) {
+          ackQueue.notifyAll();
+        }
+        this.interrupt();
+      }
+    }
+                  
+    //
+    // Processes reponses from the datanodes.  A packet is removed 
+    // from the ackQueue when its response arrives.
+    //
+    private class ResponseProcessor extends Thread {
+
+      private volatile boolean closed = false;
+      private DatanodeInfo[] targets = null;
+      private boolean lastPacketInBlock = false;
+
+      ResponseProcessor (DatanodeInfo[] targets) {
+        this.targets = targets;
+      }
+
+      public void run() {
+
+        this.setName("ResponseProcessor for block " + block);
+  
+        while (!closed && clientRunning && !lastPacketInBlock) {
+          // process responses from datanodes.
+          try {
+            // verify seqno from datanode
+            long seqno = blockReplyStream.readLong();
+            LOG.debug("AvatarClient received ack for seqno " + seqno);
+            if (seqno == -1) {
+              continue;
+            } else if (seqno == -2) {
+              // no nothing
+            } else {
+              Packet one = null;
+              synchronized (ackQueue) {
+                one = ackQueue.getFirst();
+              }
+              if (one.seqno != seqno) {
+                throw new IOException("Responseprocessor: Expecting seqno " + 
+                                      " for block " + block +
+                                      one.seqno + " but received " + seqno);
+              }
+              lastPacketInBlock = one.lastPacketInBlock;
+            }
+
+            // processes response status from all datanodes.
+            for (int i = 0; i < targets.length && clientRunning; i++) {
+              short reply = blockReplyStream.readShort();
+              if (reply != DataTransferProtocol.OP_STATUS_SUCCESS) {
+                errorIndex = i; // first bad datanode
+                throw new IOException("Bad response " + reply +
+                                      " for block " + block +
+                                      " from datanode " + 
+                                      targets[i].getName());
+              }
+            }
+
+            synchronized (ackQueue) {
+              ackQueue.removeFirst();
+              ackQueue.notifyAll();
+            }
+          } catch (Exception e) {
+            if (!closed) {
+              hasError = true;
+              if (e instanceof IOException) {
+                setLastException((IOException)e);
+              }
+              LOG.warn("DFSOutputStream ResponseProcessor exception " + 
+                       " for block " + block +
+                        StringUtils.stringifyException(e));
+              closed = true;
+            }
+          }
+
+          synchronized (dataQueue) {
+            dataQueue.notifyAll();
+          }
+          synchronized (ackQueue) {
+            ackQueue.notifyAll();
+          }
+        }
+      }
+
+      void close() {
+        closed = true;
+        this.interrupt();
+      }
+    }
+
+    // If this stream has encountered any errors so far, shutdown 
+    // threads and mark stream as closed. Returns true if we should
+    // sleep for a while after returning from this call.
+    //
+    private boolean processDatanodeError(boolean hasError, boolean isAppend) {
+      if (!hasError) {
+        return false;
+      }
+      if (response != null) {
+        LOG.info("Error Recovery for block " + block +
+                 " waiting for responder to exit. ");
+        return true;
+      }
+      if (errorIndex >= 0) {
+        LOG.warn("Error Recovery for block " + block
+            + " bad datanode[" + errorIndex + "] "
+            + (nodes == null? "nodes == null": nodes[errorIndex].getName()));
+      }
+
+      if (blockStream != null) {
+        try {
+          blockStream.close();
+          blockReplyStream.close();
+        } catch (IOException e) {
+        }
+      }
+      blockStream = null;
+      blockReplyStream = null;
+
+      // move packets from ack queue to front of the data queue
+      synchronized (ackQueue) {
+        dataQueue.addAll(0, ackQueue);
+        ackQueue.clear();
+      }
+
+      boolean success = false;
+      while (!success && clientRunning) {
+        DatanodeInfo[] newnodes = null;
+        if (nodes == null) {
+          String msg = "Could not get block locations. " +
+                                          "Source file \"" + src
+                                          + "\" - Aborting...";
+          LOG.warn(msg);
+          setLastException(new IOException(msg));
+          closed = true;
+          if (streamer != null) streamer.close();
+          return false;
+        }
+        StringBuilder pipelineMsg = new StringBuilder();
+        for (int j = 0; j < nodes.length; j++) {
+          pipelineMsg.append(nodes[j].getName());
+          if (j < nodes.length - 1) {
+            pipelineMsg.append(", ");
+          }
+        }
+        // remove bad datanode from list of datanodes.
+        // If errorIndex was not set (i.e. appends), then do not remove 
+        // any datanodes
+        // 
+        if (errorIndex < 0) {
+          newnodes = nodes;
+        } else {
+          if (nodes.length <= 1) {
+            lastException = new IOException("All datanodes " + pipelineMsg + 
+                                            " are bad. Aborting...");
+            closed = true;
+            if (streamer != null) streamer.close();
+            return false;
+          }
+          LOG.warn("Error Recovery for block " + block +
+                   " in pipeline " + pipelineMsg + 
+                   ": bad datanode " + nodes[errorIndex].getName());
+          newnodes =  new DatanodeInfo[nodes.length-1];
+          System.arraycopy(nodes, 0, newnodes, 0, errorIndex);
+          System.arraycopy(nodes, errorIndex+1, newnodes, errorIndex,
+              newnodes.length-errorIndex);
+        }
+
+        // Tell the primary datanode to do error recovery 
+        // by stamping appropriate generation stamps.
+        //
+        LocatedBlock newBlock = null;
+        ClientDatanodeProtocol primary =  null;
+        DatanodeInfo primaryNode = null;
+        try {
+          // Pick the "least" datanode as the primary datanode to avoid deadlock.
+          primaryNode = Collections.min(Arrays.asList(newnodes));
+          primary = createClientDatanodeProtocolProxy(primaryNode, conf);
+          newBlock = primary.recoverBlock(block, isAppend, newnodes);
+        } catch (IOException e) {
+          recoveryErrorCount++;
+          if (recoveryErrorCount > maxRecoveryErrorCount) {
+            if (nodes.length > 1) {
+              // if the primary datanode failed, remove it from the list.
+              // The original bad datanode is left in the list because it is
+              // conservative to remove only one datanode in one iteration.
+              for (int j = 0; j < nodes.length; j++) {
+                if (nodes[j].equals(primaryNode)) {
+                  errorIndex = j; // forget original bad node.
+                }
+              }
+              // remove primary node from list
+              newnodes =  new DatanodeInfo[nodes.length-1];
+              System.arraycopy(nodes, 0, newnodes, 0, errorIndex);
+              System.arraycopy(nodes, errorIndex+1, newnodes, errorIndex,
+                               newnodes.length-errorIndex);
+              nodes = newnodes;
+              LOG.warn("Error Recovery for block " + block + " failed " +
+                       " because recovery from primary datanode " +
+                       primaryNode + " failed " + recoveryErrorCount +
+                       " times. " + " Pipeline was " + pipelineMsg +
+                       ". Marking primary datanode as bad.");
+              recoveryErrorCount = 0; 
+              errorIndex = -1;
+              return true;          // sleep when we return from here
+            }
+            String emsg = "Error Recovery for block " + block + " failed " +
+                          " because recovery from primary datanode " +
+                          primaryNode + " failed " + recoveryErrorCount + 
+                          " times. "  + " Pipeline was " + pipelineMsg +
+                          ". Aborting...";
+            LOG.warn(emsg);
+            lastException = new IOException(emsg);
+            closed = true;
+            if (streamer != null) streamer.close();
+            return false;       // abort with IOexception
+          } 
+          LOG.warn("Error Recovery for block " + block + " failed " +
+                   " because recovery from primary datanode " +
+                   primaryNode + " failed " + recoveryErrorCount +
+                   " times. "  + " Pipeline was " + pipelineMsg +
+                   ". Will retry...");
+          return true;          // sleep when we return from here
+        } finally {
+          RPC.stopProxy(primary);
+        }
+        recoveryErrorCount = 0; // block recovery successful
+
+        // If the block recovery generated a new generation stamp, use that
+        // from now on.  Also, setup new pipeline
+        //
+        if (newBlock != null) {
+          block = newBlock.getBlock();
+          nodes = newBlock.getLocations();
+        }
+
+        this.hasError = false;
+        lastException = null;
+        errorIndex = 0;
+        success = createBlockOutputStream(nodes, clientName, true);
+      }
+
+      response = new ResponseProcessor(nodes);
+      response.start();
+      return false; // do not sleep, continue processing
+    }
+
+    private void isClosed() throws IOException {
+      if (closed && lastException != null) {
+          throw lastException;
+      }
+    }
+
+    //
+    // returns the list of targets, if any, that is being currently used.
+    //
+    DatanodeInfo[] getPipeline() {
+      synchronized (dataQueue) {
+        if (nodes == null) {
+          return null;
+        }
+        DatanodeInfo[] value = new DatanodeInfo[nodes.length];
+        for (int i = 0; i < nodes.length; i++) {
+          value[i] = nodes[i];
+        }
+        return value;
+      }
+    }
+
+    private Progressable progress;
+
+    private DFSOutputStream(String src, long blockSize, Progressable progress,
+        int bytesPerChecksum) throws IOException {
+      super(new CRC32(), bytesPerChecksum, 4);
+      this.src = src;
+      this.blockSize = blockSize;
+      this.progress = progress;
+      if (progress != null) {
+        LOG.debug("Set non-null progress callback on DFSOutputStream "+src);
+      }
+      
+      if ( bytesPerChecksum < 1 || blockSize % bytesPerChecksum != 0) {
+        throw new IOException("io.bytes.per.checksum(" + bytesPerChecksum +
+                              ") and blockSize(" + blockSize + 
+                              ") do not match. " + "blockSize should be a " +
+                              "multiple of io.bytes.per.checksum");
+                              
+      }
+      checksum = DataChecksum.newDataChecksum(DataChecksum.CHECKSUM_CRC32, 
+                                              bytesPerChecksum);
+    }
+
+    /**
+     * Create a new output stream to the given DataNode.
+     * @see AvatarProtocol#create(String, FsPermission, String, boolean, short, long)
+     */
+    DFSOutputStream(String src, FsPermission masked, boolean overwrite,
+        short replication, long blockSize, Progressable progress,
+        int buffersize, int bytesPerChecksum) throws IOException {
+      this(src, blockSize, progress, bytesPerChecksum);
+
+      computePacketChunkSize(writePacketSize, bytesPerChecksum);
+
+      try {
+        namenode.create(
+            src, masked, clientName, overwrite, replication, blockSize);
+      } catch(RemoteException re) {
+        throw re.unwrapRemoteException(AccessControlException.class,
+                                       NSQuotaExceededException.class,
+                                       DSQuotaExceededException.class);
+      }
+      streamer.start();
+    }
+  
+    /**
+     * Create a new output stream to the given DataNode.
+     * @see AvatarProtocol#create(String, FsPermission, String, boolean, short, long)
+     */
+    DFSOutputStream(String src, int buffersize, Progressable progress,
+        LocatedBlock lastBlock, FileStatus stat,
+        int bytesPerChecksum) throws IOException {
+      this(src, stat.getBlockSize(), progress, bytesPerChecksum);
+      initialFileSize = stat.getLen(); // length of file when opened
+
+      //
+      // The last partial block of the file has to be filled.
+      //
+      if (lastBlock != null) {
+        block = lastBlock.getBlock();
+        long usedInLastBlock = stat.getLen() % blockSize;
+        int freeInLastBlock = (int)(blockSize - usedInLastBlock);
+
+        // calculate the amount of free space in the pre-existing 
+        // last crc chunk
+        int usedInCksum = (int)(stat.getLen() % bytesPerChecksum);
+        int freeInCksum = bytesPerChecksum - usedInCksum;
+
+        // if there is space in the last block, then we have to 
+        // append to that block
+        if (freeInLastBlock > blockSize) {
+          throw new IOException("The last block for file " + 
+                                src + " is full.");
+        }
+
+        // indicate that we are appending to an existing block
+        bytesCurBlock = lastBlock.getBlockSize();
+
+        if (usedInCksum > 0 && freeInCksum > 0) {
+          // if there is space in the last partial chunk, then 
+          // setup in such a way that the next packet will have only 
+          // one chunk that fills up the partial chunk.
+          //
+          computePacketChunkSize(0, freeInCksum);
+          resetChecksumChunk(freeInCksum);
+          this.appendChunk = true;
+        } else {
+          // if the remaining space in the block is smaller than 
+          // that expected size of of a packet, then create 
+          // smaller size packet.
+          //
+          computePacketChunkSize(Math.min(writePacketSize, freeInLastBlock), 
+                                 bytesPerChecksum);
+        }
+
+        // setup pipeline to append to the last block XXX retries??
+        nodes = lastBlock.getLocations();
+        errorIndex = -1;   // no errors yet.
+        if (nodes.length < 1) {
+          throw new IOException("Unable to retrieve blocks locations " +
+                                " for last block " + block +
+                                "of file " + src);
+                        
+        }
+        processDatanodeError(true, true);
+        streamer.start();
+      }
+      else {
+        computePacketChunkSize(writePacketSize, bytesPerChecksum);
+        streamer.start();
+      }
+    }
+
+    private void computePacketChunkSize(int psize, int csize) {
+      int chunkSize = csize + checksum.getChecksumSize();
+      int n = DataNode.PKT_HEADER_LEN + SIZE_OF_INTEGER;
+      chunksPerPacket = Math.max((psize - n + chunkSize-1)/chunkSize, 1);
+      packetSize = n + chunkSize*chunksPerPacket;
+      if (LOG.isDebugEnabled()) {
+        LOG.debug("computePacketChunkSize: src=" + src +
+                  ", chunkSize=" + chunkSize +
+                  ", chunksPerPacket=" + chunksPerPacket +
+                  ", packetSize=" + packetSize);
+      }
+    }
+
+    /**
+     * Open a DataOutputStream to a DataNode so that it can be written to.
+     * This happens when a file is created and each time a new block is allocated.
+     * Must get block ID and the IDs of the destinations from the namenode.
+     * Returns the list of target datanodes.
+     */
+    private DatanodeInfo[] nextBlockOutputStream(String client) throws IOException {
+      LocatedBlock lb = null;
+      boolean retry = false;
+      DatanodeInfo[] nodes;
+      int count = conf.getInt("dfs.client.block.write.retries", 3);
+      boolean success;
+      do {
+        hasError = false;
+        lastException = null;
+        errorIndex = 0;
+        retry = false;
+        nodes = null;
+        success = false;
+                
+        long startTime = System.currentTimeMillis();
+        lb = locateFollowingBlock(startTime);
+        block = lb.getBlock();
+        nodes = lb.getLocations();
+  
+        //
+        // Connect to first DataNode in the list.
+        //
+        success = createBlockOutputStream(nodes, clientName, false);
+
+        if (!success) {
+          LOG.info("Abandoning block " + block);
+          namenode.abandonBlock(block, src, clientName);
+
+          // Connection failed.  Let's wait a little bit and retry
+          retry = true;
+          try {
+            if (System.currentTimeMillis() - startTime > 5000) {
+              LOG.info("Waiting to find target node: " + nodes[0].getName());
+            }
+            Thread.sleep(6000);
+          } catch (InterruptedException iex) {
+          }
+        }
+      } while (retry && --count >= 0);
+
+      if (!success) {
+        throw new IOException("Unable to create new block.");
+      }
+      return nodes;
+    }
+
+    // connects to the first datanode in the pipeline
+    // Returns true if success, otherwise return failure.
+    //
+    private boolean createBlockOutputStream(DatanodeInfo[] nodes, String client,
+                    boolean recoveryFlag) {
+      String firstBadLink = "";
+      if (LOG.isDebugEnabled()) {
+        for (int i = 0; i < nodes.length; i++) {
+          LOG.debug("pipeline = " + nodes[i].getName());
+        }
+      }
+
+      // persist blocks on namenode on next flush
+      persistBlocks = true;
+
+      try {
+        LOG.debug("Connecting to " + nodes[0].getName());
+        InetSocketAddress target = NetUtils.createSocketAddr(nodes[0].getName());
+        s = socketFactory.createSocket();
+        int timeoutValue = 3000 * nodes.length + socketTimeout;
+        NetUtils.connect(s, target, timeoutValue);
+        s.setSoTimeout(timeoutValue);
+        s.setSendBufferSize(DEFAULT_DATA_SOCKET_SIZE);
+        LOG.debug("Send buf size " + s.getSendBufferSize());
+        long writeTimeout = HdfsConstants.WRITE_TIMEOUT_EXTENSION * nodes.length +
+                            datanodeWriteTimeout;
+
+        //
+        // Xmit header info to datanode
+        //
+        DataOutputStream out = new DataOutputStream(
+            new BufferedOutputStream(NetUtils.getOutputStream(s, writeTimeout), 
+                                     DataNode.SMALL_BUFFER_SIZE));
+        blockReplyStream = new DataInputStream(NetUtils.getInputStream(s));
+
+        out.writeShort( DataTransferProtocol.DATA_TRANSFER_VERSION );
+        out.write( DataTransferProtocol.OP_WRITE_BLOCK );
+        out.writeLong( block.getBlockId() );
+        out.writeLong( block.getGenerationStamp() );
+        out.writeInt( nodes.length );
+        out.writeBoolean( recoveryFlag );       // recovery flag
+        Text.writeString( out, client );
+        out.writeBoolean(false); // Not sending src node information
+        out.writeInt( nodes.length - 1 );
+        for (int i = 1; i < nodes.length; i++) {
+          nodes[i].write(out);
+        }
+        checksum.writeHeader( out );
+        out.flush();
+
+        // receive ack for connect
+        firstBadLink = Text.readString(blockReplyStream);
+        if (firstBadLink.length() != 0) {
+          throw new IOException("Bad connect ack with firstBadLink " + firstBadLink);
+        }
+
+        blockStream = out;
+        return true;     // success
+
+      } catch (IOException ie) {
+
+        LOG.info("Exception in createBlockOutputStream " + ie);
+
+        // find the datanode that matches
+        if (firstBadLink.length() != 0) {
+          for (int i = 0; i < nodes.length; i++) {
+            if (nodes[i].getName().equals(firstBadLink)) {
+              errorIndex = i;
+              break;
+            }
+          }
+        }
+        hasError = true;
+        setLastException(ie);
+        blockReplyStream = null;
+        return false;  // error
+      }
+    }
+  
+    private LocatedBlock locateFollowingBlock(long start
+                                              ) throws IOException {     
+      int retries = conf.getInt("dfs.client.block.write.locateFollowingBlock.retries", 5);
+      long sleeptime = 400;
+      while (true) {
+        long localstart = System.currentTimeMillis();
+        while (true) {
+          try {
+            return namenode.addBlock(src, clientName);
+          } catch (RemoteException e) {
+            IOException ue = 
+              e.unwrapRemoteException(FileNotFoundException.class,
+                                      AccessControlException.class,
+                                      NSQuotaExceededException.class,
+                                      DSQuotaExceededException.class);
+            if (ue != e) { 
+              throw ue; // no need to retry these exceptions
+            }
+            
+            if (NotReplicatedYetException.class.getName().
+                equals(e.getClassName())) {
+
+                if (retries == 0) { 
+                  throw e;
+                } else {
+                  --retries;
+                  LOG.info(StringUtils.stringifyException(e));
+                  if (System.currentTimeMillis() - localstart > 5000) {
+                    LOG.info("Waiting for replication for "
+                        + (System.currentTimeMillis() - localstart) / 1000
+                        + " seconds");
+                  }
+                  try {
+                    LOG.warn("NotReplicatedYetException sleeping " + src
+                        + " retries left " + retries);
+                    Thread.sleep(sleeptime);
+                    sleeptime *= 2;
+                  } catch (InterruptedException ie) {
+                  }
+                }
+            } else {
+              throw e;
+            }
+          }
+        }
+      } 
+    }
+  
+    // @see FSOutputSummer#writeChunk()
+    @Override
+    protected synchronized void writeChunk(byte[] b, int offset, int len, byte[] checksum) 
+                                                          throws IOException {
+      checkOpen();
+      isClosed();
+  
+      int cklen = checksum.length;
+      int bytesPerChecksum = this.checksum.getBytesPerChecksum(); 
+      if (len > bytesPerChecksum) {
+        throw new IOException("writeChunk() buffer size is " + len +
+                              " is larger than supported  bytesPerChecksum " +
+                              bytesPerChecksum);
+      }
+      if (checksum.length != this.checksum.getChecksumSize()) {
+        throw new IOException("writeChunk() checksum size is supposed to be " +
+                              this.checksum.getChecksumSize() + 
+                              " but found to be " + checksum.length);
+      }
+
+      synchronized (dataQueue) {
+  
+        // If queue is full, then wait till we can create  enough space
+        while (!closed && dataQueue.size() + ackQueue.size()  > maxPackets) {
+          try {
+            dataQueue.wait();
+          } catch (InterruptedException  e) {
+          }
+        }
+        isClosed();
+  
+        if (currentPacket == null) {
+          currentPacket = new Packet(packetSize, chunksPerPacket, 
+                                     bytesCurBlock);
+          if (LOG.isDebugEnabled()) {
+            LOG.debug("AvatarClient writeChunk allocating new packet seqno=" + 
+                      currentPacket.seqno +
+                      ", src=" + src +
+                      ", packetSize=" + packetSize +
+                      ", chunksPerPacket=" + chunksPerPacket +
+                      ", bytesCurBlock=" + bytesCurBlock);
+          }
+        }
+
+        currentPacket.writeChecksum(checksum, 0, cklen);
+        currentPacket.writeData(b, offset, len);
+        currentPacket.numChunks++;
+        bytesCurBlock += len;
+
+        // If packet is full, enqueue it for transmission
+        //
+        if (currentPacket.numChunks == currentPacket.maxChunks ||
+            bytesCurBlock == blockSize) {
+          if (LOG.isDebugEnabled()) {
+            LOG.debug("AvatarClient writeChunk packet full seqno=" +
+                      currentPacket.seqno +
+                      ", src=" + src +
+                      ", bytesCurBlock=" + bytesCurBlock +
+                      ", blockSize=" + blockSize +
+                      ", appendChunk=" + appendChunk);
+          }
+          //
+          // if we allocated a new packet because we encountered a block
+          // boundary, reset bytesCurBlock.
+          //
+          if (bytesCurBlock == blockSize) {
+            currentPacket.lastPacketInBlock = true;
+            bytesCurBlock = 0;
+            lastFlushOffset = -1;
+          }
+          dataQueue.addLast(currentPacket);
+          dataQueue.notifyAll();
+          currentPacket = null;
+ 
+          // If this was the first write after reopening a file, then the above
+          // write filled up any partial chunk. Tell the summer to generate full 
+          // crc chunks from now on.
+          if (appendChunk) {
+            appendChunk = false;
+            resetChecksumChunk(bytesPerChecksum);
+          }
+          int psize = Math.min((int)(blockSize-bytesCurBlock), writePacketSize);
+          computePacketChunkSize(psize, bytesPerChecksum);
+        }
+      }
+      //LOG.debug("AvatarClient writeChunk done length " + len +
+      //          " checksum length " + cklen);
+    }
+  
+    /**
+     * All data is written out to datanodes. It is not guaranteed 
+     * that data has been flushed to persistent store on the 
+     * datanode. Block allocations are persisted on namenode.
+     */
+    public synchronized void sync() throws IOException {
+      try {
+        /* Record current blockOffset. This might be changed inside
+         * flushBuffer() where a partial checksum chunk might be flushed.
+         * After the flush, reset the bytesCurBlock back to its previous value,
+         * any partial checksum chunk will be sent now and in next packet.
+         */
+        long saveOffset = bytesCurBlock;
+
+        // flush checksum buffer, but keep checksum buffer intact
+        flushBuffer(true);
+
+        LOG.debug("AvatarClient flush() : saveOffset " + saveOffset +  
+                  " bytesCurBlock " + bytesCurBlock +
+                  " lastFlushOffset " + lastFlushOffset);
+        
+        // Flush only if we haven't already flushed till this offset.
+        if (lastFlushOffset != bytesCurBlock) {
+
+          // record the valid offset of this flush
+          lastFlushOffset = bytesCurBlock;
+
+          // wait for all packets to be sent and acknowledged
+          flushInternal();
+        } else {
+          // just discard the current packet since it is already been sent.
+          currentPacket = null;
+        }
+        
+        // Restore state of stream. Record the last flush offset 
+        // of the last full chunk that was flushed.
+        //
+        bytesCurBlock = saveOffset;
+
+        // If any new blocks were allocated since the last flush, 
+        // then persist block locations on namenode. 
+        //
+        if (persistBlocks) {
+          namenode.fsync(src, clientName);
+          persistBlocks = false;
+        }
+      } catch (IOException e) {
+          lastException = new IOException("IOException flush:" + e);
+          closed = true;
+          closeThreads();
+          throw e;
+      }
+    }
+
+    /**
+     * Waits till all existing data is flushed and confirmations 
+     * received from datanodes. 
+     */
+    private synchronized void flushInternal() throws IOException {
+      checkOpen();
+      isClosed();
+
+      while (!closed) {
+        synchronized (dataQueue) {
+          isClosed();
+          //
+          // If there is data in the current buffer, send it across
+          //
+          if (currentPacket != null) {
+            dataQueue.addLast(currentPacket);
+            dataQueue.notifyAll();
+            currentPacket = null;
+          }
+
+          // wait for all buffers to be flushed to datanodes
+          if (!closed && dataQueue.size() != 0) {
+            try {
+              dataQueue.wait();
+            } catch (InterruptedException e) {
+            }
+            continue;
+          }
+        }
+
+        // wait for all acks to be received back from datanodes
+        synchronized (ackQueue) {
+          if (!closed && ackQueue.size() != 0) {
+            try {
+              ackQueue.wait();
+            } catch (InterruptedException e) {
+            }
+            continue;
+          }
+        }
+
+        // acquire both the locks and verify that we are
+        // *really done*. In the case of error recovery, 
+        // packets might move back from ackQueue to dataQueue.
+        //
+        synchronized (dataQueue) {
+          synchronized (ackQueue) {
+            if (dataQueue.size() + ackQueue.size() == 0) {
+              break;       // we are done
+            }
+          }
+        }
+      }
+    }
+  
+    /**
+     * Closes this output stream and releases any system 
+     * resources associated with this stream.
+     */
+    @Override
+    public void close() throws IOException {
+      if (closed) {
+        IOException e = lastException;
+        if (e == null)
+          return;
+        else
+          throw e;
+      }
+      closeInternal();
+      leasechecker.remove(src);
+      
+      if (s != null) {
+        s.close();
+        s = null;
+      }
+    }
+ 
+    // shutdown datastreamer and responseprocessor threads.
+    private void closeThreads() throws IOException {
+      try {
+        streamer.close();
+        streamer.join();
+        
+        // shutdown response after streamer has exited.
+        if (response != null) {
+          response.close();
+          response.join();
+          response = null;
+        }
+      } catch (InterruptedException e) {
+        throw new IOException("Failed to shutdown response thread");
+      }
+    }
+    
+    /**
+     * Closes this output stream and releases any system 
+     * resources associated with this stream.
+     */
+    private synchronized void closeInternal() throws IOException {
+      checkOpen();
+      isClosed();
+
+      try {
+          flushBuffer();       // flush from all upper layers
+      
+          // Mark that this packet is the last packet in block.
+          // If there are no outstanding packets and the last packet
+          // was not the last one in the current block, then create a
+          // packet with empty payload.
+          synchronized (dataQueue) {
+            if (currentPacket == null && bytesCurBlock != 0) {
+              currentPacket = new Packet(packetSize, chunksPerPacket,
+                                         bytesCurBlock);
+            }
+            if (currentPacket != null) { 
+              currentPacket.lastPacketInBlock = true;
+            }
+          }
+
+        flushInternal();             // flush all data to Datanodes
+        isClosed(); // check to see if flushInternal had any exceptions
+        closed = true; // allow closeThreads() to showdown threads
+
+        closeThreads();
+        
+        synchronized (dataQueue) {
+          if (blockStream != null) {
+            blockStream.writeInt(0); // indicate end-of-block to datanode
+            blockStream.close();
+            blockReplyStream.close();
+          }
+          if (s != null) {
+            s.close();
+            s = null;
+          }
+        }
+
+        streamer = null;
+        blockStream = null;
+        blockReplyStream = null;
+
+        long localstart = System.currentTimeMillis();
+        boolean fileComplete = false;
+        while (!fileComplete) {
+          fileComplete = namenode.complete(src, clientName);
+          if (!fileComplete) {
+            try {
+              Thread.sleep(400);
+              if (System.currentTimeMillis() - localstart > 5000) {
+                LOG.info("Could not complete file " + src + " retrying...");
+              }
+            } catch (InterruptedException ie) {
+            }
+          }
+        }
+      } finally {
+        closed = true;
+      }
+    }
+
+    void setArtificialSlowdown(long period) {
+      artificialSlowdown = period;
+    }
+
+    synchronized void setChunksPerPacket(int value) {
+      chunksPerPacket = Math.min(chunksPerPacket, value);
+      packetSize = DataNode.PKT_HEADER_LEN + SIZE_OF_INTEGER +
+                   (checksum.getBytesPerChecksum() + 
+                    checksum.getChecksumSize()) * chunksPerPacket;
+    }
+
+    synchronized void setTestFilename(String newname) {
+      src = newname;
+    }
+
+    /**
+     * Returns the size of a file as it was when this stream was opened
+     */
+    long getInitialLen() {
+      return initialFileSize;
+    }
+  }
+
+  void reportChecksumFailure(String file, Block blk, DatanodeInfo dn) {
+    DatanodeInfo [] dnArr = { dn };
+    LocatedBlock [] lblocks = { new LocatedBlock(blk, dnArr) };
+    reportChecksumFailure(file, lblocks);
+  }
+    
+  // just reports checksum failure and ignores any exception during the report.
+  void reportChecksumFailure(String file, LocatedBlock lblocks[]) {
+    try {
+      reportBadBlocks(lblocks);
+    } catch (IOException ie) {
+      LOG.info("Found corruption while reading " + file 
+               + ".  Error repairing corrupt blocks.  Bad blocks remain. " 
+               + StringUtils.stringifyException(ie));
+    }
+  }
+
+  /** {@inheritDoc} */
+  public String toString() {
+    return getClass().getSimpleName() + "[clientName=" + clientName
+        + ", ugi=" + ugi + "]"; 
+  }
+}
Index: src/contrib/highavailability/README
===================================================================
--- src/contrib/highavailability/README	(revision 0)
+++ src/contrib/highavailability/README	(revision 0)
@@ -0,0 +1,66 @@
+# Copyright 2008 The Apache Software Foundation Licensed under the
+# Apache License, Version 2.0 (the "License"); you may not use this
+# file except in compliance with the License.  You may obtain a copy
+# of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless
+# required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+# implied.  See the License for the specific language governing
+# permissions and limitations under the License.
+
+/**
+ * The AvatarNode is your answer for High Availability of the 
+ * NameNode.  The AvatarNode has two avatars.. the Standby avatar 
+ * and the Active avatar.
+ * 
+ * In the Standby avatar, the AvatarNode is consuming transaction logs
+ * generated by the primary (via a transaction log stored in a shared device).
+ * Typically, the primary Namenode is writing transactions to a NFS filesystem
+ * and the Standby is reading the log from the same NFS filesystem. The 
+ * Standby is also making periodic checkpoints to the primary namenode.
+ * 
+ * A manual command can switch the AvatarNode from the Standby avatar
+ * to the Active avatar. In the Active avatar, the AvatarNode performs precisely
+ * the same functionality as a real usual Namenode. The switching from 
+ * Standby avatar to the Active avatar is fast and can typically occur 
+ * within seconds.
+ *
+ * Typically, an adminstrator will run require two shared mount points for
+ * transaction logs. It has to be set in fs.name.dir.shared0 and
+ * fs.name.dir.shared1 (similarly for edits). Then the adminstrator starts
+ * the AvatarNode on two different machines as follows:
+ *
+ * bin/hadoop org.apache.hadoop.hdfs.server.namenode.AvatarNode -zero -active
+ * bin/hadoop org.apache.hadoop.hdfs.server.namenode.AvatarNode -one -standby
+ * The first  AvatarNode uses  fs.name.dir.shared0 while the second
+ * AvatarNode uses fs.name.dir.shared1 to write its transaction logs.
+ * Also, at startup, the first instance is the primary Namenode and the
+ * second instance is the Standby
+ *
+ * After a while, the adminstrator decides to change the avatar of the
+ * second instance to Active. In this case, he/she has to first ensure that the
+ * first instance is really really dead. This code does not handle the
+ * split-brain scenario where there are two active namenodes in one cluster.
+ *
+
+--------------------------------------------------------------------------------
+
+BUILDING:
+
+In HADOOP_HOME, run ant package to build Hadoop and its contrib packages.
+
+--------------------------------------------------------------------------------
+
+INSTALLING and CONFIGURING:
+
+--------------------------------------------------------------------------------
+
+OPTIONAL CONFIGIURATION:
+
+--------------------------------------------------------------------------------
+
+ADMINISTRATION:
+
+--------------------------------------------------------------------------------
+
+IMPLEMENTATION:
Index: src/contrib/highavailability/build.xml
===================================================================
--- src/contrib/highavailability/build.xml	(revision 0)
+++ src/contrib/highavailability/build.xml	(revision 0)
@@ -0,0 +1,58 @@
+<?xml version="1.0"?>
+
+<!--
+   Licensed to the Apache Software Foundation (ASF) under one or more
+   contributor license agreements.  See the NOTICE file distributed with
+   this work for additional information regarding copyright ownership.
+   The ASF licenses this file to You under the Apache License, Version 2.0
+   (the "License"); you may not use this file except in compliance with
+   the License.  You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
+-->
+
+<!--
+Before you can run these subtargets directly, you need
+to call at top-level: ant deploy-contrib compile-core-test
+-->
+<project name="highavailability" default="jar" xmlns:ivy="antlib:org.apache.ivy.ant">
+
+  <import file="../build-contrib.xml"/>
+
+  <target name="test" depends="compile,compile-test,test-junit" description="Automated Test Framework" if="test.available"/>
+
+  <target name="test-junit" depends="compile,compile-test" if="test.available">
+    <junit fork="yes" printsummary="yes" errorProperty="tests.failed" 
+           haltonfailure="no" failureProperty="tests.failed">
+
+        <classpath refid="test.classpath"/>
+        <sysproperty key="test.build.data" value="${build.test}/data"/>
+              <sysproperty key="build.test" value="${build.test}"/>
+              <sysproperty key="user.dir" value="${build.test}/data"/>
+              <sysproperty key="fs.default.name" value="${fs.default.name}"/>
+              <sysproperty key="hadoop.test.localoutputfile" value="${hadoop.test.localoutputfile}"/>
+              <sysproperty key="hadoop.log.dir" value="${hadoop.log.dir}"/>
+        <sysproperty key="test.src.dir" value="${test.src.dir}"/>
+        <formatter type="plain" />
+        <batchtest todir="${build.test}" unless="testcase">
+           <fileset dir="${src.test}">
+             <include name="**/Test*.java"/>
+           </fileset>
+        </batchtest>
+        <batchtest todir="${build.test}" if="testcase">
+            <fileset dir="${src.test}">
+                <include name="**/${testcase}.java"/>
+            </fileset>
+         </batchtest>
+    </junit>
+    <fail if="tests.failed">Tests failed!</fail>
+ </target>
+
+</project>
+
Index: src/mapred/org/apache/hadoop/mapred/JobInProgress.java
===================================================================
--- src/mapred/org/apache/hadoop/mapred/JobInProgress.java	(revision 954586)
+++ src/mapred/org/apache/hadoop/mapred/JobInProgress.java	(working copy)
@@ -466,10 +466,17 @@
 
     // Calculate the minimum number of maps to be complete before 
     // we should start scheduling reduces
+    float completedMapsFraction = 
+        conf.getFloat("mapred.reduce.slowstart.completed.maps",
+                   DEFAULT_COMPLETED_MAPS_PERCENT_FOR_REDUCE_SLOWSTART);
+    if (completedMapsFraction < 0 ||
+          completedMapsFraction > 1) {
+      throw new IOException("mapred.reduce.slowstart.completed.maps has " +
+        "to be a float value between 0.0 and 1.0 inclusive");
+    }
     completedMapsForReduceSlowstart = 
       (int)Math.ceil(
-          (conf.getFloat("mapred.reduce.slowstart.completed.maps", 
-                         DEFAULT_COMPLETED_MAPS_PERCENT_FOR_REDUCE_SLOWSTART) * 
+          (completedMapsFraction * 
            numMapTasks));
 
     // create cleanup two cleanup tips, one map and one reduce.
